{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "g5Nk-MI4-nLz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from scipy.linalg import kron\n",
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"list_of_grads\", \"rb\") as fp:   #Pickling\n",
    "    list_of_grads = pickle.load(fp)\n",
    "\n",
    "list_of_grads = torch.stack(list_of_grads, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([625, 192, 768])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_grads.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.6783e-04,  7.5972e-05,  5.4008e-06,  ...,  2.3077e-04,\n",
       "          0.0000e+00,  3.9983e-04],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00],\n",
       "        [ 3.2776e-04,  5.8912e-05,  1.5232e-05,  ...,  1.0055e-04,\n",
       "          0.0000e+00,  1.1171e-04],\n",
       "        ...,\n",
       "        [-1.3369e-04, -2.2792e-05,  1.4925e-04,  ...,  1.3892e-04,\n",
       "          0.0000e+00,  1.1369e-04],\n",
       "        [ 3.4120e-04,  1.5226e-04,  0.0000e+00,  ...,  1.5480e-04,\n",
       "          0.0000e+00,  9.4895e-05],\n",
       "        [-2.7674e-04,  0.0000e+00,  0.0000e+00,  ..., -6.0975e-05,\n",
       "          0.0000e+00, -1.7622e-04]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_grads[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = list_of_grads[0].shape[0]\n",
    "n = list_of_grads[0].shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192 256\n",
      "torch.Size([157, 49152])\n",
      "matvec\n",
      "36864 1024 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/video_vika/lib/python3.10/site-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 36 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "Operator created\n",
      "Performing SVD...\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[ 4.2055047e-11  1.0018565e-10 -2.4735373e-11 ...  2.5676721e-13\n",
      " -5.7788943e-12  1.0330430e-11]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[ 1.7374263e-11  2.1050408e-11 -6.1603353e-12 ...  6.0821926e-14\n",
      " -9.7395800e-13  4.5132244e-12]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[ 2.8029376e-11  4.1671683e-12 -4.2693944e-13 ...  2.1808889e-14\n",
      "  9.0210131e-13  2.2330094e-12]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[ 7.5698926e-11 -1.4508956e-12  1.2979434e-11 ...  3.1836604e-14\n",
      "  1.1252966e-12 -4.1505163e-14]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[ 6.27822377e-11 -1.29258887e-12  1.02872667e-11 ...  1.12067795e-14\n",
      " -1.32225979e-12  8.46865113e-13]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[ 2.0731049e-11 -6.4113944e-12 -2.6548590e-11 ... -1.5894057e-14\n",
      " -1.8452607e-12  2.7267114e-12]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[ 1.9652122e-11 -2.0171139e-11 -4.8472684e-11 ... -2.3796696e-14\n",
      " -2.1295311e-13  4.4745136e-12]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[-9.2977423e-12 -7.6184900e-12 -9.5040659e-12 ... -2.9312168e-14\n",
      " -5.4108031e-13  1.2482446e-12]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[-1.3191175e-11  6.1485335e-12  6.9038946e-12 ... -9.3264419e-14\n",
      " -1.4904874e-12 -2.4336831e-13]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[ 5.4856987e-12  4.9738698e-12  9.6662348e-12 ... -4.3104151e-14\n",
      "  3.2416569e-13 -1.2638389e-13]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[ 2.7918904e-12  8.8681953e-13  2.0595518e-11 ... -1.0646962e-14\n",
      "  1.2131950e-12 -9.4368296e-13]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[ 7.1903729e-12 -9.8814811e-12  3.5109637e-11 ... -3.5194944e-14\n",
      "  9.1140485e-13 -1.0347798e-12]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[ 3.5608170e-12 -9.9619965e-12  3.0029628e-11 ... -2.7486731e-14\n",
      " -1.5244780e-12 -6.4919077e-13]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[ 7.6275288e-12 -7.2894113e-12  6.1883429e-11 ...  2.5839885e-14\n",
      "  1.0314967e-13  3.7194579e-14]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[ 6.3716901e-12 -1.3020592e-12  4.6005068e-11 ...  2.1302586e-14\n",
      "  2.7365016e-12  1.4917457e-12]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[-1.1225515e-11 -4.4048341e-12  2.2736668e-11 ... -1.6237893e-14\n",
      "  6.5326591e-13  1.7955899e-12]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[-1.9070899e-11 -1.2059962e-11  4.5399916e-11 ... -2.1008228e-15\n",
      "  4.7573376e-13  2.9979866e-12]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[-2.8016376e-12 -5.8102052e-12  2.5374548e-11 ...  6.5384680e-15\n",
      "  9.3211544e-14  8.6035481e-13]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[-4.3283413e-13 -2.1597750e-12  3.4890698e-11 ...  1.1257602e-14\n",
      " -1.4935968e-12 -7.2115946e-13]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[ 7.0176975e-13 -2.8793662e-12  4.6841492e-11 ...  2.0979935e-15\n",
      " -2.0613535e-12 -1.1404295e-12]\n",
      "rmatvec\n",
      "matvec\n",
      "36864 1024 36\n",
      "[ 5.2170577e-12 -9.8268182e-14  1.5706138e-11 ...  1.8793833e-14\n",
      " -7.3701163e-13  7.4411587e-14]\n",
      "rmatvec\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "\n",
    "print (m, n)\n",
    "\n",
    "from scipy.sparse.linalg import svds, LinearOperator\n",
    "\n",
    "\n",
    "grad_vectors = torch.stack([grad.T.reshape(-1) for grad in list_of_grads])\n",
    "print (grad_vectors.shape)\n",
    "d_grad_vectors = cuda.to_device(grad_vectors)\n",
    "\n",
    "\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def getitem_fmatrices_permuted(d_grad_vectors, p, q):\n",
    "    reduce_init_val = 0.0\n",
    "    for idx in range(d_grad_vectors.shape[0]):\n",
    "        #reduce_init_val+= d_grad_vectors[idx][p]*d_grad_vectors[idx][q]\n",
    "        elem = d_grad_vectors[idx]\n",
    "        reduce_init_val += elem[p] * elem[q]\n",
    "\n",
    "    return reduce_init_val/d_grad_vectors.shape[0]\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def matvec_kernel1(d_grad_vectors, x, res, m, n):\n",
    "    \"\"\"\n",
    "    CUDA kernel to compute matvec operation (A * x).\n",
    "    \"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    \n",
    "    pd, pn = divmod(np.int32(idx), m)\n",
    "    if idx < res.size:  # Each thread computes one element of the result\n",
    "        sum_ = 0.0\n",
    "        for jnd in range(x.size):\n",
    "            qd, qn = divmod(jnd, n)\n",
    "            sum_ += x[jnd]*getitem_fmatrices_permuted(d_grad_vectors, (pd)*n +qd, (pn)*n + qn)#d_grad_vector[(pd)*n +qd]*d_grad_vector[(pn)*n + qn] \n",
    "        res[idx] = sum_\n",
    "\n",
    "@cuda.jit\n",
    "def rmatvec_kernel1(d_grad_vectors, x, res, m, n):\n",
    "    \"\"\"\n",
    "    CUDA kernel to compute rmatvec operation (A.T * x).\n",
    "    \"\"\"\n",
    "    idx,elem_n = cuda.grid(2)\n",
    "    elem_n = np.int32(elem_n)\n",
    "    if idx < res.size:  # Each thread computes one element of the result\n",
    "        sum_ = 0.0\n",
    "        pd, pn = divmod(np.int32(idx), n)\n",
    "        for jnd in range(x.size):\n",
    "            qd, qn = divmod(jnd, m)\n",
    "            #jnd, idx\n",
    "            sum_ += x[jnd]*d_grad_vectors[elem_n][(jnd//m)*n +pd]*d_grad_vectors[elem_n][(jnd%m)*n + pn] \n",
    "            #sum_ += x[jnd]*getitem_fmatrices_permuted(d_grad_vectors, (jnd//m)*n +pd, (jnd%m)*n + pn)# d_grad_vector[(jnd//m)*n +pd]*d_grad_vector[(jnd%m)*n + pn]  #grad_vector[(q // m)*n +p // n]*grad_vector[(q % m)*n + p % n]\n",
    "        res[idx] += sum_\n",
    "\n",
    "def matvec_item_custom1(x):\n",
    "    \"\"\"\n",
    "    Host function to launch matvec_kernel.\n",
    "    \"\"\"\n",
    "    print (\"matvec\")\n",
    "    x = x.ravel()\n",
    "    res = np.zeros((m**2), dtype=np.float32)\n",
    "    threads_per_block = 1024\n",
    "    blocks_per_grid = (m**2 + threads_per_block - 1) // threads_per_block\n",
    "    print (m**2, threads_per_block, blocks_per_grid)\n",
    "\n",
    "\n",
    "    #d_grad_vector = cuda.to_device(grad_vector)\n",
    "    d_x = cuda.to_device(x)\n",
    "    d_res = cuda.device_array((m**2), dtype=np.float32)\n",
    "\n",
    "    matvec_kernel1[blocks_per_grid, threads_per_block](d_grad_vectors, d_x, d_res, m, n)\n",
    "\n",
    "    # Copy result back to host\n",
    "    res = d_res.copy_to_host()\n",
    "    print (res)\n",
    "    return res\n",
    "\n",
    "def rmatvec_item_custom1(x):\n",
    "    \"\"\"\n",
    "    Host function to launch rmatvec_kernel.\n",
    "    \"\"\"\n",
    "    res = np.zeros((n**2), dtype=np.float32)\n",
    "    print (\"rmatvec\")\n",
    "    x = x.ravel()\n",
    "    threads_per_block = (1024, 1)\n",
    "    blocks_per_grid = ((n**2 + threads_per_block[0] - 1) // threads_per_block[0], d_grad_vectors.shape[0])\n",
    "\n",
    "\n",
    "    #d_grad_vector = cuda.to_device(grad_vector)\n",
    "    d_x = cuda.to_device(x)\n",
    "    d_res = cuda.device_array((n**2), dtype=np.float32)\n",
    "\n",
    "    rmatvec_kernel1[blocks_per_grid, threads_per_block](d_grad_vectors, d_x, d_res, m, n)\n",
    "\n",
    "    # Copy result back to host\n",
    "    res = d_res.copy_to_host()\n",
    "    return (res/d_grad_vectors.shape[0])\n",
    "\n",
    "\n",
    "linop_m = LinearOperator(\n",
    "    shape=(m**2, n**2),\n",
    "    matvec=matvec_item_custom1,\n",
    "    rmatvec=rmatvec_item_custom1\n",
    ")\n",
    "print(\"Operator created\", flush=True)\n",
    "\n",
    "# Compute SVD\n",
    "print(\"Performing SVD...\", flush=True)\n",
    "v, s, u = svds(linop_v, k = 1, return_singular_vectors=True)\n",
    "#s = np.sort(s)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)\n",
    "\n",
    "C1 = s[0] * u[-1, :].reshape(n, n)\n",
    "B1 = v[:, -1].reshape(m, m)\n",
    "\n",
    "is_pos_def(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_pos_def(B1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_grad_vectors.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba\n",
    "\n",
    "print (m, n)\n",
    "\n",
    "from scipy.sparse.linalg import svds, LinearOperator\n",
    "\n",
    "\n",
    "grad_vectors = torch.stack([grad.T.reshape(-1) for grad in list_of_grads])\n",
    "print (grad_vectors.shape)\n",
    "d_grad_vectors = cuda.to_device(grad_vectors)\n",
    "\n",
    "reduce_init_val = 0.0\n",
    "\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def getitem_fmatrices_permuted(d_grad_vectors, p, q):\n",
    "    reduce_init_val = 0.0\n",
    "    for idx in range(d_grad_vectors.shape[0]):\n",
    "        #reduce_init_val+= d_grad_vectors[idx][p]*d_grad_vectors[idx][q]\n",
    "        elem = d_grad_vectors[idx]\n",
    "        reduce_init_val += elem[p] * elem[q]\n",
    "\n",
    "    return reduce_init_val/d_grad_vectors.shape[0]\n",
    "\n",
    "@cuda.jit\n",
    "def matvec_kernel(d_grad_vectors, x, res, m, n):\n",
    "    \"\"\"\n",
    "    CUDA kernel to compute matvec operation (A * x).\n",
    "    \"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    \n",
    "    pd, pn = divmod(np.int32(idx), m)\n",
    "    if idx < res.size:  # Each thread computes one element of the result\n",
    "        sum_ = 0.0\n",
    "        for jnd in range(x.size):\n",
    "            qd, qn = divmod(jnd, n)\n",
    "            sum_ += x[jnd]*getitem_fmatrices_permuted(d_grad_vectors, (pd)*n +qd, (pn)*n + qn)#d_grad_vector[(pd)*n +qd]*d_grad_vector[(pn)*n + qn] \n",
    "        res[idx] = sum_\n",
    "        \n",
    "@cuda.jit\n",
    "def rmatvec_kernel(d_grad_vectors, x, res, m, n):\n",
    "    \"\"\"\n",
    "    CUDA kernel to compute rmatvec operation (A.T * x).\n",
    "    \"\"\"\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < res.size:  # Each thread computes one element of the result\n",
    "        sum_ = 0.0\n",
    "        pd, pn = divmod(np.int32(idx), n)\n",
    "        for jnd in range(x.size):\n",
    "            #qd, qn = divmod(jnd, n)\n",
    "            #jnd, idx\n",
    "            sum_ += x[jnd]*getitem_fmatrices_permuted(d_grad_vectors, (jnd//m)*n +pd, (jnd%m)*n + pn)# d_grad_vector[(jnd//m)*n +pd]*d_grad_vector[(jnd%m)*n + pn]  #grad_vector[(q // m)*n +p // n]*grad_vector[(q % m)*n + p % n]\n",
    "        res[idx] = sum_\n",
    "\n",
    "def matvec_item_custom(x):\n",
    "    \"\"\"\n",
    "    Host function to launch matvec_kernel.\n",
    "    \"\"\"\n",
    "    print (\"matvec\")\n",
    "    x = x.ravel()\n",
    "    res = np.zeros((m**2), dtype=np.float32)\n",
    "    threads_per_block = 1024\n",
    "    blocks_per_grid = (m**2 + threads_per_block - 1) // threads_per_block\n",
    "    print (m**2, threads_per_block, blocks_per_grid)\n",
    "\n",
    "\n",
    "    #d_grad_vector = cuda.to_device(grad_vector)\n",
    "    d_x = cuda.to_device(x)\n",
    "    d_res = cuda.device_array((m**2), dtype=np.float32)\n",
    "\n",
    "    matvec_kernel[blocks_per_grid, threads_per_block](d_grad_vectors, d_x, d_res, m, n)\n",
    "\n",
    "    # Copy result back to host\n",
    "    res = d_res.copy_to_host()\n",
    "    print (res)\n",
    "    return res\n",
    "\n",
    "def rmatvec_item_custom(x):\n",
    "    \"\"\"\n",
    "    Host function to launch rmatvec_kernel.\n",
    "    \"\"\"\n",
    "    res = np.zeros((n**2), dtype=np.float32)\n",
    "    print (\"rmatvec\")\n",
    "    x = x.ravel()\n",
    "    threads_per_block = 1024\n",
    "    blocks_per_grid = (n**2 + threads_per_block - 1) // threads_per_block\n",
    "\n",
    "\n",
    "    #d_grad_vector = cuda.to_device(grad_vector)\n",
    "    d_x = cuda.to_device(x)\n",
    "    d_res = cuda.device_array((n**2), dtype=np.float32)\n",
    "\n",
    "    rmatvec_kernel[blocks_per_grid, threads_per_block](d_grad_vectors, d_x, d_res, m, n)\n",
    "\n",
    "    # Copy result back to host\n",
    "    res = d_res.copy_to_host()\n",
    "    return res\n",
    "\n",
    "\n",
    "linop_v = LinearOperator(\n",
    "    shape=(m**2, n**2),\n",
    "    matvec=matvec_item_custom,\n",
    "    rmatvec=rmatvec_item_custom\n",
    ")\n",
    "print(\"Operator created\", flush=True)\n",
    "\n",
    "# Compute SVD\n",
    "print(\"Performing SVD...\", flush=True)\n",
    "v, s, u = svds(linop_v, k = 3, return_singular_vectors=True)\n",
    "#s = np.sort(s)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def is_pos_def(x):\n",
    "    return np.all(np.linalg.eigvals(x) > 0)\n",
    "\n",
    "C1 = s[0] * u[-1, :].reshape(n, n)\n",
    "B1 = v[:, -1].reshape(m, m)\n",
    "\n",
    "is_pos_def(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = torch.rand(n*n)\n",
    "right = torch.rand(m*m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matvec\n",
      "36864 1024 36\n",
      "[ 3.8428084e-06  9.4785935e-07  6.1557512e-06 ... -2.2213476e-08\n",
      "  2.0142468e-07  7.8174253e-07]\n"
     ]
    }
   ],
   "source": [
    "resr_1 = linop_v.matvec(left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matvec\n",
      "36864 1024 36\n",
      "[ 3.8428084e-06  9.4785935e-07  6.1557512e-06 ... -2.2213476e-08\n",
      "  2.0142468e-07  7.8174253e-07]\n"
     ]
    }
   ],
   "source": [
    "resr_2 = linop_m.matvec(left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resr_2 = linop_m.matvec(left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(resr_1, resr_2, atol=1e-08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmatvec\n"
     ]
    }
   ],
   "source": [
    "resr_11 = linop_v.rmatvec(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmatvec\n"
     ]
    }
   ],
   "source": [
    "resr_22 =linop_m.rmatvec(right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.4164863e-05, -2.5415093e-06,  5.4153938e-06, ...,\n",
       "       -9.7085467e-06, -4.3056963e-07,  6.9029455e-05], dtype=float32)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resr_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.4168224e-05, -2.5402694e-06,  5.4232087e-06, ...,\n",
       "       -9.7085458e-06, -4.3056929e-07,  6.9029462e-05], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resr_22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(resr_11, resr_22, atol=1e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-video_vika]",
   "language": "python",
   "name": "conda-env-.mlspace-video_vika-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

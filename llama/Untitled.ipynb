{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1698388-f097-44e5-ae7f-2b14ff9220a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from compress_llama_with_kronsvd_llama2 import evaluate_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b342e4c-b01d-46db-b16e-ea4cf43d27b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from safetensors.torch import load_file\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "246c7dfb-40c1-45c2-93f1-b438461ed20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-24 22:24:06,655] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-24 22:24:06,871 - INFO - /home/jovyan/.mlspace/envs/vika_kurkin_clone/bin/x86_64-conda-linux-gnu-cc -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/jovyan/.mlspace/envs/vika_kurkin_clone/include -I/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/include -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib/stubs -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/jovyan/.mlspace/envs/vika_kurkin_clone/include -I/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/include -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib/stubs -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/jovyan/.mlspace/envs/vika_kurkin_clone/include -I/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/include -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib/stubs -c /tmp/tmpzde9uzbj/test.c -o /tmp/tmpzde9uzbj/test.o\n",
      "2025-04-24 22:24:06,895 - INFO - /home/jovyan/.mlspace/envs/vika_kurkin_clone/bin/x86_64-conda-linux-gnu-cc -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/jovyan/.mlspace/envs/vika_kurkin_clone/lib -Wl,-rpath-link,/home/jovyan/.mlspace/envs/vika_kurkin_clone/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib/stubs /tmp/tmpzde9uzbj/test.o -laio -o /tmp/tmpzde9uzbj/a.out\n",
      "/home/jovyan/.mlspace/envs/vika_kurkin_clone/bin/../lib/gcc/x86_64-conda-linux-gnu/12.4.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "2025-04-24 22:24:07,382 - INFO - /home/jovyan/.mlspace/envs/vika_kurkin_clone/bin/x86_64-conda-linux-gnu-cc -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/jovyan/.mlspace/envs/vika_kurkin_clone/include -I/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/include -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib/stubs -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/jovyan/.mlspace/envs/vika_kurkin_clone/include -I/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/include -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib/stubs -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/jovyan/.mlspace/envs/vika_kurkin_clone/include -I/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/include -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib/stubs -c /tmp/tmpd3_8imur/test.c -o /tmp/tmpd3_8imur/test.o\n",
      "2025-04-24 22:24:07,402 - INFO - /home/jovyan/.mlspace/envs/vika_kurkin_clone/bin/x86_64-conda-linux-gnu-cc -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/jovyan/.mlspace/envs/vika_kurkin_clone/lib -Wl,-rpath-link,/home/jovyan/.mlspace/envs/vika_kurkin_clone/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib/stubs /tmp/tmpd3_8imur/test.o -L/home/jovyan/.mlspace/envs/vika_kurkin_clone -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/lib64 -lcufile -o /tmp/tmpd3_8imur/a.out\n",
      "2025-04-24 22:24:07,443 - INFO - /home/jovyan/.mlspace/envs/vika_kurkin_clone/bin/x86_64-conda-linux-gnu-cc -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/jovyan/.mlspace/envs/vika_kurkin_clone/include -I/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/include -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib/stubs -DNDEBUG -D_FORTIFY_SOURCE=2 -O2 -isystem /home/jovyan/.mlspace/envs/vika_kurkin_clone/include -I/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/include -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib/stubs -fPIC -march=nocona -mtune=haswell -ftree-vectorize -fPIC -fstack-protector-strong -fno-plt -O2 -ffunction-sections -pipe -isystem /home/jovyan/.mlspace/envs/vika_kurkin_clone/include -I/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/include -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib/stubs -c /tmp/tmpchfb6p1h/test.c -o /tmp/tmpchfb6p1h/test.o\n",
      "2025-04-24 22:24:07,461 - INFO - /home/jovyan/.mlspace/envs/vika_kurkin_clone/bin/x86_64-conda-linux-gnu-cc -Wl,-O2 -Wl,--sort-common -Wl,--as-needed -Wl,-z,relro -Wl,-z,now -Wl,--disable-new-dtags -Wl,--gc-sections -Wl,--allow-shlib-undefined -Wl,-rpath,/home/jovyan/.mlspace/envs/vika_kurkin_clone/lib -Wl,-rpath-link,/home/jovyan/.mlspace/envs/vika_kurkin_clone/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib -L/home/jovyan/.mlspace/envs/vika_kurkin_clone/targets/x86_64-linux/lib/stubs /tmp/tmpchfb6p1h/test.o -laio -o /tmp/tmpchfb6p1h/a.out\n",
      "/home/jovyan/.mlspace/envs/vika_kurkin_clone/bin/../lib/gcc/x86_64-conda-linux-gnu/12.4.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd94abfa39e4d48a43dc9a95581bd4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/llama/llama10 were not used when initializing LlamaForCausalLM: ['model.layers.0.mlp.down_proj.0.weight', 'model.layers.0.mlp.down_proj.1.weight', 'model.layers.0.mlp.gate_proj.0.weight', 'model.layers.0.mlp.gate_proj.1.weight', 'model.layers.0.mlp.up_proj.0.weight', 'model.layers.0.mlp.up_proj.1.weight', 'model.layers.0.self_attn.k_proj.0.weight', 'model.layers.0.self_attn.k_proj.1.weight', 'model.layers.0.self_attn.o_proj.0.weight', 'model.layers.0.self_attn.o_proj.1.weight', 'model.layers.0.self_attn.q_proj.0.weight', 'model.layers.0.self_attn.q_proj.1.weight', 'model.layers.0.self_attn.v_proj.0.weight', 'model.layers.0.self_attn.v_proj.1.weight', 'model.layers.1.mlp.gate_proj.0.weight', 'model.layers.1.mlp.gate_proj.1.weight', 'model.layers.1.mlp.up_proj.0.weight', 'model.layers.1.mlp.up_proj.1.weight', 'model.layers.1.self_attn.k_proj.0.weight', 'model.layers.1.self_attn.k_proj.1.weight', 'model.layers.1.self_attn.o_proj.0.weight', 'model.layers.1.self_attn.o_proj.1.weight', 'model.layers.1.self_attn.q_proj.0.weight', 'model.layers.1.self_attn.q_proj.1.weight', 'model.layers.1.self_attn.v_proj.0.weight', 'model.layers.1.self_attn.v_proj.1.weight', 'model.layers.10.self_attn.k_proj.0.weight', 'model.layers.10.self_attn.k_proj.1.weight', 'model.layers.10.self_attn.q_proj.0.weight', 'model.layers.10.self_attn.q_proj.1.weight', 'model.layers.11.self_attn.k_proj.0.weight', 'model.layers.11.self_attn.k_proj.1.weight', 'model.layers.11.self_attn.o_proj.0.weight', 'model.layers.11.self_attn.o_proj.1.weight', 'model.layers.11.self_attn.q_proj.0.weight', 'model.layers.11.self_attn.q_proj.1.weight', 'model.layers.12.mlp.down_proj.0.weight', 'model.layers.12.mlp.down_proj.1.weight', 'model.layers.12.mlp.gate_proj.0.weight', 'model.layers.12.mlp.gate_proj.1.weight', 'model.layers.12.mlp.up_proj.0.weight', 'model.layers.12.mlp.up_proj.1.weight', 'model.layers.12.self_attn.k_proj.0.weight', 'model.layers.12.self_attn.k_proj.1.weight', 'model.layers.12.self_attn.q_proj.0.weight', 'model.layers.12.self_attn.q_proj.1.weight', 'model.layers.13.self_attn.k_proj.0.weight', 'model.layers.13.self_attn.k_proj.1.weight', 'model.layers.13.self_attn.o_proj.0.weight', 'model.layers.13.self_attn.o_proj.1.weight', 'model.layers.13.self_attn.q_proj.0.weight', 'model.layers.13.self_attn.q_proj.1.weight', 'model.layers.14.self_attn.k_proj.0.weight', 'model.layers.14.self_attn.k_proj.1.weight', 'model.layers.14.self_attn.q_proj.0.weight', 'model.layers.14.self_attn.q_proj.1.weight', 'model.layers.15.self_attn.q_proj.0.weight', 'model.layers.15.self_attn.q_proj.1.weight', 'model.layers.16.self_attn.k_proj.0.weight', 'model.layers.16.self_attn.k_proj.1.weight', 'model.layers.16.self_attn.q_proj.0.weight', 'model.layers.16.self_attn.q_proj.1.weight', 'model.layers.17.self_attn.k_proj.0.weight', 'model.layers.17.self_attn.k_proj.1.weight', 'model.layers.17.self_attn.q_proj.0.weight', 'model.layers.17.self_attn.q_proj.1.weight', 'model.layers.18.self_attn.k_proj.0.weight', 'model.layers.18.self_attn.k_proj.1.weight', 'model.layers.18.self_attn.o_proj.0.weight', 'model.layers.18.self_attn.o_proj.1.weight', 'model.layers.18.self_attn.q_proj.0.weight', 'model.layers.18.self_attn.q_proj.1.weight', 'model.layers.19.self_attn.k_proj.0.weight', 'model.layers.19.self_attn.k_proj.1.weight', 'model.layers.19.self_attn.q_proj.0.weight', 'model.layers.19.self_attn.q_proj.1.weight', 'model.layers.2.self_attn.k_proj.0.weight', 'model.layers.2.self_attn.k_proj.1.weight', 'model.layers.2.self_attn.q_proj.0.weight', 'model.layers.2.self_attn.q_proj.1.weight', 'model.layers.20.self_attn.k_proj.0.weight', 'model.layers.20.self_attn.k_proj.1.weight', 'model.layers.20.self_attn.q_proj.0.weight', 'model.layers.20.self_attn.q_proj.1.weight', 'model.layers.21.self_attn.k_proj.0.weight', 'model.layers.21.self_attn.k_proj.1.weight', 'model.layers.21.self_attn.q_proj.0.weight', 'model.layers.21.self_attn.q_proj.1.weight', 'model.layers.21.self_attn.v_proj.0.weight', 'model.layers.21.self_attn.v_proj.1.weight', 'model.layers.22.self_attn.k_proj.0.weight', 'model.layers.22.self_attn.k_proj.1.weight', 'model.layers.22.self_attn.q_proj.0.weight', 'model.layers.22.self_attn.q_proj.1.weight', 'model.layers.23.self_attn.k_proj.0.weight', 'model.layers.23.self_attn.k_proj.1.weight', 'model.layers.23.self_attn.o_proj.0.weight', 'model.layers.23.self_attn.o_proj.1.weight', 'model.layers.23.self_attn.q_proj.0.weight', 'model.layers.23.self_attn.q_proj.1.weight', 'model.layers.23.self_attn.v_proj.0.weight', 'model.layers.23.self_attn.v_proj.1.weight', 'model.layers.24.mlp.gate_proj.0.weight', 'model.layers.24.mlp.gate_proj.1.weight', 'model.layers.24.mlp.up_proj.0.weight', 'model.layers.24.mlp.up_proj.1.weight', 'model.layers.24.self_attn.k_proj.0.weight', 'model.layers.24.self_attn.k_proj.1.weight', 'model.layers.24.self_attn.o_proj.0.weight', 'model.layers.24.self_attn.o_proj.1.weight', 'model.layers.24.self_attn.q_proj.0.weight', 'model.layers.24.self_attn.q_proj.1.weight', 'model.layers.25.self_attn.o_proj.0.weight', 'model.layers.25.self_attn.o_proj.1.weight', 'model.layers.26.self_attn.k_proj.0.weight', 'model.layers.26.self_attn.k_proj.1.weight', 'model.layers.26.self_attn.o_proj.0.weight', 'model.layers.26.self_attn.o_proj.1.weight', 'model.layers.26.self_attn.q_proj.0.weight', 'model.layers.26.self_attn.q_proj.1.weight', 'model.layers.27.self_attn.k_proj.0.weight', 'model.layers.27.self_attn.k_proj.1.weight', 'model.layers.27.self_attn.q_proj.0.weight', 'model.layers.27.self_attn.q_proj.1.weight', 'model.layers.28.self_attn.k_proj.0.weight', 'model.layers.28.self_attn.k_proj.1.weight', 'model.layers.28.self_attn.o_proj.0.weight', 'model.layers.28.self_attn.o_proj.1.weight', 'model.layers.28.self_attn.q_proj.0.weight', 'model.layers.28.self_attn.q_proj.1.weight', 'model.layers.29.self_attn.k_proj.0.weight', 'model.layers.29.self_attn.k_proj.1.weight', 'model.layers.29.self_attn.o_proj.0.weight', 'model.layers.29.self_attn.o_proj.1.weight']\n",
      "- This IS expected if you are initializing LlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/llama/llama10 and are newly initialized: ['model.layers.0.mlp.down_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.o_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"/home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/llama/llama10\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/llama/llama10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2275f3b9-d3c9-4582-aa29-f1067e3ecdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392b4e92-2883-49b2-90c5-e76787a0e295",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-vika_kurkin_clone]",
   "language": "python",
   "name": "conda-env-.mlspace-vika_kurkin_clone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

2025-04-28 01:30:24,661 - INFO - Loading model: unsloth/llama-2-7b-chat
model.layers.28.self_attn.k_proj 0.3
model.layers.26.mlp.gate_proj 0.4
model.layers.26.mlp.up_proj 0.4
model.layers.25.mlp.up_proj 0.4
model.layers.23.mlp.gate_proj 0.4
model.layers.23.mlp.down_proj 0.4
model.layers.22.mlp.up_proj 0.4
model.layers.22.mlp.down_proj 0.4
model.layers.21.mlp.up_proj 0.4
model.layers.21.mlp.down_proj 0.4
model.layers.20.mlp.gate_proj 0.4
model.layers.19.mlp.up_proj 0.4
model.layers.19.self_attn.q_proj 0.3
model.layers.18.mlp.up_proj 0.4
model.layers.18.self_attn.q_proj 0.3
model.layers.18.self_attn.k_proj 0.3
model.layers.17.self_attn.q_proj 0.3
model.layers.16.self_attn.v_proj 0.3
model.layers.15.self_attn.q_proj 0.3
model.layers.14.mlp.gate_proj 0.4
model.layers.14.mlp.up_proj 0.4
model.layers.13.mlp.up_proj 0.4
model.layers.13.self_attn.q_proj 0.3
model.layers.13.self_attn.k_proj 0.3
model.layers.13.self_attn.v_proj 0.3
model.layers.13.self_attn.o_proj 0.3
model.layers.12.mlp.gate_proj 0.4
model.layers.12.mlp.up_proj 0.4
model.layers.12.self_attn.q_proj 0.3
model.layers.12.self_attn.k_proj 0.3
model.layers.11.self_attn.q_proj 0.3
model.layers.8.self_attn.q_proj 0.3
model.layers.8.self_attn.k_proj 0.3
model.layers.7.mlp.down_proj 0.4
model.layers.7.self_attn.q_proj 0.3
model.layers.7.self_attn.k_proj 0.3
model.layers.6.self_attn.q_proj 0.3
model.layers.6.self_attn.k_proj 0.3
model.layers.5.self_attn.k_proj 0.3
model.layers.4.self_attn.q_proj 0.3
model.layers.4.self_attn.k_proj 0.3
model.layers.3.self_attn.q_proj 0.3
model.layers.3.self_attn.k_proj 0.3
model.layers.2.mlp.gate_proj 0.4
model.layers.2.self_attn.q_proj 0.3
model.layers.2.self_attn.k_proj 0.3
[2025-04-28 01:30:29,244] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Loading checkpoint shards:   0%|                                                                                                                                                                            | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|██████████████████████████████████████████████████████▋                                                                                                             | 1/3 [00:00<00:01,  1.55it/s]Loading checkpoint shards:  67%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                      | 2/3 [00:01<00:00,  1.62it/s]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.88it/s]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.79it/s]
2025-04-28 01:30:33,398 - INFO - Model loaded with dtype torch.bfloat16
2025-04-28 01:31:05,895 - INFO - Model moved to cuda:0
2025-04-28 01:31:05,896 - INFO - Total parameters before compression: 6738415616
2025-04-28 01:31:05,896 - INFO - Found 225 linear layers to potentially compress.
Compressing Layers:   0%|                                                                                                                                                                                 | 0/225 [00:00<?, ?it/s]2025-04-28 01:31:05,897 - INFO - Skipping layer model.layers.0.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:31:05,897 - INFO - Skipping layer model.layers.0.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:31:05,897 - INFO - Skipping layer model.layers.0.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:31:05,897 - INFO - Skipping layer model.layers.0.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:31:05,897 - INFO - Skipping layer model.layers.0.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:31:05,897 - INFO - Skipping layer model.layers.0.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:31:05,897 - INFO - Skipping layer model.layers.0.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:31:05,897 - INFO - Skipping layer model.layers.1.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:31:05,897 - INFO - Skipping layer model.layers.1.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:31:05,897 - INFO - Skipping layer model.layers.1.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:31:05,897 - INFO - Skipping layer model.layers.1.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:31:05,897 - INFO - Skipping layer model.layers.1.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:31:05,897 - INFO - Skipping layer model.layers.1.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:31:05,897 - INFO - Skipping layer model.layers.1.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:31:05,897 - INFO - Layer: model.layers.2.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:31:05,899 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_q_proj.safetensors
2025-04-28 01:31:05,899 - INFO - exists: True
2025-04-28 01:31:05,901 - INFO - factorize_layer_kron_svd
2025-04-28 01:31:07,008 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:31:08,096 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:31:09,206 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:31:10,298 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:31:11,403 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:31:12,629 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-28 01:31:13,728 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:31:14,831 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:31:15,942 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:31:17,035 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:31:18,143 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:31:19,433 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_q_proj.safetensors True
Layer: model.layers.2.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.6124124e-05 1.1238000e-05 1.0046171e-05 9.5376463e-06 9.3023737e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:31:41,369 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:31:41,369 - INFO - Replacing 'model.layers.2.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:   7%|███████████▏                                                                                                                                                            | 15/225 [00:35<08:16,  2.36s/it]2025-04-28 01:31:41,369 - INFO - Layer: model.layers.2.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:31:41,370 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_k_proj.safetensors
2025-04-28 01:31:41,370 - INFO - exists: True
2025-04-28 01:31:41,374 - INFO - factorize_layer_kron_svd
2025-04-28 01:31:42,533 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:31:43,760 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-28 01:31:44,845 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:31:46,045 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_k_proj.safetensors True
Layer: model.layers.2.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [4.0046256e-03 1.5359199e-04 9.4199255e-05 5.9006747e-05 4.1726034e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:32:07,531 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:32:07,532 - INFO - Replacing 'model.layers.2.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:   7%|███████████▉                                                                                                                                                            | 16/225 [01:01<15:26,  4.43s/it]2025-04-28 01:32:07,532 - INFO - Skipping layer model.layers.2.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:32:07,532 - INFO - Skipping layer model.layers.2.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:32:07,532 - INFO - Layer: model.layers.2.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 01:32:07,534 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_gate_proj.safetensors
2025-04-28 01:32:07,534 - INFO - exists: True
2025-04-28 01:32:07,539 - INFO - factorize_layer_kron_svd
2025-04-28 01:32:08,784 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:32:09,857 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:32:10,947 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:32:12,003 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:32:13,092 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:32:14,313 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-28 01:32:15,941 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:32:17,805 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:32:19,789 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:32:21,667 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:32:23,640 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:32:26,524 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_gate_proj.safetensors True
Layer: model.layers.2.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.0536157e-05 7.8889116e-06 7.3416263e-06 6.9958951e-06 6.9346693e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-28 01:33:20,078 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-28 01:33:20,079 - INFO - Replacing 'model.layers.2.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:   8%|██████████████▏                                                                                                                                                         | 19/225 [02:14<33:37,  9.80s/it]2025-04-28 01:33:20,079 - INFO - Skipping layer model.layers.2.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:33:20,079 - INFO - Skipping layer model.layers.2.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:33:20,079 - INFO - Layer: model.layers.3.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:33:20,081 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_q_proj.safetensors
2025-04-28 01:33:20,081 - INFO - exists: True
2025-04-28 01:33:20,105 - INFO - factorize_layer_kron_svd
2025-04-28 01:33:21,269 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:33:22,525 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-28 01:33:23,620 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:33:24,886 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_q_proj.safetensors True
Layer: model.layers.3.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.1026561e-02 7.6449120e-05 6.9494883e-05 4.4865315e-05 4.0528874e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:33:49,809 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:33:49,809 - INFO - Replacing 'model.layers.3.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  10%|████████████████▍                                                                                                                                                       | 22/225 [02:43<33:15,  9.83s/it]2025-04-28 01:33:49,810 - INFO - Layer: model.layers.3.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:33:49,810 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_k_proj.safetensors
2025-04-28 01:33:49,810 - INFO - exists: True
2025-04-28 01:33:49,815 - INFO - factorize_layer_kron_svd
2025-04-28 01:33:50,964 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:33:52,030 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:33:53,106 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:33:54,173 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:33:55,239 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:33:56,423 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-28 01:33:57,470 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:33:58,520 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:33:59,583 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:34:00,641 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:34:01,728 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:34:02,934 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_k_proj.safetensors True
Layer: model.layers.3.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.5212615e-05 7.6301594e-06 6.8745117e-06 6.6968514e-06 6.6398402e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:34:24,807 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:34:24,808 - INFO - Replacing 'model.layers.3.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  10%|█████████████████▏                                                                                                                                                      | 23/225 [03:18<43:02, 12.78s/it]2025-04-28 01:34:24,809 - INFO - Skipping layer model.layers.3.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:34:24,809 - INFO - Skipping layer model.layers.3.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:34:24,809 - INFO - Skipping layer model.layers.3.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:34:24,809 - INFO - Skipping layer model.layers.3.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:34:24,809 - INFO - Skipping layer model.layers.3.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:34:24,809 - INFO - Layer: model.layers.4.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:34:24,811 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_q_proj.safetensors
2025-04-28 01:34:24,811 - INFO - exists: True
2025-04-28 01:34:24,816 - INFO - factorize_layer_kron_svd
2025-04-28 01:34:26,079 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:34:27,274 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:34:28,505 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:34:29,584 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:34:30,785 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_q_proj.safetensors True
Layer: model.layers.4.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.2226075e-02 9.3771603e-05 6.9365597e-05 3.1357977e-05 2.4107103e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:34:52,626 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:34:52,627 - INFO - Replacing 'model.layers.4.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  13%|█████████████████████▋                                                                                                                                                  | 29/225 [03:46<28:24,  8.70s/it]2025-04-28 01:34:52,627 - INFO - Layer: model.layers.4.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:34:52,628 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_k_proj.safetensors
2025-04-28 01:34:52,628 - INFO - exists: True
2025-04-28 01:34:52,634 - INFO - factorize_layer_kron_svd
2025-04-28 01:34:53,781 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:34:54,846 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:34:55,910 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:34:56,959 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:34:58,019 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:34:59,212 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-28 01:35:00,276 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:35:01,367 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:35:02,427 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:35:03,521 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:35:04,594 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:35:05,793 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_k_proj.safetensors True
Layer: model.layers.4.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.5640973e-05 8.6818372e-06 8.1892204e-06 8.0699401e-06 7.7858249e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:35:27,754 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:35:27,755 - INFO - Replacing 'model.layers.4.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  13%|██████████████████████▍                                                                                                                                                 | 30/225 [04:21<37:25, 11.52s/it]2025-04-28 01:35:27,755 - INFO - Skipping layer model.layers.4.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:35:27,755 - INFO - Skipping layer model.layers.4.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:35:27,755 - INFO - Skipping layer model.layers.4.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:35:27,755 - INFO - Skipping layer model.layers.4.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:35:27,755 - INFO - Skipping layer model.layers.4.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:35:27,755 - INFO - Skipping layer model.layers.5.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:35:27,755 - INFO - Layer: model.layers.5.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:35:27,757 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_5_self_attn_k_proj.safetensors
2025-04-28 01:35:27,757 - INFO - exists: True
2025-04-28 01:35:27,766 - INFO - factorize_layer_kron_svd
2025-04-28 01:35:28,994 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:35:30,214 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-28 01:35:31,278 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:35:32,466 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_5_self_attn_k_proj.safetensors True
Layer: model.layers.5.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [4.8379018e-03 1.5238064e-04 9.5437594e-05 3.4556335e-05 3.0823277e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:35:54,078 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:35:54,078 - INFO - Replacing 'model.layers.5.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  16%|███████████████████████████▋                                                                                                                                            | 37/225 [04:48<23:32,  7.51s/it]2025-04-28 01:35:54,078 - INFO - Skipping layer model.layers.5.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:35:54,078 - INFO - Skipping layer model.layers.5.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:35:54,078 - INFO - Skipping layer model.layers.5.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:35:54,078 - INFO - Skipping layer model.layers.5.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:35:54,078 - INFO - Skipping layer model.layers.5.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:35:54,079 - INFO - Layer: model.layers.6.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:35:54,079 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_q_proj.safetensors
2025-04-28 01:35:54,079 - INFO - exists: True
2025-04-28 01:35:54,086 - INFO - factorize_layer_kron_svd
2025-04-28 01:35:55,260 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:35:56,326 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:35:57,404 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:35:58,469 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:35:59,543 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:36:00,740 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-28 01:36:01,814 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:36:02,889 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:36:03,953 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:36:05,018 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:36:06,098 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:36:07,296 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_q_proj.safetensors True
Layer: model.layers.6.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.2221675e-05 8.1780718e-06 7.9897936e-06 7.7061068e-06 7.4714530e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:36:29,614 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:36:29,614 - INFO - Replacing 'model.layers.6.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  19%|████████████████████████████████                                                                                                                                        | 43/225 [05:23<20:55,  6.90s/it]2025-04-28 01:36:29,614 - INFO - Layer: model.layers.6.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:36:29,616 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_k_proj.safetensors
2025-04-28 01:36:29,616 - INFO - exists: True
2025-04-28 01:36:29,622 - INFO - factorize_layer_kron_svd
2025-04-28 01:36:30,745 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:36:31,820 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:36:32,885 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:36:33,948 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:36:35,013 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:36:36,203 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-28 01:36:37,241 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:36:38,283 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:36:39,327 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:36:40,360 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:36:41,407 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:36:42,582 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_k_proj.safetensors True
Layer: model.layers.6.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.4503589e-05 7.8944504e-06 7.5118478e-06 7.3252472e-06 7.0036058e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:37:05,191 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:37:05,192 - INFO - Replacing 'model.layers.6.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  20%|████████████████████████████████▊                                                                                                                                       | 44/225 [05:59<28:06,  9.32s/it]2025-04-28 01:37:05,192 - INFO - Skipping layer model.layers.6.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:37:05,192 - INFO - Skipping layer model.layers.6.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:37:05,192 - INFO - Skipping layer model.layers.6.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:37:05,192 - INFO - Skipping layer model.layers.6.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:37:05,192 - INFO - Skipping layer model.layers.6.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:37:05,192 - INFO - Layer: model.layers.7.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:37:05,193 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_q_proj.safetensors
2025-04-28 01:37:05,193 - INFO - exists: True
2025-04-28 01:37:05,232 - INFO - factorize_layer_kron_svd
2025-04-28 01:37:06,433 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:37:07,654 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-28 01:37:08,717 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:37:09,962 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_q_proj.safetensors True
Layer: model.layers.7.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [4.46691271e-03 1.17237636e-04 4.46072554e-05 2.93035046e-05
 2.62491722e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:37:32,474 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:37:32,474 - INFO - Replacing 'model.layers.7.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  22%|█████████████████████████████████████▎                                                                                                                                  | 50/225 [06:26<21:20,  7.32s/it]2025-04-28 01:37:32,475 - INFO - Layer: model.layers.7.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:37:32,476 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_k_proj.safetensors
2025-04-28 01:37:32,476 - INFO - exists: True
2025-04-28 01:37:32,483 - INFO - factorize_layer_kron_svd
2025-04-28 01:37:33,642 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:37:34,885 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-28 01:37:35,957 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:37:37,149 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_k_proj.safetensors True
Layer: model.layers.7.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [4.3513221e-03 9.5677962e-05 5.9629248e-05 3.8686332e-05 2.6624961e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:37:59,864 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:37:59,865 - INFO - Replacing 'model.layers.7.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  23%|██████████████████████████████████████                                                                                                                                  | 51/225 [06:53<26:30,  9.14s/it]2025-04-28 01:37:59,865 - INFO - Skipping layer model.layers.7.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:37:59,865 - INFO - Skipping layer model.layers.7.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:37:59,865 - INFO - Skipping layer model.layers.7.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:37:59,865 - INFO - Skipping layer model.layers.7.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:37:59,865 - INFO - Layer: model.layers.7.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 01:37:59,866 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_down_proj.safetensors
2025-04-28 01:37:59,866 - INFO - exists: True
2025-04-28 01:37:59,874 - INFO - factorize_layer_kron_svd
2025-04-28 01:38:01,903 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:38:05,077 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-28 01:38:06,139 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:38:07,294 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:38:08,513 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_down_proj.safetensors True
Layer: model.layers.7.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [4.0638614e-03 2.6415018e-04 8.9995185e-05 2.9939682e-05 2.5749245e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,11008), (4096,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)
2025-04-28 01:39:01,386 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)')
2025-04-28 01:39:01,386 - INFO - Replacing 'model.layers.7.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  25%|█████████████████████████████████████████▊                                                                                                                              | 56/225 [07:55<29:15, 10.39s/it]2025-04-28 01:39:01,387 - INFO - Layer: model.layers.8.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:39:01,388 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_q_proj.safetensors
2025-04-28 01:39:01,388 - INFO - exists: True
2025-04-28 01:39:01,398 - INFO - factorize_layer_kron_svd
2025-04-28 01:39:02,590 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:39:03,812 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:39:05,051 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:39:06,134 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:39:07,334 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_q_proj.safetensors True
Layer: model.layers.8.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [5.8933757e-03 1.1251512e-04 4.9954931e-05 2.4490531e-05 1.9172267e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:39:29,570 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:39:29,571 - INFO - Replacing 'model.layers.8.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  25%|██████████████████████████████████████████▌                                                                                                                             | 57/225 [08:23<34:07, 12.18s/it]2025-04-28 01:39:29,571 - INFO - Layer: model.layers.8.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:39:29,572 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_k_proj.safetensors
2025-04-28 01:39:29,572 - INFO - exists: True
2025-04-28 01:39:29,577 - INFO - factorize_layer_kron_svd
2025-04-28 01:39:30,667 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:39:31,727 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:39:32,792 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:39:33,858 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:39:34,928 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:39:36,096 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-28 01:39:37,111 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:39:38,122 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:39:39,146 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:39:40,175 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:39:41,228 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:39:42,393 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_k_proj.safetensors True
Layer: model.layers.8.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.4188925e-05 7.6309198e-06 7.4526779e-06 7.2449202e-06 6.7348119e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:40:04,476 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:40:04,477 - INFO - Replacing 'model.layers.8.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  26%|███████████████████████████████████████████▎                                                                                                                            | 58/225 [08:58<41:53, 15.05s/it]2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.8.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.8.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.8.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.8.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.8.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.9.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.9.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.9.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.9.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.9.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.9.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.9.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.10.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.10.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.10.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.10.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.10.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.10.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Skipping layer model.layers.10.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:04,477 - INFO - Layer: model.layers.11.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:40:04,479 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_self_attn_q_proj.safetensors
2025-04-28 01:40:04,479 - INFO - exists: True
2025-04-28 01:40:04,484 - INFO - factorize_layer_kron_svd
2025-04-28 01:40:05,622 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:40:06,803 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:40:08,025 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:40:09,102 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:40:10,245 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:40:11,459 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_self_attn_q_proj.safetensors True
Layer: model.layers.11.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [2.6593389e-02 3.8675182e-05 2.0216057e-05 1.6013246e-05 1.3640273e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:40:33,470 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:40:33,470 - INFO - Replacing 'model.layers.11.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  35%|██████████████████████████████████████████████████████████▏                                                                                                             | 78/225 [09:27<10:47,  4.40s/it]2025-04-28 01:40:33,471 - INFO - Skipping layer model.layers.11.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:33,471 - INFO - Skipping layer model.layers.11.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:33,471 - INFO - Skipping layer model.layers.11.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:33,471 - INFO - Skipping layer model.layers.11.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:33,471 - INFO - Skipping layer model.layers.11.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:33,471 - INFO - Skipping layer model.layers.11.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:40:33,471 - INFO - Layer: model.layers.12.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:40:33,472 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_q_proj.safetensors
2025-04-28 01:40:33,472 - INFO - exists: True
2025-04-28 01:40:33,487 - INFO - factorize_layer_kron_svd
2025-04-28 01:40:34,639 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:40:35,825 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:40:37,047 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:40:38,112 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:40:39,275 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:40:40,486 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_q_proj.safetensors True
Layer: model.layers.12.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [9.15819407e-03 3.75725394e-05 2.34514755e-05 1.29712425e-05
 1.20524883e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:41:02,838 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:41:02,838 - INFO - Replacing 'model.layers.12.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  38%|███████████████████████████████████████████████████████████████▍                                                                                                        | 85/225 [09:56<10:08,  4.34s/it]2025-04-28 01:41:02,839 - INFO - Layer: model.layers.12.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:41:02,840 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_k_proj.safetensors
2025-04-28 01:41:02,840 - INFO - exists: True
2025-04-28 01:41:02,844 - INFO - factorize_layer_kron_svd
2025-04-28 01:41:04,022 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:41:05,149 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:41:06,282 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:41:07,392 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:41:08,479 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:41:09,695 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-28 01:41:10,770 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:41:11,848 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:41:12,929 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:41:14,014 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:41:15,094 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:41:16,290 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_k_proj.safetensors True
Layer: model.layers.12.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.35002065e-05 7.74964155e-06 7.61668571e-06 7.45528405e-06
 7.28192163e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:41:38,825 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:41:38,825 - INFO - Replacing 'model.layers.12.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  38%|████████████████████████████████████████████████████████████████▏                                                                                                       | 86/225 [10:32<14:02,  6.06s/it]2025-04-28 01:41:38,826 - INFO - Skipping layer model.layers.12.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:41:38,826 - INFO - Skipping layer model.layers.12.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:41:38,826 - INFO - Layer: model.layers.12.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 01:41:38,828 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_gate_proj.safetensors
2025-04-28 01:41:38,828 - INFO - exists: True
2025-04-28 01:41:38,832 - INFO - factorize_layer_kron_svd
2025-04-28 01:41:40,070 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:41:41,134 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:41:42,197 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:41:43,266 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:41:44,354 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:41:45,616 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-28 01:41:47,333 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:41:49,284 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:41:51,187 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:41:53,101 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:41:55,028 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:41:58,128 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_gate_proj.safetensors True
Layer: model.layers.12.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.2988171e-05 7.1681275e-06 6.5595987e-06 6.5212694e-06 6.4388596e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-28 01:42:42,252 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-28 01:42:42,252 - INFO - Replacing 'model.layers.12.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  40%|██████████████████████████████████████████████████████████████████▍                                                                                                     | 89/225 [11:36<20:11,  8.91s/it]2025-04-28 01:42:42,252 - INFO - Layer: model.layers.12.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 01:42:42,254 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_up_proj.safetensors
2025-04-28 01:42:42,254 - INFO - exists: True
2025-04-28 01:42:42,267 - INFO - factorize_layer_kron_svd
2025-04-28 01:42:43,491 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:42:44,546 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:42:45,594 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:42:46,633 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:42:47,697 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:42:48,870 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-28 01:42:50,579 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:42:52,475 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:42:54,363 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:42:56,241 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:42:58,110 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:43:01,179 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_up_proj.safetensors True
Layer: model.layers.12.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [5.9819213e-06 4.8709753e-06 4.7432140e-06 4.6781370e-06 4.6636528e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-28 01:43:47,286 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-28 01:43:47,287 - INFO - Replacing 'model.layers.12.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  40%|███████████████████████████████████████████████████████████████████▏                                                                                                    | 90/225 [12:41<30:27, 13.54s/it]2025-04-28 01:43:47,287 - INFO - Skipping layer model.layers.12.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:43:47,287 - INFO - Layer: model.layers.13.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:43:47,289 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_q_proj.safetensors
2025-04-28 01:43:47,289 - INFO - exists: True
2025-04-28 01:43:47,299 - INFO - factorize_layer_kron_svd
2025-04-28 01:43:48,490 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:43:49,757 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:43:51,081 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:43:52,145 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:43:53,384 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:43:54,668 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_q_proj.safetensors True
Layer: model.layers.13.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.9350363e-02 5.4489075e-05 1.5019226e-05 1.3564231e-05 1.1421780e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:44:17,404 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:44:17,404 - INFO - Replacing 'model.layers.13.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  41%|████████████████████████████████████████████████████████████████████▋                                                                                                   | 92/225 [13:11<30:39, 13.83s/it]2025-04-28 01:44:17,404 - INFO - Layer: model.layers.13.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:44:17,405 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_k_proj.safetensors
2025-04-28 01:44:17,405 - INFO - exists: True
2025-04-28 01:44:17,411 - INFO - factorize_layer_kron_svd
2025-04-28 01:44:18,558 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:44:19,669 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:44:20,767 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:44:21,877 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:44:23,012 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:44:24,266 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-28 01:44:25,367 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:44:26,475 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:44:27,563 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 01:44:28,631 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 01:44:29,728 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 01:44:30,907 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_k_proj.safetensors True
Layer: model.layers.13.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.3729892e-05 7.1575278e-06 7.1094173e-06 6.8093209e-06 6.5548347e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:44:52,902 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:44:52,903 - INFO - Replacing 'model.layers.13.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  41%|█████████████████████████████████████████████████████████████████████▍                                                                                                  | 93/225 [13:47<36:08, 16.43s/it]2025-04-28 01:44:52,903 - INFO - Layer: model.layers.13.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:44:52,905 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_v_proj.safetensors
2025-04-28 01:44:52,905 - INFO - exists: True
2025-04-28 01:44:52,928 - INFO - factorize_layer_kron_svd
2025-04-28 01:44:54,100 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:44:55,318 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-28 01:44:56,400 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:44:57,572 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:44:58,801 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_v_proj.safetensors True
Layer: model.layers.13.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [5.0983722e-03 5.0585440e-05 2.1063875e-05 1.7282286e-05 1.4230653e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:45:21,212 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:45:21,212 - INFO - Replacing 'model.layers.13.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)
Compressing Layers:  42%|██████████████████████████████████████████████████████████████████████▏                                                                                                 | 94/225 [14:15<39:39, 18.16s/it]2025-04-28 01:45:21,212 - INFO - Layer: model.layers.13.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:45:21,213 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_o_proj.safetensors
2025-04-28 01:45:21,213 - INFO - exists: True
2025-04-28 01:45:21,218 - INFO - factorize_layer_kron_svd
2025-04-28 01:45:22,356 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:45:23,521 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:45:24,768 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:45:25,863 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:45:27,048 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:45:28,326 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_o_proj.safetensors True
Layer: model.layers.13.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.2446343e-02 3.8981681e-05 3.5099336e-05 2.8158551e-05 2.2302480e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:45:50,536 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:45:50,537 - INFO - Replacing 'model.layers.13.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  42%|██████████████████████████████████████████████████████████████████████▉                                                                                                 | 95/225 [14:44<43:32, 20.09s/it]2025-04-28 01:45:50,537 - INFO - Skipping layer model.layers.13.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:45:50,537 - INFO - Layer: model.layers.13.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 01:45:50,538 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_up_proj.safetensors
2025-04-28 01:45:50,538 - INFO - exists: True
2025-04-28 01:45:50,547 - INFO - factorize_layer_kron_svd
2025-04-28 01:45:51,777 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:45:52,939 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:45:54,163 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:45:55,889 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:45:58,619 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:46:01,746 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_up_proj.safetensors True
Layer: model.layers.13.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [6.4682798e-03 6.1749437e-05 2.4349400e-05 2.1919277e-05 2.0116417e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-28 01:46:46,041 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-28 01:46:46,042 - INFO - Replacing 'model.layers.13.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  43%|████████████████████████████████████████████████████████████████████████▍                                                                                               | 97/225 [15:40<48:15, 22.62s/it]2025-04-28 01:46:46,042 - INFO - Skipping layer model.layers.13.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:46:46,042 - INFO - Skipping layer model.layers.14.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:46:46,042 - INFO - Skipping layer model.layers.14.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:46:46,042 - INFO - Skipping layer model.layers.14.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:46:46,042 - INFO - Skipping layer model.layers.14.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:46:46,042 - INFO - Layer: model.layers.14.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 01:46:46,044 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_gate_proj.safetensors
2025-04-28 01:46:46,044 - INFO - exists: True
2025-04-28 01:46:46,057 - INFO - factorize_layer_kron_svd
2025-04-28 01:46:47,304 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:46:48,562 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:46:49,860 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:46:51,592 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:46:54,904 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_gate_proj.safetensors True
Layer: model.layers.14.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [3.2148813e-03 8.8560395e-05 2.4315630e-05 1.8305876e-05 1.6133905e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-28 01:47:44,991 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-28 01:47:44,992 - INFO - Replacing 'model.layers.14.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  46%|████████████████████████████████████████████████████████████████████████████▍                                                                                          | 103/225 [16:39<30:44, 15.12s/it]2025-04-28 01:47:44,993 - INFO - Layer: model.layers.14.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 01:47:44,994 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_up_proj.safetensors
2025-04-28 01:47:44,994 - INFO - exists: True
2025-04-28 01:47:45,007 - INFO - factorize_layer_kron_svd
2025-04-28 01:47:46,258 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:47:47,502 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:47:48,770 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:47:50,563 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:47:53,719 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:47:57,397 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_up_proj.safetensors True
Layer: model.layers.14.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [9.7202640e-03 2.5967920e-05 2.1275189e-05 1.6423986e-05 1.5875536e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-28 01:48:58,267 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-28 01:48:58,267 - INFO - Replacing 'model.layers.14.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  46%|█████████████████████████████████████████████████████████████████████████████▏                                                                                         | 104/225 [17:52<44:51, 22.24s/it]2025-04-28 01:48:58,268 - INFO - Skipping layer model.layers.14.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:48:58,268 - INFO - Layer: model.layers.15.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:48:58,270 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_self_attn_q_proj.safetensors
2025-04-28 01:48:58,270 - INFO - exists: True
2025-04-28 01:48:58,281 - INFO - factorize_layer_kron_svd
2025-04-28 01:48:59,399 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:49:00,620 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:49:01,965 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:49:03,061 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:49:04,247 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:49:05,521 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_self_attn_q_proj.safetensors True
Layer: model.layers.15.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.19919665e-02 7.38329763e-05 2.44418788e-05 1.36780509e-05
 1.00903217e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:49:34,074 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:49:34,075 - INFO - Replacing 'model.layers.15.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  47%|██████████████████████████████████████████████████████████████████████████████▋                                                                                        | 106/225 [18:28<41:53, 21.12s/it]2025-04-28 01:49:34,076 - INFO - Skipping layer model.layers.15.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:49:34,076 - INFO - Skipping layer model.layers.15.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:49:34,076 - INFO - Skipping layer model.layers.15.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:49:34,076 - INFO - Skipping layer model.layers.15.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:49:34,076 - INFO - Skipping layer model.layers.15.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:49:34,076 - INFO - Skipping layer model.layers.15.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:49:34,076 - INFO - Skipping layer model.layers.16.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:49:34,076 - INFO - Skipping layer model.layers.16.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:49:34,076 - INFO - Layer: model.layers.16.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:49:34,077 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_self_attn_v_proj.safetensors
2025-04-28 01:49:34,078 - INFO - exists: True
2025-04-28 01:49:34,085 - INFO - factorize_layer_kron_svd
2025-04-28 01:49:35,235 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:49:36,471 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-28 01:49:37,532 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:49:38,696 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:49:39,967 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_self_attn_v_proj.safetensors True
Layer: model.layers.16.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [2.1955712e-02 2.2352620e-05 1.8913308e-05 1.6113720e-05 1.5723988e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:50:09,002 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:50:09,002 - INFO - Replacing 'model.layers.16.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)
Compressing Layers:  51%|█████████████████████████████████████████████████████████████████████████████████████▎                                                                                 | 115/225 [19:03<18:58, 10.35s/it]2025-04-28 01:50:09,002 - INFO - Skipping layer model.layers.16.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:50:09,002 - INFO - Skipping layer model.layers.16.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:50:09,002 - INFO - Skipping layer model.layers.16.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:50:09,002 - INFO - Skipping layer model.layers.16.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:50:09,002 - INFO - Layer: model.layers.17.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:50:09,004 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_self_attn_q_proj.safetensors
2025-04-28 01:50:09,004 - INFO - exists: True
2025-04-28 01:50:09,009 - INFO - factorize_layer_kron_svd
2025-04-28 01:50:10,139 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:50:11,326 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:50:12,577 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:50:13,652 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:50:14,868 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:50:16,121 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_self_attn_q_proj.safetensors True
Layer: model.layers.17.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.9799747e-02 3.0997286e-05 1.8318075e-05 1.5734129e-05 9.7556685e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:50:44,888 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:50:44,888 - INFO - Replacing 'model.layers.17.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  53%|█████████████████████████████████████████████████████████████████████████████████████████                                                                              | 120/225 [19:38<16:15,  9.30s/it]2025-04-28 01:50:44,889 - INFO - Skipping layer model.layers.17.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:50:44,889 - INFO - Skipping layer model.layers.17.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:50:44,889 - INFO - Skipping layer model.layers.17.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:50:44,889 - INFO - Skipping layer model.layers.17.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:50:44,889 - INFO - Skipping layer model.layers.17.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:50:44,889 - INFO - Skipping layer model.layers.17.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:50:44,889 - INFO - Layer: model.layers.18.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:50:44,890 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_q_proj.safetensors
2025-04-28 01:50:44,890 - INFO - exists: True
2025-04-28 01:50:44,896 - INFO - factorize_layer_kron_svd
2025-04-28 01:50:46,057 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:50:47,250 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:50:48,505 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:50:49,568 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:50:50,699 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:50:51,952 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_q_proj.safetensors True
Layer: model.layers.18.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [4.0405821e-03 8.4903695e-05 8.4368939e-06 7.6583383e-06 6.2006247e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:51:20,436 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:51:20,436 - INFO - Replacing 'model.layers.18.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  56%|██████████████████████████████████████████████████████████████████████████████████████████████▎                                                                        | 127/225 [20:14<12:26,  7.61s/it]2025-04-28 01:51:20,436 - INFO - Layer: model.layers.18.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:51:20,438 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_k_proj.safetensors
2025-04-28 01:51:20,438 - INFO - exists: True
2025-04-28 01:51:20,445 - INFO - factorize_layer_kron_svd
2025-04-28 01:51:21,584 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:51:22,793 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:51:24,087 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:51:25,160 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:51:26,359 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:51:27,617 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_k_proj.safetensors True
Layer: model.layers.18.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [7.7125463e-03 3.3378132e-05 8.2256274e-06 7.4304949e-06 6.1785499e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:51:56,867 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:51:56,867 - INFO - Replacing 'model.layers.18.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  57%|███████████████████████████████████████████████████████████████████████████████████████████████                                                                        | 128/225 [20:50<15:48,  9.78s/it]2025-04-28 01:51:56,867 - INFO - Skipping layer model.layers.18.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:51:56,867 - INFO - Skipping layer model.layers.18.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:51:56,867 - INFO - Skipping layer model.layers.18.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:51:56,867 - INFO - Layer: model.layers.18.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 01:51:56,868 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_mlp_up_proj.safetensors
2025-04-28 01:51:56,868 - INFO - exists: True
2025-04-28 01:51:56,873 - INFO - factorize_layer_kron_svd
2025-04-28 01:51:58,160 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:51:59,342 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:52:00,602 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:52:02,430 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:52:06,128 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_mlp_up_proj.safetensors True
Layer: model.layers.18.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [5.7937209e-03 2.0253658e-05 1.6411330e-05 1.4584949e-05 1.2499403e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-28 01:53:10,350 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-28 01:53:10,350 - INFO - Replacing 'model.layers.18.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  59%|█████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                     | 132/225 [22:04<19:09, 12.36s/it]2025-04-28 01:53:10,350 - INFO - Skipping layer model.layers.18.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:53:10,350 - INFO - Layer: model.layers.19.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 01:53:10,352 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_self_attn_q_proj.safetensors
2025-04-28 01:53:10,352 - INFO - exists: True
2025-04-28 01:53:10,362 - INFO - factorize_layer_kron_svd
2025-04-28 01:53:11,491 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:53:12,740 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-28 01:53:13,824 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:53:15,068 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_self_attn_q_proj.safetensors True
Layer: model.layers.19.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [2.2634035e-03 7.5883843e-05 1.0972199e-05 7.9327338e-06 7.6394726e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 01:53:43,954 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 01:53:43,954 - INFO - Replacing 'model.layers.19.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  60%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                   | 134/225 [22:38<19:56, 13.15s/it]2025-04-28 01:53:43,954 - INFO - Skipping layer model.layers.19.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:53:43,954 - INFO - Skipping layer model.layers.19.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:53:43,954 - INFO - Skipping layer model.layers.19.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:53:43,954 - INFO - Skipping layer model.layers.19.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:53:43,954 - INFO - Layer: model.layers.19.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 01:53:43,955 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_mlp_up_proj.safetensors
2025-04-28 01:53:43,955 - INFO - exists: True
2025-04-28 01:53:43,961 - INFO - factorize_layer_kron_svd
2025-04-28 01:53:45,258 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:53:46,452 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:53:47,718 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:53:49,576 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:53:53,299 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_mlp_up_proj.safetensors True
Layer: model.layers.19.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [5.9256121e-03 1.7187345e-05 1.4408997e-05 1.2730728e-05 9.7351012e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-28 01:54:57,023 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-28 01:54:57,023 - INFO - Replacing 'model.layers.19.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  62%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                               | 139/225 [23:51<19:39, 13.72s/it]2025-04-28 01:54:57,023 - INFO - Skipping layer model.layers.19.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:54:57,023 - INFO - Skipping layer model.layers.20.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:54:57,023 - INFO - Skipping layer model.layers.20.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:54:57,023 - INFO - Skipping layer model.layers.20.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:54:57,023 - INFO - Skipping layer model.layers.20.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:54:57,023 - INFO - Layer: model.layers.20.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 01:54:57,025 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_20_mlp_gate_proj.safetensors
2025-04-28 01:54:57,025 - INFO - exists: True
2025-04-28 01:54:57,052 - INFO - factorize_layer_kron_svd
2025-04-28 01:54:58,312 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:54:59,518 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:55:00,751 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:55:02,668 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:55:06,316 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_20_mlp_gate_proj.safetensors True
Layer: model.layers.20.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [5.6294589e-03 2.1218650e-05 1.5237835e-05 1.4524096e-05 1.3646030e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-28 01:56:09,342 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-28 01:56:09,342 - INFO - Replacing 'model.layers.20.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  64%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 145/225 [25:03<17:24, 13.05s/it]2025-04-28 01:56:09,342 - INFO - Skipping layer model.layers.20.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:56:09,343 - INFO - Skipping layer model.layers.20.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:56:09,343 - INFO - Skipping layer model.layers.21.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:56:09,343 - INFO - Skipping layer model.layers.21.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:56:09,343 - INFO - Skipping layer model.layers.21.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:56:09,343 - INFO - Skipping layer model.layers.21.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:56:09,343 - INFO - Skipping layer model.layers.21.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:56:09,343 - INFO - Layer: model.layers.21.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 01:56:09,345 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_up_proj.safetensors
2025-04-28 01:56:09,345 - INFO - exists: True
2025-04-28 01:56:09,356 - INFO - factorize_layer_kron_svd
2025-04-28 01:56:10,561 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:56:11,743 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:56:12,987 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:56:14,896 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:56:18,578 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_up_proj.safetensors True
Layer: model.layers.21.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.8695735e-03 5.5963137e-05 1.4356574e-05 1.3274203e-05 1.2030486e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-28 01:57:22,564 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-28 01:57:22,564 - INFO - Replacing 'model.layers.21.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                     | 153/225 [26:16<13:38, 11.37s/it]2025-04-28 01:57:22,565 - INFO - Layer: model.layers.21.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 01:57:22,582 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_down_proj.safetensors
2025-04-28 01:57:22,582 - INFO - exists: True
2025-04-28 01:57:22,594 - INFO - factorize_layer_kron_svd
2025-04-28 01:57:24,439 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:57:26,838 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:57:30,442 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:57:31,521 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:57:32,697 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:57:33,968 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_down_proj.safetensors True
Layer: model.layers.21.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [4.5640193e-02 9.2487091e-05 5.4519722e-05 4.7700971e-05 3.9835424e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,11008), (4096,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)
2025-04-28 01:58:47,273 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)')
2025-04-28 01:58:47,273 - INFO - Replacing 'model.layers.21.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                    | 154/225 [27:41<19:39, 16.62s/it]2025-04-28 01:58:47,273 - INFO - Skipping layer model.layers.22.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:58:47,274 - INFO - Skipping layer model.layers.22.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:58:47,274 - INFO - Skipping layer model.layers.22.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:58:47,274 - INFO - Skipping layer model.layers.22.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:58:47,274 - INFO - Skipping layer model.layers.22.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 01:58:47,274 - INFO - Layer: model.layers.22.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 01:58:47,276 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_up_proj.safetensors
2025-04-28 01:58:47,276 - INFO - exists: True
2025-04-28 01:58:47,288 - INFO - factorize_layer_kron_svd
2025-04-28 01:58:48,553 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:58:49,773 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 01:58:51,032 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 01:58:52,939 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:58:56,596 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_up_proj.safetensors True
Layer: model.layers.22.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.2615041e-03 2.1604352e-05 1.3284305e-05 1.1028132e-05 1.0561911e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-28 01:59:54,906 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-28 01:59:54,906 - INFO - Replacing 'model.layers.22.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  71%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                | 160/225 [28:49<15:48, 14.59s/it]2025-04-28 01:59:54,906 - INFO - Layer: model.layers.22.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 01:59:54,908 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_down_proj.safetensors
2025-04-28 01:59:54,908 - INFO - exists: True
2025-04-28 01:59:54,921 - INFO - factorize_layer_kron_svd
2025-04-28 01:59:56,782 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 01:59:59,007 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 02:00:02,161 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 02:00:03,217 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 02:00:04,376 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 02:00:05,596 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_down_proj.safetensors True
Layer: model.layers.22.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [2.3855805e-02 8.0862206e-05 5.0205126e-05 4.2752916e-05 3.9248724e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,11008), (4096,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)
2025-04-28 02:00:57,275 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)')
2025-04-28 02:00:57,275 - INFO - Replacing 'model.layers.22.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                               | 161/225 [29:51<19:47, 18.55s/it]2025-04-28 02:00:57,275 - INFO - Skipping layer model.layers.23.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:00:57,275 - INFO - Skipping layer model.layers.23.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:00:57,275 - INFO - Skipping layer model.layers.23.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:00:57,275 - INFO - Skipping layer model.layers.23.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:00:57,275 - INFO - Layer: model.layers.23.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 02:00:57,277 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_gate_proj.safetensors
2025-04-28 02:00:57,277 - INFO - exists: True
2025-04-28 02:00:57,289 - INFO - factorize_layer_kron_svd
2025-04-28 02:00:58,528 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 02:00:59,662 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 02:01:00,878 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 02:01:02,747 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 02:01:05,914 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_gate_proj.safetensors True
Layer: model.layers.23.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.6312244e-03 1.5538668e-05 4.1256385e-06 2.1622898e-06 2.0991954e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-28 02:01:57,419 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-28 02:01:57,419 - INFO - Replacing 'model.layers.23.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  74%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                           | 166/225 [30:51<15:51, 16.12s/it]2025-04-28 02:01:57,420 - INFO - Skipping layer model.layers.23.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:01:57,420 - INFO - Layer: model.layers.23.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 02:01:57,422 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_down_proj.safetensors
2025-04-28 02:01:57,422 - INFO - exists: True
2025-04-28 02:01:57,435 - INFO - factorize_layer_kron_svd
2025-04-28 02:01:59,279 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 02:02:01,928 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 02:02:05,582 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 02:02:06,650 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 02:02:07,813 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 02:02:09,063 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_down_proj.safetensors True
Layer: model.layers.23.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [6.6739982e-03 2.3223182e-04 4.9300255e-05 3.7862912e-05 3.5104458e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,11008), (4096,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)
2025-04-28 02:03:23,781 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)')
2025-04-28 02:03:23,781 - INFO - Replacing 'model.layers.23.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                          | 168/225 [32:17<19:49, 20.87s/it]2025-04-28 02:03:23,782 - INFO - Skipping layer model.layers.24.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:03:23,782 - INFO - Skipping layer model.layers.24.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:03:23,782 - INFO - Skipping layer model.layers.24.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:03:23,782 - INFO - Skipping layer model.layers.24.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:03:23,782 - INFO - Skipping layer model.layers.24.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:03:23,782 - INFO - Skipping layer model.layers.24.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:03:23,782 - INFO - Skipping layer model.layers.24.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:03:23,782 - INFO - Skipping layer model.layers.25.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:03:23,782 - INFO - Skipping layer model.layers.25.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:03:23,782 - INFO - Skipping layer model.layers.25.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:03:23,782 - INFO - Skipping layer model.layers.25.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:03:23,782 - INFO - Skipping layer model.layers.25.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:03:23,782 - INFO - Layer: model.layers.25.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 02:03:23,785 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_25_mlp_up_proj.safetensors
2025-04-28 02:03:23,785 - INFO - exists: True
2025-04-28 02:03:23,798 - INFO - factorize_layer_kron_svd
2025-04-28 02:03:25,141 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 02:03:26,359 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 02:03:27,597 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 02:03:29,523 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 02:03:33,252 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_25_mlp_up_proj.safetensors True
Layer: model.layers.25.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [5.2369884e-03 1.2808485e-05 1.0910483e-05 1.0035876e-05 9.8339870e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-28 02:04:37,099 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-28 02:04:37,099 - INFO - Replacing 'model.layers.25.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  80%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                | 181/225 [33:31<08:23, 11.43s/it]2025-04-28 02:04:37,100 - INFO - Skipping layer model.layers.25.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:04:37,100 - INFO - Skipping layer model.layers.26.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:04:37,100 - INFO - Skipping layer model.layers.26.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:04:37,100 - INFO - Skipping layer model.layers.26.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:04:37,100 - INFO - Skipping layer model.layers.26.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:04:37,100 - INFO - Layer: model.layers.26.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 02:04:37,102 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_mlp_gate_proj.safetensors
2025-04-28 02:04:37,102 - INFO - exists: True
2025-04-28 02:04:37,114 - INFO - factorize_layer_kron_svd
2025-04-28 02:04:38,349 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 02:04:39,541 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 02:04:40,811 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-28 02:04:42,720 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 02:04:46,441 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_mlp_gate_proj.safetensors True
Layer: model.layers.26.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [3.5354062e-03 1.2539586e-05 5.3919930e-06 4.6146652e-06 4.4492190e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-28 02:05:50,096 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-28 02:05:50,096 - INFO - Replacing 'model.layers.26.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  83%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                            | 187/225 [34:44<07:22, 11.65s/it]2025-04-28 02:05:50,097 - INFO - Layer: model.layers.26.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-28 02:05:50,099 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_mlp_up_proj.safetensors
2025-04-28 02:05:50,099 - INFO - exists: True
2025-04-28 02:05:50,121 - INFO - factorize_layer_kron_svd
2025-04-28 02:05:51,336 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 02:05:52,547 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-28 02:05:54,480 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 02:05:58,099 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_mlp_up_proj.safetensors True
Layer: model.layers.26.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [5.6883384e-04 1.2054178e-04 1.2884599e-05 1.1027682e-05 9.3955405e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-28 02:07:02,934 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-28 02:07:02,934 - INFO - Replacing 'model.layers.26.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                           | 188/225 [35:57<09:37, 15.60s/it]2025-04-28 02:07:02,934 - INFO - Skipping layer model.layers.26.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:02,934 - INFO - Skipping layer model.layers.27.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:02,934 - INFO - Skipping layer model.layers.27.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:02,934 - INFO - Skipping layer model.layers.27.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:02,934 - INFO - Skipping layer model.layers.27.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:02,934 - INFO - Skipping layer model.layers.27.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:02,934 - INFO - Skipping layer model.layers.27.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:02,934 - INFO - Skipping layer model.layers.27.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:02,934 - INFO - Skipping layer model.layers.28.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:02,934 - INFO - Layer: model.layers.28.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-28 02:07:02,937 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_28_self_attn_k_proj.safetensors
2025-04-28 02:07:02,937 - INFO - exists: True
2025-04-28 02:07:02,946 - INFO - factorize_layer_kron_svd
2025-04-28 02:07:04,041 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 02:07:05,078 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 02:07:06,141 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 02:07:07,185 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 02:07:08,240 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 02:07:09,511 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-28 02:07:10,548 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-28 02:07:11,597 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-28 02:07:12,652 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-28 02:07:13,704 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-28 02:07:14,758 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-28 02:07:15,955 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_28_self_attn_k_proj.safetensors True
Layer: model.layers.28.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.3403688e-05 6.0787293e-06 5.6532840e-06 5.4129196e-06 5.2258588e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-28 02:07:42,291 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-28 02:07:42,291 - INFO - Replacing 'model.layers.28.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                    | 198/225 [36:36<04:30, 10.00s/it]2025-04-28 02:07:42,291 - INFO - Skipping layer model.layers.28.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,291 - INFO - Skipping layer model.layers.28.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,291 - INFO - Skipping layer model.layers.28.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,291 - INFO - Skipping layer model.layers.28.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,291 - INFO - Skipping layer model.layers.28.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,291 - INFO - Skipping layer model.layers.29.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,291 - INFO - Skipping layer model.layers.29.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,291 - INFO - Skipping layer model.layers.29.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,291 - INFO - Skipping layer model.layers.29.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,291 - INFO - Skipping layer model.layers.29.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,291 - INFO - Skipping layer model.layers.29.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,291 - INFO - Skipping layer model.layers.29.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,291 - INFO - Skipping layer model.layers.30.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,292 - INFO - Skipping layer model.layers.30.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,292 - INFO - Skipping layer model.layers.30.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,292 - INFO - Skipping layer model.layers.30.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,292 - INFO - Skipping layer model.layers.30.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,292 - INFO - Skipping layer model.layers.30.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,292 - INFO - Skipping layer model.layers.30.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,292 - INFO - Skipping layer model.layers.31.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,292 - INFO - Skipping layer model.layers.31.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,292 - INFO - Skipping layer model.layers.31.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,292 - INFO - Skipping layer model.layers.31.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,292 - INFO - Skipping layer model.layers.31.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,292 - INFO - Skipping layer model.layers.31.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,292 - INFO - Skipping layer model.layers.31.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-28 02:07:42,292 - INFO - Skipping layer lm_head: Sensitivity ratio >= 1.0 (1.00). No compression.
Compressing Layers: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [36:36<00:00,  9.76s/it]
2025-04-28 02:07:42,292 - INFO - Compression finished. Processed: 46, Skipped (Ratio>=1 or No Sensitivity/Factors): 179, Failed: 0
2025-04-28 02:07:42,294 - INFO - Total parameters after compression: 5909364736
2025-04-28 02:07:42,294 - INFO - C rate : 0.8769664966893013
2025-04-28 02:07:42,294 - INFO - Saving compressed model to ./llama10
2025-04-28 02:07:42,294 - INFO - Compressed model and tokenizer saved.
2025-04-28 02:07:42,302 - INFO - Evaluating on wikitext2
Evaluating:   0%|                                                                                                                                                                                          | 0/21 [00:00<?, ?it/s]Evaluating:   5%|████████▍                                                                                                                                                                         | 1/21 [00:04<01:25,  4.30s/it]Evaluating:  10%|████████████████▉                                                                                                                                                                 | 2/21 [00:05<00:46,  2.43s/it]Evaluating:  14%|█████████████████████████▍                                                                                                                                                        | 3/21 [00:06<00:32,  1.83s/it]Evaluating:  19%|█████████████████████████████████▉                                                                                                                                                | 4/21 [00:07<00:26,  1.55s/it]Evaluating:  24%|██████████████████████████████████████████▍                                                                                                                                       | 5/21 [00:08<00:22,  1.40s/it]Evaluating:  29%|██████████████████████████████████████████████████▊                                                                                                                               | 6/21 [00:09<00:19,  1.30s/it]Evaluating:  33%|███████████████████████████████████████████████████████████▎                                                                                                                      | 7/21 [00:11<00:17,  1.24s/it]Evaluating:  38%|███████████████████████████████████████████████████████████████████▊                                                                                                              | 8/21 [00:12<00:15,  1.20s/it]Evaluating:  43%|████████████████████████████████████████████████████████████████████████████▎                                                                                                     | 9/21 [00:13<00:14,  1.18s/it]Evaluating:  48%|████████████████████████████████████████████████████████████████████████████████████▎                                                                                            | 10/21 [00:14<00:12,  1.16s/it]Evaluating:  52%|████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                    | 11/21 [00:15<00:11,  1.15s/it]Evaluating:  57%|█████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                           | 12/21 [00:16<00:10,  1.14s/it]Evaluating:  62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                   | 13/21 [00:17<00:09,  1.13s/it]Evaluating:  67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                           | 14/21 [00:18<00:07,  1.13s/it]Evaluating:  71%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                  | 15/21 [00:19<00:06,  1.13s/it]Evaluating:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                          | 16/21 [00:21<00:05,  1.12s/it]Evaluating:  81%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                 | 17/21 [00:22<00:04,  1.12s/it]Evaluating:  86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                         | 18/21 [00:23<00:03,  1.12s/it]Evaluating:  90%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                | 19/21 [00:24<00:02,  1.16s/it]Evaluating:  95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 20/21 [00:25<00:01,  1.15s/it]Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:26<00:00,  1.06s/it]Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:26<00:00,  1.26s/it]
2025-04-28 02:08:16,878 - INFO - wikitext2 perplexity: 10.2500
2025-04-28 02:08:16,879 - INFO - Evaluating on ptb
nlls.shape torch.Size([16376])
Mean NLL: 2.328125
Evaluating:   0%|                                                                                                                                                                                           | 0/7 [00:00<?, ?it/s]Evaluating:  14%|█████████████████████████▌                                                                                                                                                         | 1/7 [00:01<00:06,  1.12s/it]Evaluating:  29%|███████████████████████████████████████████████████▏                                                                                                                               | 2/7 [00:02<00:05,  1.12s/it]Evaluating:  43%|████████████████████████████████████████████████████████████████████████████▋                                                                                                      | 3/7 [00:03<00:04,  1.12s/it]Evaluating:  57%|██████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                            | 4/7 [00:04<00:03,  1.12s/it]Evaluating:  71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                   | 5/7 [00:05<00:02,  1.12s/it]Evaluating:  86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                         | 6/7 [00:06<00:01,  1.12s/it]Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:07<00:00,  1.08s/it]Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:07<00:00,  1.10s/it]
2025-04-28 02:08:26,873 - INFO - ptb perplexity: 42.5000
2025-04-28 02:08:26,873 - INFO - Evaluation results:
2025-04-28 02:08:26,873 - INFO -   wikitext2: 10.2500
2025-04-28 02:08:26,873 - INFO -   ptb: 42.5000
nlls.shape torch.Size([16376])
Mean NLL: 3.75

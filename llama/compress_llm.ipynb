{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3ce69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from safetensors.torch import load_file\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48536802",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"unsloth/llama-2-7b\" \n",
    "KRON_FACTORS_DIR = \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/grads_output/Llama-3.1-8B-Instruct_32/fisher_factors_output_1404\"\n",
    "OUTPUT_DIR = f\"./{MODEL_NAME.split('/')[-1]}-kron-svd-sensitive\"\n",
    "SENSITIVITY_DICT_PATH = \"layers_min_ratio_unsloth_Llama-3.1-8B-Instruct.json\"\n",
    "RANK_ALIGNMENT = 8\n",
    "MIN_RANK = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "926c7c84-9f13-49ec-8ceb-7b705022b775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4fd2376ad5f4a658abd450b15ae1801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/752 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-21 14:54:47,399] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.mlspace/envs/vika_kurkin_clone/bin/../lib/gcc/x86_64-conda-linux-gnu/12.4.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/jovyan/.mlspace/envs/vika_kurkin_clone/bin/../lib/gcc/x86_64-conda-linux-gnu/12.4.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7a22673afc44b7cba8f3aac35a066cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9603fb5322aa483087f1f2812cf998c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2fb159c4bb34cfe93f1a4e4528874ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a039bbb36f43438daa4d4e4a88fa89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c91de84e30794fcd95ab5992c54e7d41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e989ba0491674e66b701862c84de5b44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b846433fad654aa68af277caaa02d276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        low_cpu_mem_usage=True,\n",
    "        attn_implementation=\"flash_attention_2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4534f5-65b7-44ab-8b6b-d21664f7d99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.layers[24].mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "862284ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
      ")\n",
      "model LlamaModel(\n",
      "  (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
      "  (layers): ModuleList(\n",
      "    (0-31): 32 x LlamaDecoderLayer(\n",
      "      (self_attn): LlamaAttention(\n",
      "        (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "        (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "        (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      )\n",
      "      (mlp): LlamaMLP(\n",
      "        (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "        (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "        (act_fn): SiLU()\n",
      "      )\n",
      "      (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "      (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    )\n",
      "  )\n",
      "  (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (rotary_emb): LlamaRotaryEmbedding()\n",
      ")\n",
      "model.embed_tokens Embedding(128256, 4096, padding_idx=128004)\n",
      "model.layers ModuleList(\n",
      "  (0-31): 32 x LlamaDecoderLayer(\n",
      "    (self_attn): LlamaAttention(\n",
      "      (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "      (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "      (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    )\n",
      "    (mlp): LlamaMLP(\n",
      "      (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "      (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "      (act_fn): SiLU()\n",
      "    )\n",
      "    (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "    (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  )\n",
      ")\n",
      "model.layers.0 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.0.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.0.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.0.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.0.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.0.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.0.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.0.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.0.mlp.act_fn SiLU()\n",
      "model.layers.0.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.0.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.1 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.1.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.1.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.1.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.1.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.1.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.1.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.1.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.1.mlp.act_fn SiLU()\n",
      "model.layers.1.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.1.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.2 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.2.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.2.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.2.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.2.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.2.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.2.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.2.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.2.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.2.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.2.mlp.act_fn SiLU()\n",
      "model.layers.2.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.2.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.3 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.3.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.3.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.3.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.3.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.3.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.3.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.3.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.3.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.3.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.3.mlp.act_fn SiLU()\n",
      "model.layers.3.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.3.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.4 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.4.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.4.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.4.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.4.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.4.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.4.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.4.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.4.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.4.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.4.mlp.act_fn SiLU()\n",
      "model.layers.4.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.4.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.5 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.5.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.5.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.5.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.5.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.5.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.5.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.5.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.5.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.5.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.5.mlp.act_fn SiLU()\n",
      "model.layers.5.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.5.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.6 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.6.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.6.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.6.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.6.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.6.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.6.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.6.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.6.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.6.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.6.mlp.act_fn SiLU()\n",
      "model.layers.6.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.6.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.7 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.7.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.7.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.7.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.7.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.7.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.7.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.7.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.7.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.7.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.7.mlp.act_fn SiLU()\n",
      "model.layers.7.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.7.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.8 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.8.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.8.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.8.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.8.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.8.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.8.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.8.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.8.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.8.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.8.mlp.act_fn SiLU()\n",
      "model.layers.8.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.8.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.9 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.9.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.9.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.9.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.9.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.9.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.9.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.9.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.9.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.9.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.9.mlp.act_fn SiLU()\n",
      "model.layers.9.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.9.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.10 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.10.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.10.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.10.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.10.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.10.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.10.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.10.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.10.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.10.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.10.mlp.act_fn SiLU()\n",
      "model.layers.10.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.10.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.11 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.11.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.11.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.11.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.11.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.11.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.11.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.11.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.11.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.11.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.11.mlp.act_fn SiLU()\n",
      "model.layers.11.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.11.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.12 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.12.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.12.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.12.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.12.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.12.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.12.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.12.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.12.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.12.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.12.mlp.act_fn SiLU()\n",
      "model.layers.12.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.12.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.13 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.13.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.13.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.13.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.13.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.13.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.13.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.13.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.13.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.13.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.13.mlp.act_fn SiLU()\n",
      "model.layers.13.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.13.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.14 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.14.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.14.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.14.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.14.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.14.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.14.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.14.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.14.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.14.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.14.mlp.act_fn SiLU()\n",
      "model.layers.14.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.14.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.15 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.15.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.15.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.15.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.15.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.15.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.15.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.15.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.15.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.15.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.15.mlp.act_fn SiLU()\n",
      "model.layers.15.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.15.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.16 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.16.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.16.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.16.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.16.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.16.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.16.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.16.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.16.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.16.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.16.mlp.act_fn SiLU()\n",
      "model.layers.16.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.16.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.17 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.17.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.17.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.17.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.17.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.17.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.17.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.17.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.17.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.17.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.17.mlp.act_fn SiLU()\n",
      "model.layers.17.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.17.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.18 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.18.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.18.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.18.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.18.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.18.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.18.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.18.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.18.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.18.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.18.mlp.act_fn SiLU()\n",
      "model.layers.18.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.18.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.19 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.19.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.19.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.19.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.19.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.19.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.19.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.19.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.19.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.19.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.19.mlp.act_fn SiLU()\n",
      "model.layers.19.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.19.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.20 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.20.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.20.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.20.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.20.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.20.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.20.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.20.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.20.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.20.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.20.mlp.act_fn SiLU()\n",
      "model.layers.20.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.20.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.21 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.21.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.21.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.21.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.21.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.21.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.21.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.21.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.21.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.21.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.21.mlp.act_fn SiLU()\n",
      "model.layers.21.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.21.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.22 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.22.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.22.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.22.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.22.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.22.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.22.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.22.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.22.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.22.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.22.mlp.act_fn SiLU()\n",
      "model.layers.22.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.22.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.23 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.23.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.23.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.23.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.23.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.23.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.23.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.23.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.23.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.23.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.23.mlp.act_fn SiLU()\n",
      "model.layers.23.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.23.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.24 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.24.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.24.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.24.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.24.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.24.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.24.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.24.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.24.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.24.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.24.mlp.act_fn SiLU()\n",
      "model.layers.24.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.24.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.25 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.25.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.25.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.25.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.25.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.25.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.25.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.25.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.25.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.25.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.25.mlp.act_fn SiLU()\n",
      "model.layers.25.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.25.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.26 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.26.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.26.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.26.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.26.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.26.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.26.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.26.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.26.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.26.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.26.mlp.act_fn SiLU()\n",
      "model.layers.26.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.26.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.27 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.27.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.27.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.27.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.27.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.27.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.27.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.27.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.27.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.27.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.27.mlp.act_fn SiLU()\n",
      "model.layers.27.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.27.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.28 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.28.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.28.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.28.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.28.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.28.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.28.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.28.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.28.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.28.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.28.mlp.act_fn SiLU()\n",
      "model.layers.28.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.28.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.29 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.29.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.29.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.29.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.29.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.29.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.29.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.29.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.29.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.29.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.29.mlp.act_fn SiLU()\n",
      "model.layers.29.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.29.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.30 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.30.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.30.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.30.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.30.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.30.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.30.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.30.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.30.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.30.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.30.mlp.act_fn SiLU()\n",
      "model.layers.30.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.30.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.31 LlamaDecoderLayer(\n",
      "  (self_attn): LlamaAttention(\n",
      "    (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "    (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "    (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  )\n",
      "  (mlp): LlamaMLP(\n",
      "    (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "    (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "    (act_fn): SiLU()\n",
      "  )\n",
      "  (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      "  (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
      ")\n",
      "model.layers.31.self_attn LlamaAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "  (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      ")\n",
      "model.layers.31.self_attn.q_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.31.self_attn.k_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.31.self_attn.v_proj Linear(in_features=4096, out_features=1024, bias=False)\n",
      "model.layers.31.self_attn.o_proj Linear(in_features=4096, out_features=4096, bias=False)\n",
      "model.layers.31.mlp LlamaMLP(\n",
      "  (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "  (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "  (act_fn): SiLU()\n",
      ")\n",
      "model.layers.31.mlp.gate_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.31.mlp.up_proj Linear(in_features=4096, out_features=14336, bias=False)\n",
      "model.layers.31.mlp.down_proj Linear(in_features=14336, out_features=4096, bias=False)\n",
      "model.layers.31.mlp.act_fn SiLU()\n",
      "model.layers.31.input_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.layers.31.post_attention_layernorm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.norm LlamaRMSNorm((4096,), eps=1e-05)\n",
      "model.rotary_emb LlamaRotaryEmbedding()\n",
      "lm_head Linear(in_features=4096, out_features=128256, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "    print (name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d340d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-17 16:53:01,723] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/shares/SR004.nfs2/kurkin/envs/kurkin_312_torch/bin/../lib/gcc/x86_64-conda-linux-gnu/12.4.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/jovyan/shares/SR004.nfs2/kurkin/envs/kurkin_312_torch/bin/../lib/gcc/x86_64-conda-linux-gnu/12.4.0/../../../../x86_64-conda-linux-gnu/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94d102200ed4348b112535eb98e6e2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:   0%|          | 0/225 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.3913357e-05 1.7242742e-06 4.9078540e-07 4.2490385e-07 3.5237093e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:   0%|          | 1/225 [00:27<1:42:18, 27.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [6.04108063e-05 2.99886068e-07 1.47158929e-07 1.06830811e-07\n",
      " 1.00799795e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:   1%|          | 2/225 [00:35<1:00:30, 16.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [7.765625e-03 2.720158e-05 1.058611e-05 9.339088e-06 8.808340e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:   2%|         | 4/225 [01:09<1:00:54, 16.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [7.3913147e-04 8.8265506e-06 1.4363935e-06 1.3075712e-06 1.2645786e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:   2%|         | 5/225 [02:41<2:26:34, 39.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1912,4096), (14336,1912)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.75697810e-03 1.16400815e-05 5.33070261e-06 4.91619039e-06\n",
      " 4.81952202e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:   3%|         | 6/225 [04:30<3:42:42, 61.02s/it]ERROR:root:Failed to process layer model.layers.0.mlp.down_proj: X factor shape mismatch for layer: (4096, 4096)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 332, in compress_llama_with_kron_svd\n",
      "    factorized_sequential = factorize_layer_kron_svd(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 118, in factorize_layer_kron_svd\n",
      "    assert XF.shape == (in_features, in_features), f\"X factor shape mismatch for layer: {XF.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: X factor shape mismatch for layer: (4096, 4096)\n",
      "Compressing Layers:   3%|         | 7/225 [04:30<2:34:22, 42.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2232,4096), (14336,2232)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [4.0573500e-06 2.4267133e-06 2.3514731e-06 2.2389013e-06 2.1487167e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:   4%|         | 8/225 [05:07<2:27:50, 40.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.4850119e-05 2.2664661e-07 9.7887778e-08 7.0136750e-08 6.8677132e-08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:   4%|         | 9/225 [05:16<1:52:31, 31.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.2424537e-04 1.6857345e-07 5.1099189e-08 2.7695034e-08 2.3854444e-08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:   4%|         | 10/225 [05:25<1:28:10, 24.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (656,4096), (1024,656)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.4069737e-02 8.8947227e-06 4.7645585e-06 4.6165242e-06 4.3493064e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:   5%|         | 11/225 [06:01<1:39:26, 27.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [7.3860206e-06 5.0340414e-06 4.9354849e-06 4.6487503e-06 4.6165842e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:   5%|         | 12/225 [07:59<3:15:13, 54.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.4876737e-04 1.4466496e-05 1.7380321e-06 1.5394163e-06 1.4848266e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:   6%|         | 13/225 [09:51<4:15:15, 72.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.15847615e-05 2.67572364e-06 1.45946686e-07 8.72775416e-08\n",
      " 8.09087197e-08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:   7%|         | 15/225 [10:22<2:41:10, 46.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [7.1933036e-06 4.5960346e-06 4.1855974e-06 4.1287244e-06 4.0394621e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:   7%|         | 16/225 [10:37<2:13:19, 38.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (576,4096), (1024,576)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.3279069e-04 3.9071138e-06 7.7221688e-07 6.8958275e-07 6.7742320e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:   8%|         | 18/225 [11:14<1:43:26, 29.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [5.5701454e-05 8.6610034e-06 7.6118312e-07 6.9075980e-07 6.5034993e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:   8%|         | 19/225 [13:12<2:52:03, 50.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2552,4096), (14336,2552)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.3302265e-06 2.9064659e-06 2.6803789e-06 2.6100731e-06 2.5215538e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:   9%|         | 20/225 [15:11<3:49:39, 67.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.0121101e-05 5.8944120e-06 5.7017191e-06 5.5523510e-06 5.5293999e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  10%|         | 22/225 [15:54<2:43:08, 48.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.6887825e-05 7.3859621e-07 1.9830341e-07 7.2988286e-08 6.6038140e-08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  10%|         | 23/225 [16:02<2:12:05, 39.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (416,4096), (1024,416)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [4.2968104e-06 2.2877684e-06 2.1255400e-06 2.0782311e-06 2.0486386e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  11%|         | 25/225 [16:44<1:46:29, 31.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.2412295e-04 2.1352221e-06 5.4680953e-07 4.5569459e-07 4.3034709e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  12%|        | 26/225 [18:37<2:45:03, 49.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2232,4096), (14336,2232)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process layer model.layers.3.mlp.down_proj: X factor shape mismatch for layer: (4096, 4096)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 332, in compress_llama_with_kron_svd\n",
      "    factorized_sequential = factorize_layer_kron_svd(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 118, in factorize_layer_kron_svd\n",
      "    assert XF.shape == (in_features, in_features), f\"X factor shape mismatch for layer: {XF.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: X factor shape mismatch for layer: (4096, 4096)\n",
      "Compressing Layers:  12%|        | 28/225 [18:37<1:40:31, 30.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.1211680e-06 1.9037565e-06 1.8208422e-06 1.7668757e-06 1.6697911e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  13%|        | 29/225 [19:18<1:47:04, 32.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [7.6100459e-06 4.9955133e-06 4.7773005e-06 4.5978049e-06 4.3930318e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  13%|        | 30/225 [19:31<1:31:52, 28.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.8982558e-05 4.1674939e-06 2.4723522e-07 2.1083511e-07 1.0034591e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  14%|        | 31/225 [19:41<1:15:56, 23.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (744,4096), (1024,744)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.6670751e-06 2.4026754e-06 2.2956667e-06 2.2675010e-06 2.1940784e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  14%|        | 32/225 [20:23<1:31:07, 28.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [6.4166823e-05 3.6984866e-06 4.9786757e-07 4.5706074e-07 4.2268573e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  15%|        | 33/225 [22:02<2:32:41, 47.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [7.8022640e-06 3.6230565e-06 1.2765768e-06 6.3743050e-07 2.5449199e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  16%|        | 36/225 [22:34<1:27:44, 27.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.0710308e-05 1.2675753e-06 1.1219739e-06 7.6275876e-07 6.5961780e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  16%|        | 37/225 [22:44<1:15:24, 24.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.0185989e-06 9.9370016e-07 9.8565977e-07 9.5739858e-07 9.5201278e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  17%|        | 38/225 [22:57<1:07:22, 21.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (496,4096), (1024,496)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.9316314e-05 2.1083324e-06 1.6686353e-07 1.4161340e-07 1.3852670e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  17%|        | 39/225 [23:25<1:12:11, 23.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [4.5166802e-05 1.3911108e-06 1.5688876e-07 1.4384381e-07 1.2973608e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  18%|        | 40/225 [25:02<2:09:56, 42.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1912,4096), (14336,1912)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.8553746e-05 2.2738075e-06 1.8368956e-07 1.7166386e-07 1.5942901e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  18%|        | 41/225 [26:23<2:41:58, 52.82s/it]ERROR:root:Failed to process layer model.layers.5.mlp.down_proj: X factor shape mismatch for layer: (4096, 4096)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 332, in compress_llama_with_kron_svd\n",
      "    factorized_sequential = factorize_layer_kron_svd(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 118, in factorize_layer_kron_svd\n",
      "    assert XF.shape == (in_features, in_features), f\"X factor shape mismatch for layer: {XF.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: X factor shape mismatch for layer: (4096, 4096)\n",
      "Compressing Layers:  19%|        | 42/225 [26:24<1:56:24, 38.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.9726707e-06 1.7816552e-06 1.6897205e-06 1.6249726e-06 1.5741786e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  19%|        | 43/225 [26:58<1:52:22, 37.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.3031284e-05 1.6375805e-06 9.0260573e-07 8.3924448e-07 5.0392083e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  20%|        | 44/225 [27:06<1:26:43, 28.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.1052168e-06 1.0974492e-06 1.0523607e-06 1.0425352e-06 1.0325920e-06]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  20%|        | 45/225 [27:19<1:12:12, 24.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (576,4096), (1024,576)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.1236058e-04 1.2615281e-07 1.1651123e-07 7.9070631e-08 6.9290692e-08]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  20%|        | 46/225 [27:46<1:14:26, 24.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [6.1322680e-05 2.4558008e-06 3.4849720e-07 3.2880027e-07 3.1275533e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  21%|        | 47/225 [29:15<2:10:14, 43.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2232,4096), (14336,2232)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.3121511e-05 2.3641480e-06 1.6719655e-07 1.5843588e-07 1.5415122e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  21%|       | 48/225 [30:36<2:42:31, 55.09s/it]ERROR:root:Failed to process layer model.layers.6.mlp.down_proj: X factor shape mismatch for layer: (4096, 4096)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 332, in compress_llama_with_kron_svd\n",
      "    factorized_sequential = factorize_layer_kron_svd(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 118, in factorize_layer_kron_svd\n",
      "    assert XF.shape == (in_features, in_features), f\"X factor shape mismatch for layer: {XF.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: X factor shape mismatch for layer: (4096, 4096)\n",
      "Compressing Layers:  22%|       | 49/225 [30:37<1:53:33, 38.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [6.8571226e-06 2.3478881e-06 1.9948027e-06 3.9922924e-07 2.7595888e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  22%|       | 50/225 [31:05<1:44:06, 35.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [8.4509236e-07 5.3105333e-07 4.9607002e-07 4.7299801e-07 4.5661091e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  23%|       | 51/225 [31:15<1:21:28, 28.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.2939703e-05 9.2121718e-07 7.2539842e-07 4.0783070e-07 3.2835510e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  23%|       | 52/225 [31:24<1:03:49, 22.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (576,4096), (1024,576)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.36856061e-05 1.60714671e-06 1.07695016e-07 9.86864350e-08\n",
      " 9.39998159e-08]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  24%|       | 53/225 [31:51<1:08:17, 23.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.4806213e-05 1.2553639e-06 1.6335601e-07 1.3818330e-07 1.3086232e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  24%|       | 54/225 [33:13<1:56:53, 41.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2232,4096), (14336,2232)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.5064608e-05 1.3422383e-06 1.7232266e-07 1.5628012e-07 1.4820436e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  24%|       | 55/225 [34:34<2:30:12, 53.02s/it]ERROR:root:Failed to process layer model.layers.7.mlp.down_proj: X factor shape mismatch for layer: (4096, 4096)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 332, in compress_llama_with_kron_svd\n",
      "    factorized_sequential = factorize_layer_kron_svd(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 118, in factorize_layer_kron_svd\n",
      "    assert XF.shape == (in_features, in_features), f\"X factor shape mismatch for layer: {XF.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: X factor shape mismatch for layer: (4096, 4096)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2552,4096), (14336,2552)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  25%|       | 56/225 [34:34<1:44:41, 37.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [4.7043904e-06 3.2668572e-06 6.1526026e-07 4.6986409e-07 3.1718983e-07]\n",
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  25%|       | 57/225 [35:02<1:36:14, 34.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.1029374e-05 3.0178451e-06 9.6892006e-07 8.6262685e-07 5.9960115e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  26%|       | 58/225 [35:10<1:13:57, 26.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [7.5041226e-06 1.0899640e-06 7.5002663e-07 5.7133821e-07 3.7502215e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  26%|       | 59/225 [35:18<58:25, 21.12s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (416,4096), (1024,416)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.1876423e-05 3.8990575e-07 1.1115482e-07 6.6238442e-08 6.3719277e-08]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  27%|       | 60/225 [35:45<1:03:00, 22.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [4.2187501e-05 2.0450329e-06 2.6382773e-07 2.3273246e-07 2.0389123e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  27%|       | 61/225 [37:14<1:56:03, 42.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1912,4096), (14336,1912)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [4.4588851e-05 8.1474786e-07 2.5987657e-07 1.6190300e-07 1.5439277e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  28%|       | 62/225 [38:39<2:30:00, 55.22s/it]ERROR:root:Failed to process layer model.layers.8.mlp.down_proj: X factor shape mismatch for layer: (4096, 4096)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 332, in compress_llama_with_kron_svd\n",
      "    factorized_sequential = factorize_layer_kron_svd(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 118, in factorize_layer_kron_svd\n",
      "    assert XF.shape == (in_features, in_features), f\"X factor shape mismatch for layer: {XF.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: X factor shape mismatch for layer: (4096, 4096)\n",
      "Compressing Layers:  28%|       | 63/225 [38:39<1:44:28, 38.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2552,4096), (14336,2552)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [6.9902567e-06 1.3183835e-06 4.5598080e-07 2.6395566e-07 1.8620581e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  28%|       | 64/225 [39:07<1:35:24, 35.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.1487909e-05 9.0624928e-07 4.9679744e-07 3.0565243e-07 2.4892046e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  29%|       | 65/225 [39:15<1:13:08, 27.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.3576881e-06 1.3274503e-06 1.3005410e-06 1.2903046e-06 1.2829223e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  29%|       | 66/225 [39:28<1:01:15, 23.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (496,4096), (1024,496)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.3412405e-06 2.4729961e-06 2.4560222e-06 2.4520195e-06 2.4336491e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  30%|       | 67/225 [40:05<1:11:24, 27.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.1240343e-05 1.0061859e-06 1.9563147e-07 1.6141206e-07 1.4020932e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  30%|       | 68/225 [41:29<1:55:30, 44.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2232,4096), (14336,2232)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.2245191e-05 1.4897820e-06 2.6284872e-07 1.6324944e-07 1.3938499e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  31%|       | 69/225 [42:53<2:25:44, 56.05s/it]ERROR:root:Failed to process layer model.layers.9.mlp.down_proj: X factor shape mismatch for layer: (4096, 4096)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 332, in compress_llama_with_kron_svd\n",
      "    factorized_sequential = factorize_layer_kron_svd(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 118, in factorize_layer_kron_svd\n",
      "    assert XF.shape == (in_features, in_features), f\"X factor shape mismatch for layer: {XF.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: X factor shape mismatch for layer: (4096, 4096)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2552,4096), (14336,2552)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  31%|       | 70/225 [42:53<1:41:29, 39.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [4.6364139e-06 7.5621261e-07 3.6786443e-07 2.5630590e-07 2.0471276e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  32%|      | 71/225 [43:21<1:32:23, 35.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [8.5243319e-06 6.4524005e-07 5.0477797e-07 4.5106651e-07 4.1702094e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  32%|      | 72/225 [43:30<1:10:44, 27.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [7.2853882e-06 1.1126355e-06 4.2909849e-07 2.4130483e-07 1.6436053e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  32%|      | 73/225 [43:38<55:46, 22.02s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (416,4096), (1024,416)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.1845293e-05 3.6575784e-07 1.2655242e-07 6.0418913e-08 5.6642854e-08]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  33%|      | 74/225 [44:07<1:00:22, 23.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.3479515e-05 1.2020918e-06 2.0998536e-07 1.6604510e-07 1.4730149e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  33%|      | 75/225 [45:29<1:43:16, 41.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1912,4096), (14336,1912)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.0217934e-05 1.7874731e-06 3.2261636e-07 1.7874061e-07 1.5347734e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  34%|      | 76/225 [46:52<2:13:44, 53.86s/it]ERROR:root:Failed to process layer model.layers.10.mlp.down_proj: X factor shape mismatch for layer: (4096, 4096)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 332, in compress_llama_with_kron_svd\n",
      "    factorized_sequential = factorize_layer_kron_svd(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 118, in factorize_layer_kron_svd\n",
      "    assert XF.shape == (in_features, in_features), f\"X factor shape mismatch for layer: {XF.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: X factor shape mismatch for layer: (4096, 4096)\n",
      "Compressing Layers:  34%|      | 77/225 [46:52<1:33:04, 37.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2552,4096), (14336,2552)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [8.1535705e-07 5.4725899e-07 5.2441050e-07 4.9454593e-07 4.8770920e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  35%|      | 78/225 [47:25<1:29:24, 36.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.7704660e-05 5.7331276e-06 1.4609728e-06 1.0420400e-06 9.6980511e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  35%|      | 79/225 [47:34<1:08:21, 28.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.2875901e-06 1.2538085e-06 1.2357125e-06 1.2080054e-06 1.1965182e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  36%|      | 80/225 [47:47<56:46, 23.49s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (416,4096), (1024,416)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.1273098e-06 2.8277670e-06 2.6110361e-06 2.6058383e-06 2.5346149e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  36%|      | 81/225 [48:22<1:05:17, 27.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [4.0129460e-05 6.7763148e-07 2.6699323e-07 1.9514266e-07 1.5081541e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  36%|      | 82/225 [49:43<1:42:53, 43.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1600,4096), (14336,1600)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.5884297e-05 1.3562785e-06 3.5441525e-07 1.7331101e-07 1.4553670e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  37%|      | 83/225 [51:06<2:10:32, 55.16s/it]ERROR:root:Failed to process layer model.layers.11.mlp.down_proj: X factor shape mismatch for layer: (4096, 4096)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 332, in compress_llama_with_kron_svd\n",
      "    factorized_sequential = factorize_layer_kron_svd(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 118, in factorize_layer_kron_svd\n",
      "    assert XF.shape == (in_features, in_features), f\"X factor shape mismatch for layer: {XF.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: X factor shape mismatch for layer: (4096, 4096)\n",
      "Compressing Layers:  37%|      | 84/225 [51:06<1:30:49, 38.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2552,4096), (14336,2552)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.0041748e-06 6.1163377e-07 5.8747850e-07 5.7309438e-07 5.3814267e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  38%|      | 85/225 [51:40<1:26:35, 37.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.5366051e-05 9.9090983e-07 7.2874298e-07 5.0716983e-07 4.4647416e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  38%|      | 86/225 [51:48<1:06:03, 28.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [8.1079361e-06 9.2402036e-07 2.0830602e-07 1.8244347e-07 1.5839831e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  39%|      | 87/225 [51:57<51:44, 22.50s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (576,4096), (1024,576)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.2402982e-05 2.7186911e-07 2.2252716e-07 9.7238171e-08 7.9089219e-08]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  39%|      | 88/225 [52:25<55:04, 24.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.5184836e-05 1.6971719e-06 3.2399529e-07 2.3094488e-07 1.7241355e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  40%|      | 89/225 [53:54<1:39:20, 43.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2232,4096), (14336,2232)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.8518003e-05 2.4961646e-06 3.7014203e-07 1.8088545e-07 1.7998568e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  40%|      | 90/225 [55:25<2:10:21, 57.94s/it]ERROR:root:Failed to process layer model.layers.12.mlp.down_proj: X factor shape mismatch for layer: (4096, 4096)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 332, in compress_llama_with_kron_svd\n",
      "    factorized_sequential = factorize_layer_kron_svd(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 118, in factorize_layer_kron_svd\n",
      "    assert XF.shape == (in_features, in_features), f\"X factor shape mismatch for layer: {XF.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: X factor shape mismatch for layer: (4096, 4096)\n",
      "Compressing Layers:  40%|      | 91/225 [55:25<1:30:39, 40.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [5.6297872e-06 2.4681822e-07 1.7562114e-07 1.5922861e-07 1.5128786e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  41%|      | 92/225 [55:54<1:22:03, 37.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [8.7196022e-06 2.1267374e-06 4.5305012e-07 3.3118707e-07 2.2178149e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  41%|     | 93/225 [56:03<1:02:44, 28.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [6.5259710e-06 3.7757778e-07 3.0459930e-07 1.4193184e-07 1.0412596e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  42%|     | 94/225 [56:11<49:14, 22.55s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (496,4096), (1024,496)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [6.3605832e-05 3.3667217e-07 9.9502252e-08 8.6842213e-08 7.4513395e-08]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  42%|     | 95/225 [56:40<52:36, 24.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.5590987e-05 1.8200055e-06 3.1952990e-07 2.4025107e-07 1.7612889e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  43%|     | 96/225 [58:09<1:34:27, 43.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1912,4096), (14336,1912)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.8767796e-05 1.9912927e-06 3.7221648e-07 1.7357078e-07 1.7213611e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  43%|     | 97/225 [59:39<2:02:46, 57.55s/it]ERROR:root:Failed to process layer model.layers.13.mlp.down_proj: X factor shape mismatch for layer: (4096, 4096)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 332, in compress_llama_with_kron_svd\n",
      "    factorized_sequential = factorize_layer_kron_svd(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 118, in factorize_layer_kron_svd\n",
      "    assert XF.shape == (in_features, in_features), f\"X factor shape mismatch for layer: {XF.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: X factor shape mismatch for layer: (4096, 4096)\n",
      "Compressing Layers:  44%|     | 98/225 [59:39<1:25:20, 40.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [4.1227040e-06 1.0368565e-07 8.2475800e-08 7.8233143e-08 7.7941024e-08]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  44%|     | 99/225 [1:00:07<1:16:57, 36.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [7.2241426e-07 5.3243195e-07 4.9279953e-07 4.6152903e-07 4.5649224e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  44%|     | 100/225 [1:00:18<1:00:08, 28.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (576,4096), (1024,576)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [9.16233512e-06 2.99283442e-07 1.50514751e-07 1.24478362e-07\n",
      " 1.07031404e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  45%|     | 101/225 [1:00:26<46:56, 22.71s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (576,4096), (1024,576)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [9.0669646e-06 6.0937447e-07 1.3761112e-07 7.7136420e-08 6.1648635e-08]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  45%|     | 102/225 [1:00:55<50:11, 24.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.3713916e-05 1.8989813e-06 3.5352232e-07 2.8988291e-07 2.1535509e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  46%|     | 103/225 [1:02:25<1:30:02, 44.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2232,4096), (14336,2232)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.0815041e-05 2.2468664e-06 4.7540451e-07 2.2051165e-07 2.1119391e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  46%|     | 104/225 [1:03:54<1:56:17, 57.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [9.9068632e-07 5.5864308e-07 5.4813864e-07 5.3237727e-07 5.1565712e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  47%|     | 106/225 [1:04:27<1:16:47, 38.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [7.6911397e-07 4.7309695e-07 4.4360647e-07 4.4251942e-07 4.2027781e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  48%|     | 107/225 [1:04:38<1:02:41, 31.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.07889828e-05 4.11451651e-07 1.97244319e-07 1.06750086e-07\n",
      " 9.30309696e-08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  48%|     | 108/225 [1:04:47<50:09, 25.72s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (576,4096), (1024,576)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.0797943e-05 5.3656368e-07 2.1091617e-07 1.0512577e-07 9.4699843e-08]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  48%|     | 109/225 [1:05:15<51:07, 26.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.9062032e-05 7.3320462e-07 3.8570610e-07 3.1534151e-07 3.0276610e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  49%|     | 110/225 [1:06:37<1:20:37, 42.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1912,4096), (14336,1912)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.8817056e-05 1.9585650e-06 4.2704050e-07 2.8289435e-07 2.6315175e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  49%|     | 111/225 [1:08:07<1:45:56, 55.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [9.8810574e-07 5.2491282e-07 5.0008987e-07 4.8632000e-07 4.7113593e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  50%|     | 113/225 [1:08:47<1:14:04, 39.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.5846555e-07 1.5813235e-07 1.4059222e-07 1.3221327e-07 1.2989875e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  51%|     | 114/225 [1:08:58<1:00:20, 32.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [6.4298420e-06 3.7534809e-07 1.9333086e-07 1.8645999e-07 1.4572664e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  51%|     | 115/225 [1:09:06<48:18, 26.35s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (576,4096), (1024,576)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.5581125e-05 3.2382724e-07 1.2907564e-07 1.0053136e-07 9.8209725e-08]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  52%|    | 116/225 [1:09:35<48:54, 26.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.0546720e-05 7.4719770e-07 4.3448642e-07 3.9369590e-07 3.6864756e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  52%|    | 117/225 [1:10:57<1:16:16, 42.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2552,4096), (14336,2552)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.8372489e-05 9.4182417e-07 4.4794976e-07 3.7802153e-07 3.5331632e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  52%|    | 118/225 [1:12:43<1:47:26, 60.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.0308228e-06 5.4527550e-07 5.3334759e-07 5.0896699e-07 4.9510339e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  53%|    | 120/225 [1:13:23<1:13:53, 42.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.3743807e-07 1.5527991e-07 1.4575858e-07 1.4406957e-07 1.4179162e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  54%|    | 121/225 [1:13:33<59:43, 34.46s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [8.0495893e-06 3.5336308e-07 2.3483788e-07 2.2873834e-07 1.7689320e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  54%|    | 122/225 [1:13:42<47:55, 27.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (656,4096), (1024,656)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [8.8951829e-06 4.2035816e-07 2.0041014e-07 1.2741252e-07 1.1579728e-07]\n",
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  55%|    | 123/225 [1:14:16<50:28, 29.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.4048759e-05 7.1804971e-07 5.4484622e-07 5.3347986e-07 4.6641148e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  55%|    | 124/225 [1:16:01<1:25:09, 50.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2552,4096), (14336,2552)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.1190288e-07 1.5400775e-07 1.4897060e-07 1.4643136e-07 1.4577330e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  56%|    | 127/225 [1:16:40<49:04, 30.05s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.4776733e-07 1.4789019e-07 1.3800116e-07 1.3501983e-07 1.3454851e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  57%|    | 128/225 [1:16:50<42:03, 26.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.7799438e-07 3.6829763e-07 3.6658332e-07 3.6295938e-07 3.5967014e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  57%|    | 129/225 [1:17:03<36:44, 22.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (496,4096), (1024,496)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [6.6140910e-06 5.0395323e-07 1.6822588e-07 1.2804182e-07 1.2369441e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  58%|    | 130/225 [1:17:37<40:50, 25.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.2831697e-05 7.2855318e-07 6.3853878e-07 6.2097143e-07 5.3156037e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  58%|    | 131/225 [1:19:24<1:13:20, 46.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2552,4096), (14336,2552)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.1255137e-05 9.4211799e-07 5.6595132e-07 5.2183066e-07 4.8230567e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  59%|    | 132/225 [1:21:11<1:37:38, 63.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [9.8072883e-07 4.8114038e-07 4.6581226e-07 4.5642955e-07 4.5103010e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  60%|    | 134/225 [1:21:51<1:07:11, 44.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.2707687e-07 1.3629132e-07 1.3352293e-07 1.2451301e-07 1.2247327e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  60%|    | 135/225 [1:22:00<53:53, 35.93s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.0481819e-05 3.7995866e-07 3.2789197e-07 2.4003100e-07 2.0793033e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  60%|    | 136/225 [1:22:08<42:59, 28.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (416,4096), (1024,416)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [8.3692685e-06 4.5889172e-07 2.1954243e-07 1.5071394e-07 1.4150386e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  61%|    | 137/225 [1:22:43<44:28, 30.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.1668624e-05 9.0146068e-07 7.0593609e-07 6.5428151e-07 5.8524597e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  61%|   | 138/225 [1:24:24<1:12:10, 49.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2552,4096), (14336,2552)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.7206467e-05 1.2500113e-06 5.0933153e-07 4.5227495e-07 4.4631869e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  62%|   | 139/225 [1:26:14<1:35:44, 66.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.9883577e-07 1.4050705e-07 1.3895132e-07 1.3736538e-07 1.3148565e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  63%|   | 141/225 [1:26:51<1:03:35, 45.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.4121044e-05 3.1580130e-06 1.9984732e-06 1.3095345e-06 5.8281000e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  63%|   | 142/225 [1:27:01<50:41, 36.64s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [4.1238789e-07 3.9624220e-07 3.8291455e-07 3.7679757e-07 3.7407833e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  64%|   | 143/225 [1:27:13<41:33, 30.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [7.0056672e-06 3.9702249e-07 1.8631299e-07 1.4431481e-07 1.4102812e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  64%|   | 144/225 [1:27:47<42:31, 31.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.8759490e-06 1.6225675e-06 1.4802374e-06 1.4278733e-06 1.4142092e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  64%|   | 145/225 [1:29:49<1:15:06, 56.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2552,4096), (14336,2552)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.9839563e-05 8.1692929e-07 5.7270654e-07 5.3893433e-07 4.9899921e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  65%|   | 146/225 [1:31:33<1:32:10, 70.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [4.0102950e-06 1.2335194e-07 7.9989114e-08 7.7539198e-08 6.8255822e-08]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  66%|   | 148/225 [1:32:07<59:24, 46.30s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [6.4493765e-06 6.6421063e-07 3.4570914e-07 2.6972654e-07 2.6419096e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  66%|   | 149/225 [1:32:16<47:02, 37.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [4.1980860e-07 4.0216875e-07 3.9609628e-07 3.9263989e-07 3.8776969e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  67%|   | 150/225 [1:32:27<38:16, 30.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (496,4096), (1024,496)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [9.4637553e-06 3.5181742e-07 1.9908775e-07 1.5802713e-07 1.5544572e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  67%|   | 151/225 [1:33:02<38:56, 31.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.7376544e-06 1.7036406e-06 1.4424535e-06 1.4353727e-06 1.3951129e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  68%|   | 152/225 [1:35:04<1:09:00, 56.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.8512404e-05 9.2742107e-07 5.5983105e-07 5.2455511e-07 4.9237781e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  68%|   | 153/225 [1:36:29<1:17:52, 64.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.8662134e-07 1.3811584e-07 1.2979653e-07 1.2755874e-07 1.2430553e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  69%|   | 155/225 [1:37:02<50:15, 43.08s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.1133627e-07 1.6080526e-07 1.4022361e-07 1.2480029e-07 1.2107186e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  69%|   | 156/225 [1:37:11<40:11, 34.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [4.3454961e-07 4.2883528e-07 4.2212545e-07 4.1922925e-07 4.1804392e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  70%|   | 157/225 [1:37:23<32:51, 29.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [6.4805017e-06 3.6613227e-07 1.9734661e-07 1.6687157e-07 1.5433027e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  70%|   | 158/225 [1:37:52<32:23, 29.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.7263042e-06 1.6129610e-06 1.4633013e-06 1.3694687e-06 1.3562344e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  71%|   | 159/225 [1:39:38<55:22, 50.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.2767803e-06 9.9515478e-07 8.9814046e-07 8.9013287e-07 8.6377912e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  71%|   | 160/225 [1:41:39<1:16:18, 70.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.9805457e-07 1.3478157e-07 1.3246164e-07 1.3064388e-07 1.2645324e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  72%|  | 162/225 [1:42:18<49:55, 47.55s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.1382751e-07 1.7242229e-07 1.2892239e-07 1.2318648e-07 1.2100136e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  72%|  | 163/225 [1:42:28<39:44, 38.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [5.1576262e-07 4.4736225e-07 4.4342065e-07 4.3576441e-07 4.3144829e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  73%|  | 164/225 [1:42:40<32:16, 31.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [8.0395757e-06 3.5728121e-07 1.8477448e-07 1.5799442e-07 1.4891253e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  73%|  | 165/225 [1:43:14<32:11, 32.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.5464593e-06 1.5996080e-06 1.3445325e-06 1.2964641e-06 1.2804112e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  74%|  | 166/225 [1:45:16<56:19, 57.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.2090204e-06 9.8839200e-07 9.1058541e-07 8.7669929e-07 8.7006180e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  74%|  | 167/225 [1:47:13<1:11:32, 74.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.8071574e-07 1.4195285e-07 1.3480842e-07 1.3016560e-07 1.2679759e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  75%|  | 169/225 [1:47:51<46:16, 49.59s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.0457780e-07 1.8557837e-07 1.2168802e-07 1.1972725e-07 1.1304870e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  76%|  | 170/225 [1:48:02<36:40, 40.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [4.917152e-07 4.853898e-07 4.836479e-07 4.827264e-07 4.796106e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  76%|  | 171/225 [1:48:14<29:37, 32.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [7.8854764e-06 3.5470705e-07 1.9019053e-07 1.7042643e-07 1.4854311e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  76%|  | 172/225 [1:48:49<29:31, 33.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.0400888e-06 5.1067394e-07 4.3044861e-07 4.2407632e-07 4.0096896e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  77%|  | 173/225 [1:50:47<49:19, 56.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2552,4096), (14336,2552)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.1106400e-06 1.0340281e-06 9.4785787e-07 8.5623373e-07 8.4275575e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  77%|  | 174/225 [1:52:50<1:04:20, 75.69s/it]ERROR:root:Failed to process layer model.layers.24.mlp.down_proj: X factor shape mismatch for layer: (4096, 4096)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 332, in compress_llama_with_kron_svd\n",
      "    factorized_sequential = factorize_layer_kron_svd(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 118, in factorize_layer_kron_svd\n",
      "    assert XF.shape == (in_features, in_features), f\"X factor shape mismatch for layer: {XF.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: X factor shape mismatch for layer: (4096, 4096)\n",
      "Compressing Layers:  78%|  | 175/225 [1:52:50<44:54, 53.89s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.9143480e-06 1.4445652e-06 1.3394397e-06 1.2918169e-06 1.2882923e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  78%|  | 176/225 [1:53:29<40:25, 49.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [4.9498999e-06 1.6986601e-06 6.1885351e-07 3.8541049e-07 3.0465921e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  79%|  | 177/225 [1:53:38<30:07, 37.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [5.484912e-07 5.049904e-07 5.031985e-07 4.987933e-07 4.971964e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  79%|  | 178/225 [1:53:51<23:45, 30.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (416,4096), (1024,416)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.1254148e-05 2.8972235e-07 2.0036602e-07 1.6069528e-07 1.4208929e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  80%|  | 179/225 [1:54:26<24:13, 31.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.0125824e-06 5.4124558e-07 4.4865030e-07 4.0855127e-07 3.9656794e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  80%|  | 180/225 [1:56:21<42:26, 56.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2552,4096), (14336,2552)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.1439041e-06 1.0066751e-06 8.6859200e-07 8.6025432e-07 8.1626717e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  80%|  | 181/225 [1:58:43<1:00:13, 82.13s/it]ERROR:root:Failed to process layer model.layers.25.mlp.down_proj: X factor shape mismatch for layer: (4096, 4096)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 332, in compress_llama_with_kron_svd\n",
      "    factorized_sequential = factorize_layer_kron_svd(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 118, in factorize_layer_kron_svd\n",
      "    assert XF.shape == (in_features, in_features), f\"X factor shape mismatch for layer: {XF.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: X factor shape mismatch for layer: (4096, 4096)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  81%|  | 182/225 [1:58:43<41:17, 57.63s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.8541714e-07 1.4348761e-07 1.3927728e-07 1.3521192e-07 1.2833172e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  81%| | 183/225 [1:59:37<39:27, 56.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.2078500e-07 1.8336767e-07 1.4434940e-07 1.3302109e-07 1.2853846e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  82%| | 184/225 [1:59:50<29:39, 43.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [5.2060091e-07 5.0433073e-07 5.0056502e-07 4.9385903e-07 4.6801563e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  82%| | 185/225 [2:00:06<23:25, 35.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [7.9912297e-06 2.6315018e-07 1.8209772e-07 1.5094830e-07 1.4242330e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  83%| | 186/225 [2:00:57<26:01, 40.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.1088536e-06 5.7977303e-07 5.0967986e-07 4.2283315e-07 4.0888074e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  83%| | 187/225 [2:03:32<47:12, 74.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.3146501e-06 9.6327483e-07 8.8556607e-07 8.4249899e-07 8.0841346e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  84%| | 188/225 [2:06:01<59:37, 96.69s/it]ERROR:root:Failed to process layer model.layers.26.mlp.down_proj: X factor shape mismatch for layer: (4096, 4096)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 332, in compress_llama_with_kron_svd\n",
      "    factorized_sequential = factorize_layer_kron_svd(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 118, in factorize_layer_kron_svd\n",
      "    assert XF.shape == (in_features, in_features), f\"X factor shape mismatch for layer: {XF.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: X factor shape mismatch for layer: (4096, 4096)\n",
      "Compressing Layers:  84%| | 189/225 [2:06:01<40:38, 67.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [5.88679086e-06 1.33522917e-07 1.03039696e-07 7.18387554e-08\n",
      " 6.35154294e-08]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  84%| | 190/225 [2:06:35<33:41, 57.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.0537142e-05 2.0674329e-06 7.8120820e-07 4.1699940e-07 2.3203941e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  85%| | 191/225 [2:06:44<24:24, 43.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [8.2814395e-06 3.0761094e-06 8.4084127e-07 3.9652033e-07 3.4834278e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  85%| | 192/225 [2:06:53<18:05, 32.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (416,4096), (1024,416)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.1264481e-05 2.5789780e-07 1.9658941e-07 1.4349644e-07 1.3683764e-07]\n",
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  86%| | 193/225 [2:07:28<17:49, 33.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.9490957e-05 7.0225894e-07 4.5406452e-07 4.2238798e-07 3.8627638e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  86%| | 194/225 [2:08:52<25:09, 48.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.8056322e-05 6.8047416e-07 4.4346743e-07 3.8212141e-07 3.5896815e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  87%| | 195/225 [2:10:31<31:48, 63.62s/it]ERROR:root:Failed to process layer model.layers.27.mlp.down_proj: X factor shape mismatch for layer: (4096, 4096)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 332, in compress_llama_with_kron_svd\n",
      "    factorized_sequential = factorize_layer_kron_svd(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 118, in factorize_layer_kron_svd\n",
      "    assert XF.shape == (in_features, in_features), f\"X factor shape mismatch for layer: {XF.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: X factor shape mismatch for layer: (4096, 4096)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  87%| | 196/225 [2:10:31<21:32, 44.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.0227060e-06 4.5608067e-07 4.1760148e-07 4.1510245e-07 4.1197475e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  88%| | 197/225 [2:11:10<20:05, 43.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.0821381e-05 2.3355758e-06 6.5315447e-07 3.5472732e-07 3.1660386e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  88%| | 198/225 [2:11:20<14:50, 32.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [9.7257480e-06 1.7913121e-06 5.1372467e-07 4.4371259e-07 2.2818972e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  88%| | 199/225 [2:11:30<11:15, 26.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (496,4096), (1024,496)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.3249642e-05 2.1825757e-07 1.9507098e-07 1.3950178e-07 1.3067694e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  89%| | 200/225 [2:11:58<11:09, 26.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.2881030e-06 7.2212339e-07 6.1241411e-07 4.8978558e-07 4.5812217e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  89%| | 201/225 [2:14:17<24:07, 60.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.5605196e-05 9.3450660e-07 3.4606398e-07 2.5004610e-07 2.3202034e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  90%| | 202/225 [2:15:46<26:24, 68.88s/it]ERROR:root:Failed to process layer model.layers.28.mlp.down_proj: X factor shape mismatch for layer: (4096, 4096)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 332, in compress_llama_with_kron_svd\n",
      "    factorized_sequential = factorize_layer_kron_svd(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 118, in factorize_layer_kron_svd\n",
      "    assert XF.shape == (in_features, in_features), f\"X factor shape mismatch for layer: {XF.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: X factor shape mismatch for layer: (4096, 4096)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  90%| | 203/225 [2:15:46<17:41, 48.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [9.3751050e-06 4.7075018e-06 4.5012871e-06 4.1694848e-06 4.0001405e-06]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  91%| | 204/225 [2:16:23<15:41, 44.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.1988311e-05 2.5011707e-07 1.5505023e-07 1.0966812e-07 9.6873286e-08]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  91%| | 205/225 [2:16:31<11:18, 33.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (416,4096), (1024,416)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.6718690e-05 2.1647305e-07 1.8326548e-07 1.3912150e-07 1.0623976e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  92%|| 207/225 [2:17:00<07:29, 24.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.7569506e-05 5.7000005e-07 4.2657007e-07 3.5659735e-07 3.4155045e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  92%|| 208/225 [2:18:26<11:22, 40.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2552,4096), (14336,2552)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Factor is positive definite (alpha=1.00e-02)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (14336x4096)...\n",
      "  SVD complete. Singular values (top 5): [2.7804877e-05 1.6310109e-06 5.2699698e-07 1.6826776e-07 1.3833862e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  93%|| 209/225 [2:20:24<16:05, 60.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (2872,4096), (14336,2872)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Failed to process layer model.layers.29.mlp.down_proj: X factor shape mismatch for layer: (4096, 4096)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 332, in compress_llama_with_kron_svd\n",
      "    factorized_sequential = factorize_layer_kron_svd(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/compress_llama_with_kronsvd.py\", line 118, in factorize_layer_kron_svd\n",
      "    assert XF.shape == (in_features, in_features), f\"X factor shape mismatch for layer: {XF.shape}\"\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: X factor shape mismatch for layer: (4096, 4096)\n",
      "Compressing Layers:  93%|| 210/225 [2:20:24<11:00, 44.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [3.5072145e-07 1.7092594e-07 1.6322360e-07 1.5684195e-07 1.5211896e-07]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  94%|| 211/225 [2:20:57<09:31, 40.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [6.2514309e-06 4.8170000e-07 2.9302473e-07 2.5369931e-07 2.0256979e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  94%|| 212/225 [2:21:05<06:49, 31.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.3097830e-05 2.0649499e-07 1.6153173e-07 1.4636164e-07 1.4090276e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  95%|| 214/225 [2:21:34<04:22, 23.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Factor is positive definite (alpha=1.00e-03)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [9.7740767e-06 1.1108622e-07 6.4539591e-08 4.8762740e-08 3.7879971e-08]\n",
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  97%|| 218/225 [2:22:02<01:41, 14.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Factor is positive definite (alpha=1.00e-04)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (1024x4096)...\n",
      "  SVD complete. Singular values (top 5): [8.4140293e-06 5.3724966e-07 1.4004478e-07 1.2285818e-07 8.5727088e-08]\n",
      "  Cholesky factor inverses computed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers:  97%|| 219/225 [2:22:11<01:20, 13.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Factorization complete for layer. New shapes: (328,4096), (1024,328)\n",
      "Regularizing factors for layer (initial alpha=1.00e-05)...\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Factor is positive definite (alpha=1.00e-01)\n",
      "  Regularizing factor (try 1, alpha=1.00e-05)\n",
      "  Regularizing factor (try 2, alpha=1.00e-04)\n",
      "  Regularizing factor (try 3, alpha=1.00e-03)\n",
      "  Regularizing factor (try 4, alpha=1.00e-02)\n",
      "  Regularizing factor (try 5, alpha=1.00e-01)\n",
      "  Factor is positive definite (alpha=1.00e+00)\n",
      "  Cholesky decomposition successful.\n",
      "  Performing SVD on transformed matrix (4096x4096)...\n",
      "  SVD complete. Singular values (top 5): [1.1743012e-06 1.0243543e-06 9.8327052e-07 9.5862492e-07 8.6886826e-07]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing Layers: 100%|| 225/225 [2:22:46<00:00, 38.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Cholesky factor inverses computed.\n",
      "  Factorization complete for layer. New shapes: (824,4096), (4096,824)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from compress_llama_with_kronsvd import load_sensitivity_dict, compress_llama_with_kron_svd \n",
    "sensitivity_dict = load_sensitivity_dict(SENSITIVITY_DICT_PATH)\n",
    "\n",
    "# Compress model\n",
    "compressed_model, compression_stats = compress_llama_with_kron_svd(\n",
    "    model_name_or_path=MODEL_NAME,\n",
    "    kron_factors_dir=KRON_FACTORS_DIR,\n",
    "    sensitivity_dict=sensitivity_dict,\n",
    "    device=\"cuda:0\",\n",
    "    rank_alignment=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c01ac0-5490-4e17-a4bb-ae518262fd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc7c1cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (289077 > 131072). Running this sequence through the model will result in indexing errors\n",
      "Evaluating: 100%|| 36/36 [00:21<00:00,  1.66it/s]\n",
      "ERROR:root:Failed to evaluate on wikitext2: exp(): argument 'input' (position 1) must be Tensor, not float\n",
      "Evaluating: 100%|| 12/12 [00:07<00:00,  1.63it/s]\n",
      "ERROR:root:Failed to evaluate on ptb: exp(): argument 'input' (position 1) must be Tensor, not float\n",
      "ERROR:root:Failed to evaluate on c4: Unable to find '/home/jovyan/shares/SR004.nfs2/kurkin/FisherKronecker/utils/c4-validation.json'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'wikitext2': nan, 'ptb': nan, 'c4': nan}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4cda1dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IndexDataset(Dataset):\n",
    "    \"\"\"Simple dataset class for indexed tensors.\"\"\"\n",
    "    def __init__(self, tensors):\n",
    "        self.tensors = tensors\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.tensors[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensors)\n",
    "\n",
    "def get_test_dataset(name: str, tokenizer, seq_len: int = 2048) -> IndexDataset:\n",
    "    \"\"\"Prepare test dataset for perplexity evaluation.\"\"\"\n",
    "    def process_data(samples, field_name):\n",
    "        test_ids = tokenizer(\"\\n\\n\".join(samples[field_name]), return_tensors='pt').input_ids[0]\n",
    "        test_ids_batch = []\n",
    "        nsamples = test_ids.numel() // seq_len\n",
    "\n",
    "        for i in range(nsamples):\n",
    "            batch = test_ids[(i * seq_len):((i + 1) * seq_len)]\n",
    "            test_ids_batch.append(batch)\n",
    "        test_ids_batch = torch.stack(test_ids_batch)\n",
    "        return IndexDataset(tensors=test_ids_batch)\n",
    "    \n",
    "    if 'wikitext2' in name:\n",
    "        test_data = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "        return process_data(test_data, 'text')\n",
    "    if 'ptb' in name:\n",
    "        test_data = load_dataset('ptb_text_only', 'penn_treebank', split='test')\n",
    "        return process_data(test_data, 'sentence')\n",
    "    elif 'c4' in name:\n",
    "        test_data = load_dataset(\"json\", data_files=\"utils/c4-validation.json\")['train']\n",
    "        return process_data(test_data[0:2000], 'text')\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {name}\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_perplexity(\n",
    "    model: nn.Module, \n",
    "    dataset: IndexDataset, \n",
    "    batch_size: int = 4,\n",
    "    device: str = \"cuda\"\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate model perplexity on a dataset.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        dataset: Dataset of tokenized inputs\n",
    "        batch_size: Batch size for evaluation\n",
    "        device: Device for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Perplexity score (lower is better)\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    nlls = []\n",
    "    for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        outputs = model(batch, use_cache=False)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        if not torch.isfinite(logits).all():\n",
    "            logging.warning(\"Skipping batch with non-finite logits\")\n",
    "            continue\n",
    "            \n",
    "        shift_logits = logits[:, :-1, :].contiguous()\n",
    "        shift_labels = batch[:, 1:].contiguous()\n",
    "        \n",
    "        loss_fct = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "        loss = loss_fct(\n",
    "            shift_logits.reshape(-1, shift_logits.size(-1)), \n",
    "            shift_labels.view(-1)\n",
    "        )\n",
    "        # Inside the evaluate_perplexity loop, before appending loss\n",
    "        mean_batch_nll = loss.mean().item()\n",
    "        print(f\"Batch mean NLL: {mean_batch_nll:.4f}\")\n",
    "        nlls.append(loss)\n",
    "\n",
    "    # Before the return statement\n",
    "    mean_nll_tensor = torch.cat(nlls, dim=-1).mean()\n",
    "    print(f\"Overall Mean NLL: {mean_nll_tensor.item():.4f}\")\n",
    "    ppl = torch.exp(mean_nll_tensor).item()\n",
    "    return ppl\n",
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    tokenizer,\n",
    "    datasets: List[str] = ['wikitext2', 'ptb', 'c4'],\n",
    "    seq_len: int = 2048,\n",
    "    batch_size: int = 4,\n",
    "    device: str = \"cuda\"\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate model on multiple datasets.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to evaluate\n",
    "        tokenizer: Tokenizer for preprocessing\n",
    "        datasets: List of dataset names to evaluate on\n",
    "        seq_len: Maximum sequence length\n",
    "        batch_size: Batch size for evaluation\n",
    "        device: Device for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping dataset names to perplexity scores\n",
    "    \"\"\"\n",
    "    # Save initial model state\n",
    "    model.eval()\n",
    "    use_cache = model.config.use_cache\n",
    "    model.config.use_cache = False\n",
    "    model.to(device)\n",
    "    \n",
    "    results = {}\n",
    "    for dataset_name in datasets:\n",
    "        logging.info(f\"Evaluating on {dataset_name}\")\n",
    "        try:\n",
    "            # Prepare dataset\n",
    "            test_dataset = get_test_dataset(dataset_name, tokenizer, seq_len)\n",
    "            \n",
    "            # Evaluate perplexity\n",
    "            ppl = evaluate_perplexity(\n",
    "                model, \n",
    "                test_dataset, \n",
    "                batch_size=batch_size,\n",
    "                device=device\n",
    "            )\n",
    "            \n",
    "            results[dataset_name] = ppl\n",
    "            logging.info(f\"{dataset_name} perplexity: {ppl:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to evaluate on {dataset_name}: {e}\")\n",
    "            results[dataset_name] = float('nan')\n",
    "    \n",
    "    # Restore model state\n",
    "    model.config.use_cache = use_cache\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fba6337b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  11%|         | 1/9 [00:02<00:18,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch mean NLL: 13.7500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  22%|       | 2/9 [00:04<00:16,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch mean NLL: 13.2500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  33%|      | 3/9 [00:07<00:14,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch mean NLL: 13.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  44%|     | 4/9 [00:09<00:11,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch mean NLL: 13.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  56%|    | 5/9 [00:11<00:09,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch mean NLL: 13.6250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  67%|   | 6/9 [00:14<00:07,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch mean NLL: 13.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  78%|  | 7/9 [00:16<00:04,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch mean NLL: 13.1875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  89%| | 8/9 [00:18<00:02,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch mean NLL: 13.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|| 9/9 [00:20<00:00,  2.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch mean NLL: 13.5000\n",
      "Overall Mean NLL: 13.4375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "684032.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = get_test_dataset(\"wikitext2\", tokenizer)\n",
    "evaluate_perplexity(\n",
    "    model=compressed_model,\n",
    "    dataset=dataset,\n",
    "    batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3ec6c4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5b01d7d485484bb1a5101a4a6a6b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME).eval().to(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "667cac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_model.save_pretrained(\"llama_compressed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89a60130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096, padding_idx=128004)\n",
       "    (layers): ModuleList(\n",
       "      (0): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1912, bias=False)\n",
       "            (1): Linear(in_features=1912, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2232, bias=False)\n",
       "            (1): Linear(in_features=2232, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (1): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=656, bias=False)\n",
       "            (1): Linear(in_features=656, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (2): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=576, bias=False)\n",
       "            (1): Linear(in_features=576, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2552, bias=False)\n",
       "            (1): Linear(in_features=2552, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (3): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=416, bias=False)\n",
       "            (1): Linear(in_features=416, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2232, bias=False)\n",
       "            (1): Linear(in_features=2232, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (4): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=744, bias=False)\n",
       "            (1): Linear(in_features=744, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (5): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=496, bias=False)\n",
       "            (1): Linear(in_features=496, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1912, bias=False)\n",
       "            (1): Linear(in_features=1912, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (6): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=576, bias=False)\n",
       "            (1): Linear(in_features=576, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2232, bias=False)\n",
       "            (1): Linear(in_features=2232, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (7): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=576, bias=False)\n",
       "            (1): Linear(in_features=576, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1232, bias=False)\n",
       "            (1): Linear(in_features=1232, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2232, bias=False)\n",
       "            (1): Linear(in_features=2232, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2552, bias=False)\n",
       "            (1): Linear(in_features=2552, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (8): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=416, bias=False)\n",
       "            (1): Linear(in_features=416, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1232, bias=False)\n",
       "            (1): Linear(in_features=1232, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1912, bias=False)\n",
       "            (1): Linear(in_features=1912, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2552, bias=False)\n",
       "            (1): Linear(in_features=2552, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (9): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=496, bias=False)\n",
       "            (1): Linear(in_features=496, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1232, bias=False)\n",
       "            (1): Linear(in_features=1232, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2232, bias=False)\n",
       "            (1): Linear(in_features=2232, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2552, bias=False)\n",
       "            (1): Linear(in_features=2552, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (10): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=416, bias=False)\n",
       "            (1): Linear(in_features=416, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1912, bias=False)\n",
       "            (1): Linear(in_features=1912, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2552, bias=False)\n",
       "            (1): Linear(in_features=2552, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (11): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=416, bias=False)\n",
       "            (1): Linear(in_features=416, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1232, bias=False)\n",
       "            (1): Linear(in_features=1232, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1600, bias=False)\n",
       "            (1): Linear(in_features=1600, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2552, bias=False)\n",
       "            (1): Linear(in_features=2552, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (12): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=576, bias=False)\n",
       "            (1): Linear(in_features=576, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2232, bias=False)\n",
       "            (1): Linear(in_features=2232, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (13): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=496, bias=False)\n",
       "            (1): Linear(in_features=496, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1912, bias=False)\n",
       "            (1): Linear(in_features=1912, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (14): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=576, bias=False)\n",
       "            (1): Linear(in_features=576, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=576, bias=False)\n",
       "            (1): Linear(in_features=576, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1232, bias=False)\n",
       "            (1): Linear(in_features=1232, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2232, bias=False)\n",
       "            (1): Linear(in_features=2232, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (15): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=576, bias=False)\n",
       "            (1): Linear(in_features=576, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1232, bias=False)\n",
       "            (1): Linear(in_features=1232, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1912, bias=False)\n",
       "            (1): Linear(in_features=1912, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (16): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=576, bias=False)\n",
       "            (1): Linear(in_features=576, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1232, bias=False)\n",
       "            (1): Linear(in_features=1232, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2552, bias=False)\n",
       "            (1): Linear(in_features=2552, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (17): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=656, bias=False)\n",
       "            (1): Linear(in_features=656, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (1): Linear(in_features=1024, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2552, bias=False)\n",
       "            (1): Linear(in_features=2552, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (18): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=496, bias=False)\n",
       "            (1): Linear(in_features=496, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=1232, bias=False)\n",
       "            (1): Linear(in_features=1232, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2552, bias=False)\n",
       "            (1): Linear(in_features=2552, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (19): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=416, bias=False)\n",
       "            (1): Linear(in_features=416, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2552, bias=False)\n",
       "            (1): Linear(in_features=2552, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (20): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2552, bias=False)\n",
       "            (1): Linear(in_features=2552, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (21): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=496, bias=False)\n",
       "            (1): Linear(in_features=496, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (22-23): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (24): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2552, bias=False)\n",
       "            (1): Linear(in_features=2552, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (25): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=416, bias=False)\n",
       "            (1): Linear(in_features=416, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2552, bias=False)\n",
       "            (1): Linear(in_features=2552, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (26): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (27): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=416, bias=False)\n",
       "            (1): Linear(in_features=416, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (28): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=496, bias=False)\n",
       "            (1): Linear(in_features=496, out_features=1024, bias=False)\n",
       "          )\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (29): LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=416, bias=False)\n",
       "            (1): Linear(in_features=416, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2552, bias=False)\n",
       "            (1): Linear(in_features=2552, out_features=14336, bias=False)\n",
       "          )\n",
       "          (up_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=2872, bias=False)\n",
       "            (1): Linear(in_features=2872, out_features=14336, bias=False)\n",
       "          )\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "      (30-31): 2 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "          (k_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=328, bias=False)\n",
       "            (1): Linear(in_features=328, out_features=1024, bias=False)\n",
       "          )\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Sequential(\n",
       "            (0): Linear(in_features=4096, out_features=824, bias=False)\n",
       "            (1): Linear(in_features=824, out_features=4096, bias=False)\n",
       "          )\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1319a39e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.mlspace-vika_kurkin_clone]",
   "language": "python",
   "name": "conda-env-.mlspace-vika_kurkin_clone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

2025-04-27 22:24:23,567 - INFO - Loading model: unsloth/llama-2-7b-chatmodel.layers.28.self_attn.k_proj 0.3model.layers.22.mlp.gate_proj 0.5model.layers.19.mlp.gate_proj 0.5model.layers.19.self_attn.q_proj 0.3model.layers.18.mlp.gate_proj 0.5model.layers.18.self_attn.q_proj 0.3model.layers.18.self_attn.k_proj 0.3model.layers.17.mlp.gate_proj 0.5model.layers.17.self_attn.q_proj 0.3model.layers.16.mlp.gate_proj 0.5model.layers.16.self_attn.v_proj 0.3model.layers.15.self_attn.q_proj 0.3model.layers.14.mlp.gate_proj 0.5model.layers.14.mlp.up_proj 0.5model.layers.13.mlp.gate_proj 0.5model.layers.13.mlp.up_proj 0.5model.layers.13.self_attn.q_proj 0.3model.layers.13.self_attn.k_proj 0.3model.layers.13.self_attn.v_proj 0.3model.layers.13.self_attn.o_proj 0.3model.layers.12.mlp.gate_proj 0.5model.layers.12.mlp.up_proj 0.5model.layers.12.self_attn.q_proj 0.3model.layers.12.self_attn.k_proj 0.3model.layers.11.mlp.gate_proj 0.5model.layers.11.self_attn.q_proj 0.3model.layers.9.mlp.down_proj 0.5model.layers.8.mlp.gate_proj 0.5model.layers.8.mlp.down_proj 0.5model.layers.8.self_attn.q_proj 0.3model.layers.8.self_attn.k_proj 0.3model.layers.7.mlp.gate_proj 0.5model.layers.7.mlp.down_proj 0.5model.layers.7.self_attn.q_proj 0.3model.layers.7.self_attn.k_proj 0.3model.layers.6.mlp.down_proj 0.5model.layers.6.self_attn.q_proj 0.3model.layers.6.self_attn.k_proj 0.3model.layers.5.self_attn.k_proj 0.3model.layers.4.mlp.gate_proj 0.5model.layers.4.mlp.down_proj 0.5model.layers.4.self_attn.q_proj 0.3model.layers.4.self_attn.k_proj 0.3model.layers.3.self_attn.q_proj 0.3model.layers.3.self_attn.k_proj 0.3model.layers.2.mlp.gate_proj 0.5model.layers.2.mlp.up_proj 0.5model.layers.2.self_attn.q_proj 0.3model.layers.2.self_attn.k_proj 0.3[2025-04-27 22:24:28,293] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Loading checkpoint shards:   0%|                                                                                                                                                                           | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|██████████████████████████████████████████████████████▎                                                                                                            | 1/3 [00:13<00:26, 13.20s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 2/3 [00:28<00:14, 14.29s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:37<00:00, 12.08s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:37<00:00, 12.57s/it]2025-04-27 22:25:09,163 - INFO - Model loaded with dtype torch.bfloat162025-04-27 22:26:08,575 - INFO - Model moved to cuda:02025-04-27 22:26:08,577 - INFO - Total parameters before compression: 67384156162025-04-27 22:26:08,578 - INFO - Found 225 linear layers to potentially compress.Compressing Layers:   0%|                                                                                                                                                                                | 0/225 [00:00<?, ?it/s]2025-04-27 22:26:08,579 - INFO - Skipping layer model.layers.0.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:08,579 - INFO - Skipping layer model.layers.0.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:08,579 - INFO - Skipping layer model.layers.0.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:08,579 - INFO - Skipping layer model.layers.0.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:08,579 - INFO - Skipping layer model.layers.0.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:08,579 - INFO - Skipping layer model.layers.0.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:08,579 - INFO - Skipping layer model.layers.0.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:08,579 - INFO - Skipping layer model.layers.1.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:08,580 - INFO - Skipping layer model.layers.1.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:08,580 - INFO - Skipping layer model.layers.1.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:08,580 - INFO - Skipping layer model.layers.1.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:08,580 - INFO - Skipping layer model.layers.1.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:08,580 - INFO - Skipping layer model.layers.1.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:08,580 - INFO - Skipping layer model.layers.1.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:08,580 - INFO - Layer: model.layers.2.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:26:08,581 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_q_proj.safetensors2025-04-27 22:26:08,581 - INFO - exists: True2025-04-27 22:26:08,584 - INFO - factorize_layer_kron_svd2025-04-27 22:26:10,265 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:26:11,883 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:26:13,513 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:26:15,221 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:26:17,158 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:26:21,993 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:26:25,409 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:26:29,092 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:26:32,665 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:26:35,055 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:26:37,017 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:26:41,962 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_q_proj.safetensors TrueLayer: model.layers.2.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.6124124e-05 1.1238000e-05 1.0046171e-05 9.5376463e-06 9.3023737e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:27:54,403 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:27:54,403 - INFO - Replacing 'model.layers.2.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:   7%|███████████▏                                                                                                                                                           | 15/225 [01:45<24:41,  7.05s/it]2025-04-27 22:27:54,404 - INFO - Layer: model.layers.2.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:27:54,405 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_k_proj.safetensors2025-04-27 22:27:54,405 - INFO - exists: True2025-04-27 22:27:54,468 - INFO - factorize_layer_kron_svd2025-04-27 22:27:58,673 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:28:01,909 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:28:03,789 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:28:07,505 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_k_proj.safetensors TrueLayer: model.layers.2.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.0046256e-03 1.5359199e-04 9.4199255e-05 5.9006747e-05 4.1726034e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:28:59,986 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:28:59,987 - INFO - Replacing 'model.layers.2.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:   7%|███████████▉                                                                                                                                                           | 16/225 [02:51<42:18, 12.14s/it]2025-04-27 22:28:59,989 - INFO - Skipping layer model.layers.2.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:28:59,989 - INFO - Skipping layer model.layers.2.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:28:59,989 - INFO - Layer: model.layers.2.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:28:59,991 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_gate_proj.safetensors2025-04-27 22:28:59,991 - INFO - exists: True2025-04-27 22:29:00,062 - INFO - factorize_layer_kron_svd2025-04-27 22:29:03,781 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:29:06,868 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:29:09,997 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:29:11,950 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:29:14,098 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:29:17,898 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:29:21,751 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:29:25,325 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:29:28,947 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:29:31,654 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:29:34,505 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:29:40,003 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_gate_proj.safetensors TrueLayer: model.layers.2.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.0536157e-05 7.8889116e-06 7.3416263e-06 6.9958951e-06 6.9346693e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:32:16,976 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:32:16,976 - INFO - Replacing 'model.layers.2.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:   8%|█████████████▉                                                                                                                                                       | 19/225 [06:08<1:31:34, 26.67s/it]2025-04-27 22:32:16,977 - INFO - Layer: model.layers.2.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:32:16,980 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_up_proj.safetensors2025-04-27 22:32:16,980 - INFO - exists: True2025-04-27 22:32:17,009 - INFO - factorize_layer_kron_svd2025-04-27 22:32:18,850 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:32:20,487 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:32:23,087 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:32:25,015 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:32:26,591 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:32:28,410 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:32:30,848 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:32:33,661 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:32:36,483 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:32:40,570 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:32:45,993 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:32:55,213 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_up_proj.safetensors TrueLayer: model.layers.2.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [4.5019283e-06 4.2492616e-06 4.1393623e-06 4.1020489e-06 4.0891123e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:34:29,793 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:34:29,793 - INFO - Replacing 'model.layers.2.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:   9%|██████████████▋                                                                                                                                                      | 20/225 [08:21<2:12:39, 38.83s/it]2025-04-27 22:34:29,795 - INFO - Skipping layer model.layers.2.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:29,795 - INFO - Layer: model.layers.3.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:34:29,800 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_q_proj.safetensors2025-04-27 22:34:29,800 - INFO - exists: True2025-04-27 22:34:29,889 - INFO - factorize_layer_kron_svd2025-04-27 22:34:32,932 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:34:35,090 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:34:37,980 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:34:41,868 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_q_proj.safetensors TrueLayer: model.layers.3.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.1026561e-02 7.6449120e-05 6.9494883e-05 4.4865315e-05 4.0528874e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:35:24,813 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:35:24,813 - INFO - Replacing 'model.layers.3.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  10%|████████████████▏                                                                                                                                                    | 22/225 [09:16<2:01:55, 36.04s/it]2025-04-27 22:35:24,813 - INFO - Layer: model.layers.3.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:35:24,815 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_k_proj.safetensors2025-04-27 22:35:24,815 - INFO - exists: True2025-04-27 22:35:24,835 - INFO - factorize_layer_kron_svd2025-04-27 22:35:26,882 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:35:30,069 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:35:33,300 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:35:36,504 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:35:39,301 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:35:41,384 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:35:43,979 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:35:47,185 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:35:50,055 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:35:51,551 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:35:53,988 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:35:55,813 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_k_proj.safetensors TrueLayer: model.layers.3.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.5212615e-05 7.6301594e-06 6.8745117e-06 6.6968514e-06 6.6398402e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:36:39,000 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:36:39,001 - INFO - Replacing 'model.layers.3.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  10%|████████████████▊                                                                                                                                                    | 23/225 [10:30<2:20:33, 41.75s/it]2025-04-27 22:36:39,001 - INFO - Skipping layer model.layers.3.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:36:39,001 - INFO - Skipping layer model.layers.3.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:36:39,001 - INFO - Skipping layer model.layers.3.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:36:39,001 - INFO - Skipping layer model.layers.3.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:36:39,001 - INFO - Skipping layer model.layers.3.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:36:39,001 - INFO - Layer: model.layers.4.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:36:39,003 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_q_proj.safetensors2025-04-27 22:36:39,003 - INFO - exists: True2025-04-27 22:36:39,071 - INFO - factorize_layer_kron_svd2025-04-27 22:36:42,731 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:36:44,878 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:36:47,711 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:36:50,790 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:36:53,948 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_q_proj.safetensors TrueLayer: model.layers.4.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.2226075e-02 9.3771603e-05 6.9365597e-05 3.1357977e-05 2.4107103e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:37:32,105 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:37:32,105 - INFO - Replacing 'model.layers.4.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  13%|█████████████████████▎                                                                                                                                               | 29/225 [11:23<1:15:58, 23.26s/it]2025-04-27 22:37:32,106 - INFO - Layer: model.layers.4.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:37:32,107 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_k_proj.safetensors2025-04-27 22:37:32,107 - INFO - exists: True2025-04-27 22:37:32,116 - INFO - factorize_layer_kron_svd2025-04-27 22:37:33,692 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:37:35,336 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:37:37,003 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:37:39,404 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:37:41,277 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:37:45,279 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:37:48,273 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:37:51,304 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:37:53,991 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:37:55,788 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:37:58,588 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:38:02,408 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_k_proj.safetensors TrueLayer: model.layers.4.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.5640973e-05 8.6818372e-06 8.1892204e-06 8.0699401e-06 7.7858249e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:38:48,315 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:38:48,315 - INFO - Replacing 'model.layers.4.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  13%|██████████████████████                                                                                                                                               | 30/225 [12:39<1:35:54, 29.51s/it]2025-04-27 22:38:48,316 - INFO - Skipping layer model.layers.4.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:38:48,316 - INFO - Skipping layer model.layers.4.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:38:48,316 - INFO - Layer: model.layers.4.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:38:48,317 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_mlp_gate_proj.safetensors2025-04-27 22:38:48,317 - INFO - exists: True2025-04-27 22:38:48,336 - INFO - factorize_layer_kron_svd2025-04-27 22:38:50,202 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:38:51,725 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:38:53,226 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:38:54,697 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:38:56,994 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:38:59,580 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:39:02,704 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:39:06,597 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:39:09,164 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:39:14,601 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:39:20,313 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:39:32,975 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_mlp_gate_proj.safetensors TrueLayer: model.layers.4.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.0836617e-05 6.9572156e-06 6.8608979e-06 6.6034477e-06 6.4066685e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:41:09,163 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:41:09,163 - INFO - Replacing 'model.layers.4.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  15%|████████████████████████▏                                                                                                                                            | 33/225 [15:00<1:53:10, 35.37s/it]2025-04-27 22:41:09,164 - INFO - Skipping layer model.layers.4.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:09,164 - INFO - Layer: model.layers.4.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:41:09,166 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_mlp_down_proj.safetensors2025-04-27 22:41:09,166 - INFO - exists: True2025-04-27 22:41:09,242 - INFO - factorize_layer_kron_svd2025-04-27 22:41:12,241 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:41:15,856 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:41:21,538 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:41:23,003 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:41:25,505 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:41:28,586 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_mlp_down_proj.safetensors TrueLayer: model.layers.4.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=11008, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x11008)...  SVD complete. Singular values (top 5): [2.7665207e-02 6.1080464e-05 2.8755128e-05 2.1798001e-05 1.3714681e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,11008), (4096,1496)factorized_sequential Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))2025-04-27 22:43:52,649 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))')2025-04-27 22:43:52,649 - INFO - Replacing 'model.layers.4.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)Compressing Layers:  16%|█████████████████████████▋                                                                                                                                           | 35/225 [17:44<2:27:35, 46.61s/it]2025-04-27 22:43:52,649 - INFO - Skipping layer model.layers.5.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:43:52,649 - INFO - Layer: model.layers.5.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:43:52,650 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_5_self_attn_k_proj.safetensors2025-04-27 22:43:52,651 - INFO - exists: True2025-04-27 22:43:52,686 - INFO - factorize_layer_kron_svd2025-04-27 22:43:54,546 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:43:56,578 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:43:58,184 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:44:00,341 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_5_self_attn_k_proj.safetensors TrueLayer: model.layers.5.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.8379018e-03 1.5238064e-04 9.5437594e-05 3.4556335e-05 3.0823277e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:44:54,103 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:44:54,103 - INFO - Replacing 'model.layers.5.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  16%|███████████████████████████▏                                                                                                                                         | 37/225 [18:45<2:13:14, 42.52s/it]2025-04-27 22:44:54,103 - INFO - Skipping layer model.layers.5.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:44:54,103 - INFO - Skipping layer model.layers.5.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:44:54,104 - INFO - Skipping layer model.layers.5.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:44:54,104 - INFO - Skipping layer model.layers.5.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:44:54,104 - INFO - Skipping layer model.layers.5.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:44:54,104 - INFO - Layer: model.layers.6.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:44:54,105 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_q_proj.safetensors2025-04-27 22:44:54,105 - INFO - exists: True2025-04-27 22:44:54,113 - INFO - factorize_layer_kron_svd2025-04-27 22:44:55,729 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:44:57,217 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:44:58,760 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:45:00,258 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:45:01,875 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:45:03,542 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:45:05,005 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:45:06,572 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:45:08,052 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:45:09,549 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:45:11,112 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:45:12,867 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_q_proj.safetensors TrueLayer: model.layers.6.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.2221675e-05 8.1780718e-06 7.9897936e-06 7.7061068e-06 7.4714530e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:46:14,462 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:46:14,463 - INFO - Replacing 'model.layers.6.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  19%|███████████████████████████████▌                                                                                                                                     | 43/225 [20:05<1:22:39, 27.25s/it]2025-04-27 22:46:14,463 - INFO - Layer: model.layers.6.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:46:14,468 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_k_proj.safetensors2025-04-27 22:46:14,468 - INFO - exists: True2025-04-27 22:46:14,474 - INFO - factorize_layer_kron_svd2025-04-27 22:46:17,003 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:46:18,581 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:46:20,318 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:46:21,820 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:46:23,378 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:46:25,358 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:46:26,956 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:46:28,648 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:46:30,482 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:46:33,272 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:46:36,472 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:46:40,291 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_k_proj.safetensors TrueLayer: model.layers.6.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.4503589e-05 7.8944504e-06 7.5118478e-06 7.3252472e-06 7.0036058e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:47:34,262 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:47:34,263 - INFO - Replacing 'model.layers.6.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  20%|████████████████████████████████▎                                                                                                                                    | 44/225 [21:25<1:39:47, 33.08s/it]2025-04-27 22:47:34,263 - INFO - Skipping layer model.layers.6.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:47:34,263 - INFO - Skipping layer model.layers.6.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:47:34,263 - INFO - Skipping layer model.layers.6.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:47:34,263 - INFO - Skipping layer model.layers.6.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:47:34,263 - INFO - Layer: model.layers.6.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:47:34,264 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_mlp_down_proj.safetensors2025-04-27 22:47:34,264 - INFO - exists: True2025-04-27 22:47:34,285 - INFO - factorize_layer_kron_svd2025-04-27 22:47:37,357 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:47:42,425 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:47:44,418 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:47:47,279 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:47:51,085 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_mlp_down_proj.safetensors TrueLayer: model.layers.6.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=11008, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x11008)...  SVD complete. Singular values (top 5): [1.1730455e-02 7.5274904e-05 5.8971975e-05 3.9316310e-05 2.7261001e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,11008), (4096,1496)factorized_sequential Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))2025-04-27 22:49:45,701 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))')2025-04-27 22:49:45,702 - INFO - Replacing 'model.layers.6.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)Compressing Layers:  22%|███████████████████████████████████▉                                                                                                                                 | 49/225 [23:37<1:28:13, 30.08s/it]2025-04-27 22:49:45,703 - INFO - Layer: model.layers.7.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:49:45,708 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_q_proj.safetensors2025-04-27 22:49:45,708 - INFO - exists: True2025-04-27 22:49:45,867 - INFO - factorize_layer_kron_svd2025-04-27 22:49:49,911 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:49:54,592 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:49:58,063 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:50:02,393 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_q_proj.safetensors TrueLayer: model.layers.7.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.46691271e-03 1.17237636e-04 4.46072554e-05 2.93035046e-05 2.62491722e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:50:57,295 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:50:57,295 - INFO - Replacing 'model.layers.7.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  22%|████████████████████████████████████▋                                                                                                                                | 50/225 [24:48<1:41:18, 34.73s/it]2025-04-27 22:50:57,295 - INFO - Layer: model.layers.7.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:50:57,298 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_k_proj.safetensors2025-04-27 22:50:57,298 - INFO - exists: True2025-04-27 22:50:57,317 - INFO - factorize_layer_kron_svd2025-04-27 22:50:59,784 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:51:01,682 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:51:03,221 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:51:05,055 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_k_proj.safetensors TrueLayer: model.layers.7.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.3513221e-03 9.5677962e-05 5.9629248e-05 3.8686332e-05 2.6624961e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:51:49,167 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:51:49,168 - INFO - Replacing 'model.layers.7.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  23%|█████████████████████████████████████▍                                                                                                                               | 51/225 [25:40<1:47:35, 37.10s/it]2025-04-27 22:51:49,168 - INFO - Skipping layer model.layers.7.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:51:49,168 - INFO - Skipping layer model.layers.7.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:51:49,168 - INFO - Layer: model.layers.7.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:51:49,169 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_gate_proj.safetensors2025-04-27 22:51:49,169 - INFO - exists: True2025-04-27 22:51:49,197 - INFO - factorize_layer_kron_svd2025-04-27 22:51:53,472 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:51:57,187 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:52:01,067 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:52:06,669 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:52:15,880 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_gate_proj.safetensors TrueLayer: model.layers.7.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [6.1598225e-03 4.3083266e-05 2.9594266e-05 2.0862246e-05 1.8714514e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:53:41,462 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:53:41,463 - INFO - Replacing 'model.layers.7.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  24%|███████████████████████████████████████▌                                                                                                                             | 54/225 [27:32<1:46:05, 37.22s/it]2025-04-27 22:53:41,463 - INFO - Skipping layer model.layers.7.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:53:41,463 - INFO - Layer: model.layers.7.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:53:41,466 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_down_proj.safetensors2025-04-27 22:53:41,466 - INFO - exists: True2025-04-27 22:53:41,512 - INFO - factorize_layer_kron_svd2025-04-27 22:53:45,377 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:53:51,499 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:53:54,572 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:53:57,898 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:54:01,504 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_down_proj.safetensors TrueLayer: model.layers.7.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=11008, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x11008)...  SVD complete. Singular values (top 5): [4.0638614e-03 2.6415018e-04 8.9995185e-05 2.9939682e-05 2.5749245e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,11008), (4096,1496)factorized_sequential Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))2025-04-27 22:56:08,162 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))')2025-04-27 22:56:08,163 - INFO - Replacing 'model.layers.7.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)Compressing Layers:  25%|█████████████████████████████████████████                                                                                                                            | 56/225 [29:59<2:11:27, 46.67s/it]2025-04-27 22:56:08,163 - INFO - Layer: model.layers.8.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:56:08,164 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_q_proj.safetensors2025-04-27 22:56:08,164 - INFO - exists: True2025-04-27 22:56:08,199 - INFO - factorize_layer_kron_svd2025-04-27 22:56:11,897 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:56:15,294 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:56:17,420 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:56:19,779 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:56:23,564 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_q_proj.safetensors TrueLayer: model.layers.8.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [5.8933757e-03 1.1251512e-04 4.9954931e-05 2.4490531e-05 1.9172267e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:57:06,497 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:57:06,497 - INFO - Replacing 'model.layers.8.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  25%|█████████████████████████████████████████▊                                                                                                                           | 57/225 [30:57<2:15:49, 48.51s/it]2025-04-27 22:57:06,498 - INFO - Layer: model.layers.8.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:57:06,500 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_k_proj.safetensors2025-04-27 22:57:06,500 - INFO - exists: True2025-04-27 22:57:06,508 - INFO - factorize_layer_kron_svd2025-04-27 22:57:08,691 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:57:11,865 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:57:15,085 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:57:18,307 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:57:21,127 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:57:23,406 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:57:26,304 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:57:29,564 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:57:31,276 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:57:33,421 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:57:34,942 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:57:37,770 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_k_proj.safetensors TrueLayer: model.layers.8.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.4188925e-05 7.6309198e-06 7.4526779e-06 7.2449202e-06 6.7348119e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:58:30,003 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:58:30,003 - INFO - Replacing 'model.layers.8.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  26%|██████████████████████████████████████████▌                                                                                                                          | 58/225 [32:21<2:32:53, 54.93s/it]2025-04-27 22:58:30,004 - INFO - Skipping layer model.layers.8.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:30,004 - INFO - Skipping layer model.layers.8.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:30,004 - INFO - Layer: model.layers.8.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:58:30,007 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_mlp_gate_proj.safetensors2025-04-27 22:58:30,007 - INFO - exists: True2025-04-27 22:58:30,028 - INFO - factorize_layer_kron_svd2025-04-27 22:58:33,383 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:58:36,884 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:58:39,836 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:58:43,289 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:58:49,913 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_mlp_gate_proj.safetensors TrueLayer: model.layers.8.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.4417933e-03 7.4210926e-05 3.5308200e-05 2.5064986e-05 1.6473876e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:00:30,404 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:00:30,404 - INFO - Replacing 'model.layers.8.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  27%|████████████████████████████████████████████▋                                                                                                                        | 61/225 [34:21<2:12:20, 48.42s/it]2025-04-27 23:00:30,404 - INFO - Skipping layer model.layers.8.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:00:30,405 - INFO - Layer: model.layers.8.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:00:30,406 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_mlp_down_proj.safetensors2025-04-27 23:00:30,406 - INFO - exists: True2025-04-27 23:00:30,465 - INFO - factorize_layer_kron_svd2025-04-27 23:00:33,626 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:00:38,807 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:00:40,639 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:00:44,065 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:00:47,899 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_mlp_down_proj.safetensors TrueLayer: model.layers.8.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=11008, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x11008)...  SVD complete. Singular values (top 5): [1.9795836e-03 5.3476641e-04 5.7899004e-05 3.2476404e-05 3.0444553e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,11008), (4096,1496)factorized_sequential Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))2025-04-27 23:02:48,067 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))')2025-04-27 23:02:48,067 - INFO - Replacing 'model.layers.8.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)Compressing Layers:  28%|██████████████████████████████████████████████▏                                                                                                                      | 63/225 [36:39<2:27:00, 54.45s/it]2025-04-27 23:02:48,068 - INFO - Skipping layer model.layers.9.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:02:48,068 - INFO - Skipping layer model.layers.9.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:02:48,068 - INFO - Skipping layer model.layers.9.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:02:48,068 - INFO - Skipping layer model.layers.9.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:02:48,068 - INFO - Skipping layer model.layers.9.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:02:48,068 - INFO - Skipping layer model.layers.9.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:02:48,068 - INFO - Layer: model.layers.9.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:02:48,072 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_9_mlp_down_proj.safetensors2025-04-27 23:02:48,072 - INFO - exists: True2025-04-27 23:02:48,203 - INFO - factorize_layer_kron_svd2025-04-27 23:02:54,668 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:03:02,903 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:03:09,865 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:03:13,065 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:03:16,398 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:03:19,253 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_9_mlp_down_proj.safetensors TrueLayer: model.layers.9.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=11008, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x11008)...  SVD complete. Singular values (top 5): [2.8424084e-02 1.8538581e-04 1.7036285e-04 1.0448481e-04 9.5918804e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,11008), (4096,1496)factorized_sequential Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))2025-04-27 23:05:23,768 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))')2025-04-27 23:05:23,769 - INFO - Replacing 'model.layers.9.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)Compressing Layers:  31%|███████████████████████████████████████████████████▎                                                                                                                 | 70/225 [39:15<1:31:02, 35.24s/it]2025-04-27 23:05:23,769 - INFO - Skipping layer model.layers.10.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:05:23,769 - INFO - Skipping layer model.layers.10.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:05:23,769 - INFO - Skipping layer model.layers.10.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:05:23,769 - INFO - Skipping layer model.layers.10.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:05:23,769 - INFO - Skipping layer model.layers.10.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:05:23,769 - INFO - Skipping layer model.layers.10.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:05:23,769 - INFO - Skipping layer model.layers.10.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:05:23,769 - INFO - Layer: model.layers.11.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:05:23,771 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_self_attn_q_proj.safetensors2025-04-27 23:05:23,771 - INFO - exists: True2025-04-27 23:05:23,825 - INFO - factorize_layer_kron_svd2025-04-27 23:05:25,626 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:05:27,517 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:05:29,252 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:05:30,745 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:05:33,799 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:05:35,624 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_self_attn_q_proj.safetensors TrueLayer: model.layers.11.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [2.6593389e-02 3.8675182e-05 2.0216057e-05 1.6013246e-05 1.3640273e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:06:38,471 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:06:38,472 - INFO - Replacing 'model.layers.11.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  35%|█████████████████████████████████████████████████████████▉                                                                                                             | 78/225 [40:29<55:02, 22.46s/it]2025-04-27 23:06:38,472 - INFO - Skipping layer model.layers.11.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:06:38,472 - INFO - Skipping layer model.layers.11.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:06:38,472 - INFO - Skipping layer model.layers.11.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:06:38,473 - INFO - Layer: model.layers.11.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:06:38,477 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_mlp_gate_proj.safetensors2025-04-27 23:06:38,477 - INFO - exists: True2025-04-27 23:06:38,493 - INFO - factorize_layer_kron_svd2025-04-27 23:06:42,768 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:06:45,669 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:06:47,895 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:06:52,895 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:06:59,409 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_mlp_gate_proj.safetensors TrueLayer: model.layers.11.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [2.99881306e-03 1.06764026e-04 2.91248271e-05 2.53626113e-05 2.07210996e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:08:22,484 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:08:22,485 - INFO - Replacing 'model.layers.11.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  36%|████████████████████████████████████████████████████████████▊                                                                                                          | 82/225 [42:13<55:44, 23.39s/it]2025-04-27 23:08:22,485 - INFO - Skipping layer model.layers.11.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:08:22,485 - INFO - Skipping layer model.layers.11.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:08:22,485 - INFO - Layer: model.layers.12.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:08:22,489 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_q_proj.safetensors2025-04-27 23:08:22,489 - INFO - exists: True2025-04-27 23:08:22,611 - INFO - factorize_layer_kron_svd2025-04-27 23:08:26,813 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:08:30,608 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:08:34,609 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:08:38,072 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:08:41,700 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:08:43,975 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_q_proj.safetensors TrueLayer: model.layers.12.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [9.15819407e-03 3.75725394e-05 2.34514755e-05 1.29712425e-05 1.20524883e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:09:44,399 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:09:44,399 - INFO - Replacing 'model.layers.12.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  38%|███████████████████████████████████████████████████████████████                                                                                                        | 85/225 [43:35<56:33, 24.24s/it]2025-04-27 23:09:44,399 - INFO - Layer: model.layers.12.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:09:44,400 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_k_proj.safetensors2025-04-27 23:09:44,400 - INFO - exists: True2025-04-27 23:09:44,410 - INFO - factorize_layer_kron_svd2025-04-27 23:09:46,141 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:09:47,737 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:09:49,376 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:09:51,041 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:09:52,541 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:09:55,255 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:09:56,736 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:09:58,398 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:10:00,397 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:10:02,293 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:10:05,110 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:10:06,752 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_k_proj.safetensors TrueLayer: model.layers.12.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.35002065e-05 7.74964155e-06 7.61668571e-06 7.45528405e-06 7.28192163e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:11:18,804 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:11:18,804 - INFO - Replacing 'model.layers.12.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  38%|███████████████████████████████████████████████████████████████                                                                                                      | 86/225 [45:10<1:11:27, 30.85s/it]2025-04-27 23:11:18,805 - INFO - Skipping layer model.layers.12.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:11:18,805 - INFO - Skipping layer model.layers.12.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:11:18,805 - INFO - Layer: model.layers.12.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:11:18,847 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_gate_proj.safetensors2025-04-27 23:11:18,848 - INFO - exists: True2025-04-27 23:11:18,884 - INFO - factorize_layer_kron_svd2025-04-27 23:11:21,057 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:11:23,168 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:11:24,803 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:11:26,612 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:11:29,867 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:11:33,576 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:11:38,711 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:11:42,179 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:11:45,693 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:11:48,387 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:11:51,189 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:11:56,613 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_gate_proj.safetensors TrueLayer: model.layers.12.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.2988171e-05 7.1681275e-06 6.5595987e-06 6.5212694e-06 6.4388596e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:14:05,771 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:14:05,771 - INFO - Replacing 'model.layers.12.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  40%|█████████████████████████████████████████████████████████████████▎                                                                                                   | 89/225 [47:57<1:26:05, 37.98s/it]2025-04-27 23:14:05,772 - INFO - Layer: model.layers.12.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:14:05,777 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_up_proj.safetensors2025-04-27 23:14:05,777 - INFO - exists: True2025-04-27 23:14:05,872 - INFO - factorize_layer_kron_svd2025-04-27 23:14:09,703 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:14:12,891 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:14:14,991 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:14:16,459 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:14:19,594 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:14:23,298 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:14:28,271 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:14:32,469 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:14:35,202 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:14:39,529 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:14:42,142 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:14:47,293 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_up_proj.safetensors TrueLayer: model.layers.12.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [5.9819213e-06 4.8709753e-06 4.7432140e-06 4.6781370e-06 4.6636528e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:16:48,168 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:16:48,169 - INFO - Replacing 'model.layers.12.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  40%|██████████████████████████████████████████████████████████████████                                                                                                   | 90/225 [50:39<1:59:09, 52.96s/it]2025-04-27 23:16:48,169 - INFO - Skipping layer model.layers.12.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:16:48,169 - INFO - Layer: model.layers.13.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:16:48,173 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_q_proj.safetensors2025-04-27 23:16:48,173 - INFO - exists: True2025-04-27 23:16:48,195 - INFO - factorize_layer_kron_svd2025-04-27 23:16:50,072 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:16:53,888 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:16:57,573 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:17:00,676 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:17:02,448 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:17:05,186 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_q_proj.safetensors TrueLayer: model.layers.13.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.9350363e-02 5.4489075e-05 1.5019226e-05 1.3564231e-05 1.1421780e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:17:43,615 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:17:43,615 - INFO - Replacing 'model.layers.13.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  41%|███████████████████████████████████████████████████████████████████▍                                                                                                 | 92/225 [51:35<1:43:04, 46.50s/it]2025-04-27 23:17:43,616 - INFO - Layer: model.layers.13.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:17:43,617 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_k_proj.safetensors2025-04-27 23:17:43,617 - INFO - exists: True2025-04-27 23:17:43,631 - INFO - factorize_layer_kron_svd2025-04-27 23:17:46,476 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:17:49,478 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:17:52,581 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:17:55,597 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:17:58,679 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:18:02,197 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:18:05,179 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:18:08,293 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:18:11,364 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:18:14,369 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:18:17,383 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:18:21,163 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_k_proj.safetensors TrueLayer: model.layers.13.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.3729892e-05 7.1575278e-06 7.1094173e-06 6.8093209e-06 6.5548347e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:19:20,406 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:19:20,407 - INFO - Replacing 'model.layers.13.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  41%|████████████████████████████████████████████████████████████████████▏                                                                                                | 93/225 [53:11<1:59:24, 54.27s/it]2025-04-27 23:19:20,407 - INFO - Layer: model.layers.13.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:19:20,410 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_v_proj.safetensors2025-04-27 23:19:20,410 - INFO - exists: True2025-04-27 23:19:20,432 - INFO - factorize_layer_kron_svd2025-04-27 23:19:22,236 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:19:24,001 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:19:26,543 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:19:28,281 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:19:30,086 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_v_proj.safetensors TrueLayer: model.layers.13.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [5.0983722e-03 5.0585440e-05 2.1063875e-05 1.7282286e-05 1.4230653e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:20:14,772 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:20:14,772 - INFO - Replacing 'model.layers.13.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)Compressing Layers:  42%|████████████████████████████████████████████████████████████████████▉                                                                                                | 94/225 [54:06<1:58:32, 54.29s/it]2025-04-27 23:20:14,773 - INFO - Layer: model.layers.13.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:20:14,775 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_o_proj.safetensors2025-04-27 23:20:14,775 - INFO - exists: True2025-04-27 23:20:14,783 - INFO - factorize_layer_kron_svd2025-04-27 23:20:18,769 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:20:23,111 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:20:27,506 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:20:30,763 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:20:34,290 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:20:37,989 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_o_proj.safetensors TrueLayer: model.layers.13.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.2446343e-02 3.8981681e-05 3.5099336e-05 2.8158551e-05 2.2302480e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:21:41,117 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:21:41,117 - INFO - Replacing 'model.layers.13.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)Compressing Layers:  42%|█████████████████████████████████████████████████████████████████████▋                                                                                               | 95/225 [55:32<2:11:53, 60.87s/it]2025-04-27 23:21:41,118 - INFO - Layer: model.layers.13.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:21:41,163 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_gate_proj.safetensors2025-04-27 23:21:41,163 - INFO - exists: True2025-04-27 23:21:41,172 - INFO - factorize_layer_kron_svd2025-04-27 23:21:44,012 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:21:45,797 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:21:47,477 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:21:50,587 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:21:58,635 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_gate_proj.safetensors TrueLayer: model.layers.13.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [6.0720551e-03 4.1635503e-05 2.5478646e-05 2.1152418e-05 1.8824872e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:24:07,077 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:24:07,077 - INFO - Replacing 'model.layers.13.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  43%|██████████████████████████████████████████████████████████████████████▍                                                                                              | 96/225 [57:58<2:52:22, 80.17s/it]2025-04-27 23:24:07,077 - INFO - Layer: model.layers.13.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:24:07,080 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_up_proj.safetensors2025-04-27 23:24:07,080 - INFO - exists: True2025-04-27 23:24:07,133 - INFO - factorize_layer_kron_svd2025-04-27 23:24:09,070 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:24:12,203 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:24:16,173 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:24:20,520 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:24:24,813 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:24:30,279 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_up_proj.safetensors TrueLayer: model.layers.13.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [6.4682798e-03 6.1749437e-05 2.4349400e-05 2.1919277e-05 2.0116417e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:26:00,575 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:26:00,575 - INFO - Replacing 'model.layers.13.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  43%|███████████████████████████████████████████████████████████████████████▏                                                                                             | 97/225 [59:51<3:08:25, 88.33s/it]2025-04-27 23:26:00,576 - INFO - Skipping layer model.layers.13.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,576 - INFO - Skipping layer model.layers.14.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,576 - INFO - Skipping layer model.layers.14.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,576 - INFO - Skipping layer model.layers.14.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,576 - INFO - Skipping layer model.layers.14.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,576 - INFO - Layer: model.layers.14.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:26:00,579 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_gate_proj.safetensors2025-04-27 23:26:00,579 - INFO - exists: True2025-04-27 23:26:00,637 - INFO - factorize_layer_kron_svd2025-04-27 23:26:02,410 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:26:04,020 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:26:05,676 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:26:08,089 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:26:12,470 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_gate_proj.safetensors TrueLayer: model.layers.14.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [3.2148813e-03 8.8560395e-05 2.4315630e-05 1.8305876e-05 1.6133905e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:27:23,753 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:27:23,754 - INFO - Replacing 'model.layers.14.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  46%|██████████████████████████████████████████████████████████████████████████▏                                                                                       | 103/225 [1:01:15<1:17:04, 37.90s/it]2025-04-27 23:27:23,754 - INFO - Layer: model.layers.14.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:27:23,756 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_up_proj.safetensors2025-04-27 23:27:23,756 - INFO - exists: True2025-04-27 23:27:23,774 - INFO - factorize_layer_kron_svd2025-04-27 23:27:25,648 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:27:27,342 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:27:29,128 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:27:31,523 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:27:35,642 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:27:40,357 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_up_proj.safetensors TrueLayer: model.layers.14.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [9.7202640e-03 2.5967920e-05 2.1275189e-05 1.6423986e-05 1.5875536e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:28:52,972 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:28:52,972 - INFO - Replacing 'model.layers.14.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  46%|██████████████████████████████████████████████████████████████████████████▉                                                                                       | 104/225 [1:02:44<1:30:48, 45.03s/it]2025-04-27 23:28:52,972 - INFO - Skipping layer model.layers.14.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:28:52,973 - INFO - Layer: model.layers.15.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:28:52,975 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_self_attn_q_proj.safetensors2025-04-27 23:28:52,975 - INFO - exists: True2025-04-27 23:28:53,031 - INFO - factorize_layer_kron_svd2025-04-27 23:28:54,776 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:28:56,495 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:28:58,235 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:28:59,840 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:29:01,432 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:29:03,237 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_self_attn_q_proj.safetensors TrueLayer: model.layers.15.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.19919665e-02 7.38329763e-05 2.44418788e-05 1.36780509e-05 1.00903217e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:29:40,220 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:29:40,220 - INFO - Replacing 'model.layers.15.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  47%|████████████████████████████████████████████████████████████████████████████▎                                                                                     | 106/225 [1:03:31<1:17:14, 38.95s/it]2025-04-27 23:29:40,221 - INFO - Skipping layer model.layers.15.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:29:40,221 - INFO - Skipping layer model.layers.15.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:29:40,221 - INFO - Skipping layer model.layers.15.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:29:40,221 - INFO - Skipping layer model.layers.15.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:29:40,221 - INFO - Skipping layer model.layers.15.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:29:40,221 - INFO - Skipping layer model.layers.15.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:29:40,221 - INFO - Skipping layer model.layers.16.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:29:40,221 - INFO - Skipping layer model.layers.16.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:29:40,221 - INFO - Layer: model.layers.16.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:29:40,222 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_self_attn_v_proj.safetensors2025-04-27 23:29:40,223 - INFO - exists: True2025-04-27 23:29:40,239 - INFO - factorize_layer_kron_svd2025-04-27 23:29:42,017 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:29:43,747 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:29:45,225 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:29:46,925 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:29:48,565 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_self_attn_v_proj.safetensors TrueLayer: model.layers.16.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [2.1955712e-02 2.2352620e-05 1.8913308e-05 1.6113720e-05 1.5723988e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:30:24,416 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:30:24,417 - INFO - Replacing 'model.layers.16.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)Compressing Layers:  51%|███████████████████████████████████████████████████████████████████████████████████▊                                                                                | 115/225 [1:04:15<31:05, 16.96s/it]2025-04-27 23:30:24,417 - INFO - Skipping layer model.layers.16.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:30:24,417 - INFO - Layer: model.layers.16.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:30:24,420 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_mlp_gate_proj.safetensors2025-04-27 23:30:24,420 - INFO - exists: True2025-04-27 23:30:24,435 - INFO - factorize_layer_kron_svd2025-04-27 23:30:26,393 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:30:28,033 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:30:29,764 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:30:32,119 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:30:36,639 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_mlp_gate_proj.safetensors TrueLayer: model.layers.16.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [3.0645484e-03 2.1843742e-05 8.2429324e-06 3.9983265e-06 3.0144206e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:31:45,161 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:31:45,161 - INFO - Replacing 'model.layers.16.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  52%|█████████████████████████████████████████████████████████████████████████████████████▎                                                                              | 117/225 [1:05:36<37:41, 20.94s/it]2025-04-27 23:31:45,162 - INFO - Skipping layer model.layers.16.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:31:45,162 - INFO - Skipping layer model.layers.16.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:31:45,162 - INFO - Layer: model.layers.17.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:31:45,165 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_self_attn_q_proj.safetensors2025-04-27 23:31:45,165 - INFO - exists: True2025-04-27 23:31:45,212 - INFO - factorize_layer_kron_svd2025-04-27 23:31:46,814 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:31:48,437 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:31:50,145 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:31:51,692 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:31:53,328 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:31:55,043 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_self_attn_q_proj.safetensors TrueLayer: model.layers.17.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.9799747e-02 3.0997286e-05 1.8318075e-05 1.5734129e-05 9.7556685e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:32:32,792 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:32:32,792 - INFO - Replacing 'model.layers.17.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  53%|███████████████████████████████████████████████████████████████████████████████████████▍                                                                            | 120/225 [1:06:24<34:16, 19.59s/it]2025-04-27 23:32:32,793 - INFO - Skipping layer model.layers.17.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:32:32,793 - INFO - Skipping layer model.layers.17.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:32:32,793 - INFO - Skipping layer model.layers.17.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:32:32,793 - INFO - Layer: model.layers.17.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:32:32,795 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_mlp_gate_proj.safetensors2025-04-27 23:32:32,795 - INFO - exists: True2025-04-27 23:32:32,803 - INFO - factorize_layer_kron_svd2025-04-27 23:32:34,778 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:32:36,428 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:32:38,234 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:32:40,725 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:32:45,557 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_mlp_gate_proj.safetensors TrueLayer: model.layers.17.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.2734500e-02 1.9533880e-05 1.8360362e-05 1.5285072e-05 1.4013754e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:34:03,767 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:34:03,767 - INFO - Replacing 'model.layers.17.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  55%|██████████████████████████████████████████████████████████████████████████████████████████▍                                                                         | 124/225 [1:07:55<34:45, 20.65s/it]2025-04-27 23:34:03,767 - INFO - Skipping layer model.layers.17.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:34:03,767 - INFO - Skipping layer model.layers.17.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:34:03,768 - INFO - Layer: model.layers.18.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:34:03,769 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_q_proj.safetensors2025-04-27 23:34:03,769 - INFO - exists: True2025-04-27 23:34:03,813 - INFO - factorize_layer_kron_svd2025-04-27 23:34:05,375 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:34:07,136 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:34:08,842 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:34:10,383 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:34:11,942 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:34:13,748 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_q_proj.safetensors TrueLayer: model.layers.18.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.0405821e-03 8.4903695e-05 8.4368939e-06 7.6583383e-06 6.2006247e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:34:51,944 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:34:51,944 - INFO - Replacing 'model.layers.18.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  56%|████████████████████████████████████████████████████████████████████████████████████████████▌                                                                       | 127/225 [1:08:43<31:44, 19.43s/it]2025-04-27 23:34:51,945 - INFO - Layer: model.layers.18.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:34:51,947 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_k_proj.safetensors2025-04-27 23:34:51,947 - INFO - exists: True2025-04-27 23:34:51,960 - INFO - factorize_layer_kron_svd2025-04-27 23:34:53,632 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:34:55,337 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:34:57,136 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:34:58,721 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:35:00,513 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:35:02,269 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_k_proj.safetensors TrueLayer: model.layers.18.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [7.7125463e-03 3.3378132e-05 8.2256274e-06 7.4304949e-06 6.1785499e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:35:40,808 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:35:40,808 - INFO - Replacing 'model.layers.18.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  57%|█████████████████████████████████████████████████████████████████████████████████████████████▎                                                                      | 128/225 [1:09:32<36:45, 22.74s/it]2025-04-27 23:35:40,808 - INFO - Skipping layer model.layers.18.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:35:40,809 - INFO - Skipping layer model.layers.18.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:35:40,809 - INFO - Layer: model.layers.18.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:35:40,811 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_mlp_gate_proj.safetensors2025-04-27 23:35:40,811 - INFO - exists: True2025-04-27 23:35:40,832 - INFO - factorize_layer_kron_svd2025-04-27 23:35:42,818 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:35:44,436 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:35:46,180 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:35:48,817 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:35:53,502 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_mlp_gate_proj.safetensors TrueLayer: model.layers.18.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [8.0129793e-03 1.8024977e-05 1.6487887e-05 1.3588725e-05 1.1236772e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:37:11,767 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:37:11,768 - INFO - Replacing 'model.layers.18.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  58%|███████████████████████████████████████████████████████████████████████████████████████████████▍                                                                    | 131/225 [1:11:03<39:28, 25.20s/it]2025-04-27 23:37:11,768 - INFO - Skipping layer model.layers.18.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:37:11,768 - INFO - Skipping layer model.layers.18.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:37:11,768 - INFO - Layer: model.layers.19.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:37:11,771 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_self_attn_q_proj.safetensors2025-04-27 23:37:11,771 - INFO - exists: True2025-04-27 23:37:11,838 - INFO - factorize_layer_kron_svd2025-04-27 23:37:13,537 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:37:15,321 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:37:16,812 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:37:18,689 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_self_attn_q_proj.safetensors TrueLayer: model.layers.19.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [2.2634035e-03 7.5883843e-05 1.0972199e-05 7.9327338e-06 7.6394726e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:37:58,121 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:37:58,122 - INFO - Replacing 'model.layers.19.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  60%|█████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                  | 134/225 [1:11:49<33:31, 22.11s/it]2025-04-27 23:37:58,123 - INFO - Skipping layer model.layers.19.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:37:58,123 - INFO - Skipping layer model.layers.19.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:37:58,123 - INFO - Skipping layer model.layers.19.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:37:58,123 - INFO - Layer: model.layers.19.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:37:58,124 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_mlp_gate_proj.safetensors2025-04-27 23:37:58,124 - INFO - exists: True2025-04-27 23:37:58,141 - INFO - factorize_layer_kron_svd2025-04-27 23:38:00,205 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:38:01,893 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:38:03,612 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:38:06,112 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:38:10,858 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_mlp_gate_proj.safetensors TrueLayer: model.layers.19.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [6.8501011e-03 1.8891638e-05 1.4157393e-05 1.3611631e-05 1.2496623e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:39:31,862 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:39:31,863 - INFO - Replacing 'model.layers.19.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  61%|████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                               | 138/225 [1:13:23<32:46, 22.61s/it]2025-04-27 23:39:31,863 - INFO - Skipping layer model.layers.19.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,863 - INFO - Skipping layer model.layers.19.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,863 - INFO - Skipping layer model.layers.20.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,863 - INFO - Skipping layer model.layers.20.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,863 - INFO - Skipping layer model.layers.20.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,863 - INFO - Skipping layer model.layers.20.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,863 - INFO - Skipping layer model.layers.20.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,864 - INFO - Skipping layer model.layers.20.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,864 - INFO - Skipping layer model.layers.20.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,864 - INFO - Skipping layer model.layers.21.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,864 - INFO - Skipping layer model.layers.21.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,864 - INFO - Skipping layer model.layers.21.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,864 - INFO - Skipping layer model.layers.21.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,864 - INFO - Skipping layer model.layers.21.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,864 - INFO - Skipping layer model.layers.21.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,864 - INFO - Skipping layer model.layers.21.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,864 - INFO - Skipping layer model.layers.22.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,864 - INFO - Skipping layer model.layers.22.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,864 - INFO - Skipping layer model.layers.22.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,864 - INFO - Skipping layer model.layers.22.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:31,864 - INFO - Layer: model.layers.22.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:39:31,871 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_gate_proj.safetensors2025-04-27 23:39:31,871 - INFO - exists: True2025-04-27 23:39:31,924 - INFO - factorize_layer_kron_svd2025-04-27 23:39:33,814 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:39:35,329 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:39:36,855 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:39:38,347 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:39:39,847 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:39:41,618 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:39:43,852 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:39:46,407 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:39:48,873 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:39:51,378 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:39:53,841 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:39:58,617 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_gate_proj.safetensors TrueLayer: model.layers.22.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.1882697e-05 6.5561608e-06 6.1485889e-06 6.0917669e-06 5.8484889e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:41:20,562 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:41:20,563 - INFO - Replacing 'model.layers.22.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                | 159/225 [1:15:11<10:42,  9.74s/it]2025-04-27 23:41:20,563 - INFO - Skipping layer model.layers.22.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,563 - INFO - Skipping layer model.layers.22.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,563 - INFO - Skipping layer model.layers.23.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,563 - INFO - Skipping layer model.layers.23.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,563 - INFO - Skipping layer model.layers.23.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,563 - INFO - Skipping layer model.layers.23.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,563 - INFO - Skipping layer model.layers.23.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,563 - INFO - Skipping layer model.layers.23.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.23.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.24.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.24.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.24.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.24.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.24.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.24.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.24.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.25.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.25.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.25.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.25.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.25.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.25.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.25.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.26.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.26.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.26.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.26.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.26.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.26.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.26.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.27.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.27.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.27.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.27.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,564 - INFO - Skipping layer model.layers.27.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,565 - INFO - Skipping layer model.layers.27.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,565 - INFO - Skipping layer model.layers.27.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,565 - INFO - Skipping layer model.layers.28.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:41:20,565 - INFO - Layer: model.layers.28.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:41:20,567 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_28_self_attn_k_proj.safetensors2025-04-27 23:41:20,567 - INFO - exists: True2025-04-27 23:41:20,608 - INFO - factorize_layer_kron_svd2025-04-27 23:41:22,120 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:41:23,657 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:41:25,141 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:41:26,609 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:41:28,125 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:41:29,976 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:41:31,447 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:41:32,937 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:41:34,411 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:41:35,912 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:41:37,415 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:41:39,283 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_28_self_attn_k_proj.safetensors TrueLayer: model.layers.28.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.3403688e-05 6.0787293e-06 5.6532840e-06 5.4129196e-06 5.2258588e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:42:20,716 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:42:20,716 - INFO - Replacing 'model.layers.28.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 198/225 [1:16:12<01:56,  4.31s/it]2025-04-27 23:42:20,716 - INFO - Skipping layer model.layers.28.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,716 - INFO - Skipping layer model.layers.28.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,716 - INFO - Skipping layer model.layers.28.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,716 - INFO - Skipping layer model.layers.28.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.28.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.29.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.29.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.29.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.29.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.29.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.29.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.29.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.30.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.30.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.30.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.30.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.30.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.30.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.30.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.31.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.31.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.31.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.31.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.31.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.31.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer model.layers.31.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:20,717 - INFO - Skipping layer lm_head: Sensitivity ratio >= 1.0 (1.00). No compression.Compressing Layers: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [1:16:12<00:00, 20.32s/it]2025-04-27 23:42:20,718 - INFO - Compression finished. Processed: 49, Skipped (Ratio>=1 or No Sensitivity/Factors): 176, Failed: 02025-04-27 23:42:20,720 - INFO - Total parameters after compression: 59268300802025-04-27 23:42:20,720 - INFO - C rate : 0.87955840330285742025-04-27 23:42:20,720 - INFO - Saving compressed model to ./llama102025-04-27 23:42:20,720 - INFO - Compressed model and tokenizer saved.2025-04-27 23:42:20,735 - INFO - Evaluating on wikitext2Evaluating:   0%|                                                                                                                                                                                         | 0/21 [00:00<?, ?it/s]Evaluating:   5%|████████▍                                                                                                                                                                        | 1/21 [00:02<00:59,  2.95s/it]Evaluating:  10%|████████████████▊                                                                                                                                                                | 2/21 [00:04<00:35,  1.85s/it]Evaluating:  14%|█████████████████████████▎                                                                                                                                                       | 3/21 [00:05<00:26,  1.50s/it]Evaluating:  19%|█████████████████████████████████▋                                                                                                                                               | 4/21 [00:06<00:22,  1.33s/it]Evaluating:  24%|██████████████████████████████████████████▏                                                                                                                                      | 5/21 [00:07<00:19,  1.24s/it]Evaluating:  29%|██████████████████████████████████████████████████▌                                                                                                                              | 6/21 [00:08<00:17,  1.19s/it]Evaluating:  33%|███████████████████████████████████████████████████████████                                                                                                                      | 7/21 [00:09<00:16,  1.15s/it]Evaluating:  38%|███████████████████████████████████████████████████████████████████▍                                                                                                             | 8/21 [00:10<00:14,  1.13s/it]Evaluating:  43%|███████████████████████████████████████████████████████████████████████████▊                                                                                                     | 9/21 [00:11<00:13,  1.12s/it]Evaluating:  48%|███████████████████████████████████████████████████████████████████████████████████▊                                                                                            | 10/21 [00:12<00:12,  1.11s/it]Evaluating:  52%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                   | 11/21 [00:13<00:11,  1.10s/it]Evaluating:  57%|████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                           | 12/21 [00:14<00:09,  1.10s/it]Evaluating:  62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                   | 13/21 [00:15<00:08,  1.09s/it]Evaluating:  67%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                          | 14/21 [00:17<00:07,  1.09s/it]Evaluating:  71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                  | 15/21 [00:18<00:06,  1.09s/it]Evaluating:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                          | 16/21 [00:19<00:05,  1.09s/it]Evaluating:  81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                 | 17/21 [00:20<00:04,  1.09s/it]Evaluating:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                         | 18/21 [00:21<00:03,  1.09s/it]Evaluating:  90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                | 19/21 [00:22<00:02,  1.10s/it]Evaluating:  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 20/21 [00:23<00:01,  1.09s/it]Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:24<00:00,  1.02s/it]Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:24<00:00,  1.16s/it]2025-04-27 23:42:52,991 - INFO - wikitext2 perplexity: 10.25002025-04-27 23:42:52,991 - INFO - Evaluating on ptbnlls.shape torch.Size([16376])Mean NLL: 2.328125Evaluating:   0%|                                                                                                                                                                                          | 0/7 [00:00<?, ?it/s]Evaluating:  14%|█████████████████████████▍                                                                                                                                                        | 1/7 [00:01<00:06,  1.11s/it]Evaluating:  29%|██████████████████████████████████████████████████▊                                                                                                                               | 2/7 [00:02<00:05,  1.10s/it]Evaluating:  43%|████████████████████████████████████████████████████████████████████████████▎                                                                                                     | 3/7 [00:03<00:04,  1.09s/it]Evaluating:  57%|█████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                            | 4/7 [00:04<00:03,  1.09s/it]Evaluating:  71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                  | 5/7 [00:05<00:02,  1.09s/it]Evaluating:  86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 6/7 [00:06<00:01,  1.09s/it]Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:07<00:00,  1.05s/it]Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:07<00:00,  1.07s/it]2025-04-27 23:43:03,679 - INFO - ptb perplexity: 49.00002025-04-27 23:43:03,679 - INFO - Evaluation results:2025-04-27 23:43:03,679 - INFO -   wikitext2: 10.25002025-04-27 23:43:03,679 - INFO -   ptb: 49.0000nlls.shape torch.Size([16376])Mean NLL: 3.890625
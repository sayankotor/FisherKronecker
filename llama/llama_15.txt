2025-04-27 22:57:56,495 - INFO - Loading model: unsloth/llama-2-7b-chatmodel.layers.29.self_attn.q_proj 0.3model.layers.29.self_attn.k_proj 0.3model.layers.28.self_attn.q_proj 0.3model.layers.28.self_attn.k_proj 0.3model.layers.27.self_attn.q_proj 0.3model.layers.27.self_attn.k_proj 0.3model.layers.26.mlp.gate_proj 0.5model.layers.26.mlp.up_proj 0.5model.layers.26.self_attn.v_proj 0.3model.layers.26.self_attn.o_proj 0.3model.layers.25.mlp.up_proj 0.5model.layers.24.self_attn.q_proj 0.3model.layers.24.self_attn.k_proj 0.3model.layers.24.self_attn.v_proj 0.3model.layers.23.mlp.gate_proj 0.5model.layers.23.mlp.down_proj 0.5model.layers.22.mlp.up_proj 0.5model.layers.22.mlp.down_proj 0.5model.layers.21.mlp.up_proj 0.5model.layers.21.mlp.down_proj 0.5model.layers.21.self_attn.q_proj 0.3model.layers.21.self_attn.k_proj 0.3model.layers.21.self_attn.v_proj 0.3model.layers.20.mlp.gate_proj 0.5model.layers.19.mlp.up_proj 0.5model.layers.19.self_attn.q_proj 0.3model.layers.18.mlp.up_proj 0.5model.layers.18.self_attn.q_proj 0.3model.layers.18.self_attn.k_proj 0.3model.layers.17.self_attn.q_proj 0.3model.layers.16.self_attn.v_proj 0.3model.layers.16.self_attn.o_proj 0.3model.layers.15.self_attn.q_proj 0.3model.layers.15.self_attn.v_proj 0.3model.layers.15.self_attn.o_proj 0.3model.layers.14.mlp.gate_proj 0.5model.layers.14.mlp.up_proj 0.5model.layers.14.self_attn.v_proj 0.3model.layers.14.self_attn.o_proj 0.3model.layers.13.mlp.up_proj 0.5model.layers.13.self_attn.q_proj 0.3model.layers.13.self_attn.k_proj 0.3model.layers.13.self_attn.v_proj 0.3model.layers.13.self_attn.o_proj 0.3model.layers.12.mlp.gate_proj 0.5model.layers.12.mlp.up_proj 0.5model.layers.12.self_attn.q_proj 0.3model.layers.12.self_attn.k_proj 0.3model.layers.12.self_attn.v_proj 0.3model.layers.12.self_attn.o_proj 0.3model.layers.11.self_attn.q_proj 0.3model.layers.11.self_attn.o_proj 0.3model.layers.9.self_attn.v_proj 0.3model.layers.8.self_attn.q_proj 0.3model.layers.8.self_attn.k_proj 0.3model.layers.7.mlp.down_proj 0.5model.layers.7.self_attn.q_proj 0.3model.layers.7.self_attn.k_proj 0.3model.layers.6.self_attn.q_proj 0.3model.layers.6.self_attn.k_proj 0.3model.layers.5.self_attn.k_proj 0.3model.layers.4.self_attn.q_proj 0.3model.layers.4.self_attn.k_proj 0.3model.layers.4.self_attn.v_proj 0.3model.layers.3.self_attn.q_proj 0.3model.layers.3.self_attn.k_proj 0.3model.layers.2.mlp.gate_proj 0.5model.layers.2.self_attn.q_proj 0.3model.layers.2.self_attn.k_proj 0.3[2025-04-27 22:58:01,404] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Loading checkpoint shards:   0%|                                                                                                                                                                      | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|████████████████████████████████████████████████████▋                                                                                                         | 1/3 [00:00<00:01,  1.13it/s]Loading checkpoint shards:  67%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                    | 2/3 [00:01<00:00,  1.32it/s]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.62it/s]Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:02<00:00,  1.49it/s]2025-04-27 22:58:05,571 - INFO - Model loaded with dtype torch.bfloat162025-04-27 22:58:42,730 - INFO - Model moved to cuda:02025-04-27 22:58:42,731 - INFO - Total parameters before compression: 67384156162025-04-27 22:58:42,731 - INFO - Found 225 linear layers to potentially compress.Compressing Layers:   0%|                                                                                                                                                                           | 0/225 [00:00<?, ?it/s]2025-04-27 22:58:42,732 - INFO - Skipping layer model.layers.0.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:42,732 - INFO - Skipping layer model.layers.0.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:42,733 - INFO - Skipping layer model.layers.0.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:42,733 - INFO - Skipping layer model.layers.0.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:42,733 - INFO - Skipping layer model.layers.0.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:42,733 - INFO - Skipping layer model.layers.0.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:42,733 - INFO - Skipping layer model.layers.0.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:42,733 - INFO - Skipping layer model.layers.1.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:42,733 - INFO - Skipping layer model.layers.1.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:42,733 - INFO - Skipping layer model.layers.1.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:42,733 - INFO - Skipping layer model.layers.1.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:42,733 - INFO - Skipping layer model.layers.1.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:42,733 - INFO - Skipping layer model.layers.1.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:42,733 - INFO - Skipping layer model.layers.1.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:42,733 - INFO - Layer: model.layers.2.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:58:42,734 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_q_proj.safetensors2025-04-27 22:58:42,734 - INFO - exists: True2025-04-27 22:58:42,736 - INFO - factorize_layer_kron_svd2025-04-27 22:58:43,971 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:58:45,027 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:58:46,066 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:58:47,121 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:58:48,171 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:58:49,430 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:58:50,468 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:58:51,519 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:58:52,567 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:58:53,632 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:58:54,686 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:58:56,078 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_q_proj.safetensors TrueLayer: model.layers.2.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.6124124e-05 1.1238000e-05 1.0046171e-05 9.5376463e-06 9.3023737e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:59:23,615 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:59:23,616 - INFO - Replacing 'model.layers.2.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:   7%|██████████▊                                                                                                                                                       | 15/225 [00:40<09:32,  2.73s/it]2025-04-27 22:59:23,616 - INFO - Layer: model.layers.2.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:59:23,617 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_k_proj.safetensors2025-04-27 22:59:23,617 - INFO - exists: True2025-04-27 22:59:23,620 - INFO - factorize_layer_kron_svd2025-04-27 22:59:24,743 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:59:25,990 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:59:27,051 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:59:28,286 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_k_proj.safetensors TrueLayer: model.layers.2.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.0046256e-03 1.5359199e-04 9.4199255e-05 5.9006747e-05 4.1726034e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:59:57,068 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:59:57,068 - INFO - Replacing 'model.layers.2.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:   7%|███████████▌                                                                                                                                                      | 16/225 [01:14<18:48,  5.40s/it]2025-04-27 22:59:57,068 - INFO - Skipping layer model.layers.2.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:59:57,068 - INFO - Skipping layer model.layers.2.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:59:57,068 - INFO - Layer: model.layers.2.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:59:57,070 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_gate_proj.safetensors2025-04-27 22:59:57,070 - INFO - exists: True2025-04-27 22:59:57,076 - INFO - factorize_layer_kron_svd2025-04-27 22:59:58,285 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:59:59,330 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:00:00,376 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:00:01,431 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:00:02,478 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:00:03,701 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:00:05,323 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:00:07,202 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:00:08,998 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:00:10,790 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:00:12,619 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:00:16,212 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_gate_proj.safetensors TrueLayer: model.layers.2.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.0536157e-05 7.8889116e-06 7.3416263e-06 6.9958951e-06 6.9346693e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:01:19,284 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:01:19,285 - INFO - Replacing 'model.layers.2.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:   8%|█████████████▋                                                                                                                                                    | 19/225 [02:36<39:02, 11.37s/it]2025-04-27 23:01:19,285 - INFO - Skipping layer model.layers.2.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:01:19,285 - INFO - Skipping layer model.layers.2.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:01:19,285 - INFO - Layer: model.layers.3.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:01:19,286 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_q_proj.safetensors2025-04-27 23:01:19,286 - INFO - exists: True2025-04-27 23:01:19,310 - INFO - factorize_layer_kron_svd2025-04-27 23:01:20,496 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:01:21,750 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:01:22,818 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:01:24,062 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_q_proj.safetensors TrueLayer: model.layers.3.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.1026561e-02 7.6449120e-05 6.9494883e-05 4.4865315e-05 4.0528874e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:01:51,272 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:01:51,272 - INFO - Replacing 'model.layers.3.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  10%|███████████████▊                                                                                                                                                  | 22/225 [03:08<37:48, 11.17s/it]2025-04-27 23:01:51,272 - INFO - Layer: model.layers.3.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:01:51,273 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_k_proj.safetensors2025-04-27 23:01:51,273 - INFO - exists: True2025-04-27 23:01:51,277 - INFO - factorize_layer_kron_svd2025-04-27 23:01:52,397 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:01:53,470 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:01:54,535 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:01:55,581 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:01:56,622 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:01:57,833 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:01:58,880 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:01:59,941 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:02:01,006 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:02:02,092 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:02:03,165 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:02:04,397 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_k_proj.safetensors TrueLayer: model.layers.3.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.5212615e-05 7.6301594e-06 6.8745117e-06 6.6968514e-06 6.6398402e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:02:33,531 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:02:33,531 - INFO - Replacing 'model.layers.3.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  10%|████████████████▌                                                                                                                                                 | 23/225 [03:50<49:54, 14.82s/it]2025-04-27 23:02:33,532 - INFO - Skipping layer model.layers.3.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:02:33,532 - INFO - Skipping layer model.layers.3.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:02:33,532 - INFO - Skipping layer model.layers.3.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:02:33,532 - INFO - Skipping layer model.layers.3.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:02:33,532 - INFO - Skipping layer model.layers.3.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:02:33,532 - INFO - Layer: model.layers.4.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:02:33,533 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_q_proj.safetensors2025-04-27 23:02:33,533 - INFO - exists: True2025-04-27 23:02:33,538 - INFO - factorize_layer_kron_svd2025-04-27 23:02:34,748 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:02:35,996 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:02:37,279 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:02:38,373 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:02:39,617 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_q_proj.safetensors TrueLayer: model.layers.4.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.2226075e-02 9.3771603e-05 6.9365597e-05 3.1357977e-05 2.4107103e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:03:08,959 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:03:08,960 - INFO - Replacing 'model.layers.4.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  13%|████████████████████▉                                                                                                                                             | 29/225 [04:26<33:48, 10.35s/it]2025-04-27 23:03:08,960 - INFO - Layer: model.layers.4.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:03:08,961 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_k_proj.safetensors2025-04-27 23:03:08,961 - INFO - exists: True2025-04-27 23:03:08,970 - INFO - factorize_layer_kron_svd2025-04-27 23:03:10,110 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:03:11,175 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:03:12,245 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:03:13,321 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:03:14,450 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:03:15,725 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:03:16,802 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:03:17,901 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:03:18,983 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:03:20,101 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:03:21,183 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:03:22,435 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_k_proj.safetensors TrueLayer: model.layers.4.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.5640973e-05 8.6818372e-06 8.1892204e-06 8.0699401e-06 7.7858249e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:03:51,122 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:03:51,122 - INFO - Replacing 'model.layers.4.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  13%|█████████████████████▌                                                                                                                                            | 30/225 [05:08<44:40, 13.74s/it]2025-04-27 23:03:51,122 - INFO - Layer: model.layers.4.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:03:51,124 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_v_proj.safetensors2025-04-27 23:03:51,124 - INFO - exists: True2025-04-27 23:03:51,131 - INFO - factorize_layer_kron_svd2025-04-27 23:03:52,341 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:03:53,628 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:03:54,741 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:03:55,951 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:03:57,218 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_v_proj.safetensors TrueLayer: model.layers.4.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.2608044e-03 1.0490499e-04 2.8823766e-05 1.9398125e-05 1.6777065e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:04:26,440 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:04:26,440 - INFO - Replacing 'model.layers.4.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)Compressing Layers:  14%|██████████████████████▎                                                                                                                                           | 31/225 [05:43<53:39, 16.60s/it]2025-04-27 23:04:26,440 - INFO - Skipping layer model.layers.4.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:04:26,440 - INFO - Skipping layer model.layers.4.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:04:26,440 - INFO - Skipping layer model.layers.4.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:04:26,440 - INFO - Skipping layer model.layers.4.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:04:26,440 - INFO - Skipping layer model.layers.5.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:04:26,440 - INFO - Layer: model.layers.5.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:04:26,441 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_5_self_attn_k_proj.safetensors2025-04-27 23:04:26,441 - INFO - exists: True2025-04-27 23:04:26,449 - INFO - factorize_layer_kron_svd2025-04-27 23:04:27,643 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:04:28,898 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:04:29,976 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:04:31,196 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_5_self_attn_k_proj.safetensors TrueLayer: model.layers.5.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.8379018e-03 1.5238064e-04 9.5437594e-05 3.4556335e-05 3.0823277e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:04:59,773 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:04:59,773 - INFO - Replacing 'model.layers.5.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  16%|██████████████████████████▋                                                                                                                                       | 37/225 [06:17<33:37, 10.73s/it]2025-04-27 23:04:59,773 - INFO - Skipping layer model.layers.5.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:04:59,773 - INFO - Skipping layer model.layers.5.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:04:59,773 - INFO - Skipping layer model.layers.5.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:04:59,773 - INFO - Skipping layer model.layers.5.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:04:59,773 - INFO - Skipping layer model.layers.5.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:04:59,773 - INFO - Layer: model.layers.6.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:04:59,775 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_q_proj.safetensors2025-04-27 23:04:59,775 - INFO - exists: True2025-04-27 23:04:59,784 - INFO - factorize_layer_kron_svd2025-04-27 23:05:00,890 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:05:01,968 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:05:03,037 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:05:04,096 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:05:05,149 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:05:06,359 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:05:07,391 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:05:08,423 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:05:09,456 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:05:10,494 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:05:11,528 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:05:12,742 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_q_proj.safetensors TrueLayer: model.layers.6.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.2221675e-05 8.1780718e-06 7.9897936e-06 7.7061068e-06 7.4714530e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:05:41,738 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:05:41,738 - INFO - Replacing 'model.layers.6.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  19%|██████████████████████████████▉                                                                                                                                   | 43/225 [06:59<27:39,  9.12s/it]2025-04-27 23:05:41,739 - INFO - Layer: model.layers.6.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:05:41,739 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_k_proj.safetensors2025-04-27 23:05:41,739 - INFO - exists: True2025-04-27 23:05:41,744 - INFO - factorize_layer_kron_svd2025-04-27 23:05:42,909 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:05:43,991 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:05:45,047 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:05:46,083 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:05:47,143 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:05:48,351 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:05:49,401 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:05:50,460 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:05:51,525 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:05:52,602 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:05:53,676 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:05:54,891 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_k_proj.safetensors TrueLayer: model.layers.6.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.4503589e-05 7.8944504e-06 7.5118478e-06 7.3252472e-06 7.0036058e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:06:23,834 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:06:23,834 - INFO - Replacing 'model.layers.6.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  20%|███████████████████████████████▋                                                                                                                                  | 44/225 [07:41<36:46, 12.19s/it]2025-04-27 23:06:23,834 - INFO - Skipping layer model.layers.6.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:06:23,834 - INFO - Skipping layer model.layers.6.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:06:23,834 - INFO - Skipping layer model.layers.6.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:06:23,834 - INFO - Skipping layer model.layers.6.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:06:23,834 - INFO - Skipping layer model.layers.6.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:06:23,834 - INFO - Layer: model.layers.7.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:06:23,836 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_q_proj.safetensors2025-04-27 23:06:23,837 - INFO - exists: True2025-04-27 23:06:23,849 - INFO - factorize_layer_kron_svd2025-04-27 23:06:25,033 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:06:26,282 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:06:27,394 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:06:28,663 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_q_proj.safetensors TrueLayer: model.layers.7.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.46691271e-03 1.17237636e-04 4.46072554e-05 2.93035046e-05 2.62491722e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:06:55,589 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:06:55,589 - INFO - Replacing 'model.layers.7.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  22%|████████████████████████████████████                                                                                                                              | 50/225 [08:12<26:37,  9.13s/it]2025-04-27 23:06:55,589 - INFO - Layer: model.layers.7.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:06:55,590 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_k_proj.safetensors2025-04-27 23:06:55,590 - INFO - exists: True2025-04-27 23:06:55,596 - INFO - factorize_layer_kron_svd2025-04-27 23:06:56,761 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:06:57,995 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:06:59,049 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:07:00,273 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_k_proj.safetensors TrueLayer: model.layers.7.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.3513221e-03 9.5677962e-05 5.9629248e-05 3.8686332e-05 2.6624961e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:07:28,738 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:07:28,738 - INFO - Replacing 'model.layers.7.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  23%|████████████████████████████████████▋                                                                                                                             | 51/225 [08:46<33:07, 11.42s/it]2025-04-27 23:07:28,738 - INFO - Skipping layer model.layers.7.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:07:28,738 - INFO - Skipping layer model.layers.7.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:07:28,738 - INFO - Skipping layer model.layers.7.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:07:28,738 - INFO - Skipping layer model.layers.7.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:07:28,738 - INFO - Layer: model.layers.7.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:07:28,740 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_down_proj.safetensors2025-04-27 23:07:28,740 - INFO - exists: True2025-04-27 23:07:28,746 - INFO - factorize_layer_kron_svd2025-04-27 23:07:30,692 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:07:34,382 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:07:35,434 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:07:36,607 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:07:37,864 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_down_proj.safetensors TrueLayer: model.layers.7.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=11008, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x11008)...  SVD complete. Singular values (top 5): [4.0638614e-03 2.6415018e-04 8.9995185e-05 2.9939682e-05 2.5749245e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,11008), (4096,1496)factorized_sequential Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))2025-04-27 23:08:48,368 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))')2025-04-27 23:08:48,368 - INFO - Replacing 'model.layers.7.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)Compressing Layers:  25%|████████████████████████████████████████▎                                                                                                                         | 56/225 [10:05<37:19, 13.25s/it]2025-04-27 23:08:48,369 - INFO - Layer: model.layers.8.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:08:48,371 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_q_proj.safetensors2025-04-27 23:08:48,371 - INFO - exists: True2025-04-27 23:08:48,389 - INFO - factorize_layer_kron_svd2025-04-27 23:08:49,568 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:08:50,815 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:08:52,078 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:08:53,199 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:08:54,451 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_q_proj.safetensors TrueLayer: model.layers.8.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [5.8933757e-03 1.1251512e-04 4.9954931e-05 2.4490531e-05 1.9172267e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:09:23,098 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:09:23,098 - INFO - Replacing 'model.layers.8.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  25%|█████████████████████████████████████████                                                                                                                         | 57/225 [10:40<43:21, 15.48s/it]2025-04-27 23:09:23,098 - INFO - Layer: model.layers.8.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:09:23,098 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_k_proj.safetensors2025-04-27 23:09:23,099 - INFO - exists: True2025-04-27 23:09:23,102 - INFO - factorize_layer_kron_svd2025-04-27 23:09:24,218 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:09:25,295 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:09:26,346 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:09:27,405 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:09:28,455 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:09:29,680 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:09:30,724 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:09:31,798 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:09:32,871 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:09:33,960 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:09:35,042 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:09:36,279 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_k_proj.safetensors TrueLayer: model.layers.8.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.4188925e-05 7.6309198e-06 7.4526779e-06 7.2449202e-06 6.7348119e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:10:05,565 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:10:05,565 - INFO - Replacing 'model.layers.8.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  26%|█████████████████████████████████████████▊                                                                                                                        | 58/225 [11:22<52:48, 18.97s/it]2025-04-27 23:10:05,566 - INFO - Skipping layer model.layers.8.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:05,566 - INFO - Skipping layer model.layers.8.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:05,566 - INFO - Skipping layer model.layers.8.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:05,566 - INFO - Skipping layer model.layers.8.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:05,566 - INFO - Skipping layer model.layers.8.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:05,566 - INFO - Skipping layer model.layers.9.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:05,566 - INFO - Skipping layer model.layers.9.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:05,566 - INFO - Layer: model.layers.9.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:10:05,567 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_9_self_attn_v_proj.safetensors2025-04-27 23:10:05,567 - INFO - exists: True2025-04-27 23:10:05,573 - INFO - factorize_layer_kron_svd2025-04-27 23:10:06,781 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:10:08,031 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:10:09,112 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:10:10,304 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:10:11,565 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_9_self_attn_v_proj.safetensors TrueLayer: model.layers.9.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.6650885e-03 4.7429068e-05 1.5233511e-05 1.3756523e-05 1.1949596e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:10:40,963 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:10:40,963 - INFO - Replacing 'model.layers.9.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)Compressing Layers:  29%|███████████████████████████████████████████████▌                                                                                                                  | 66/225 [11:58<27:17, 10.30s/it]2025-04-27 23:10:40,963 - INFO - Skipping layer model.layers.9.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:40,963 - INFO - Skipping layer model.layers.9.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:40,963 - INFO - Skipping layer model.layers.9.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:40,963 - INFO - Skipping layer model.layers.9.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:40,963 - INFO - Skipping layer model.layers.10.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:40,963 - INFO - Skipping layer model.layers.10.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:40,964 - INFO - Skipping layer model.layers.10.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:40,964 - INFO - Skipping layer model.layers.10.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:40,964 - INFO - Skipping layer model.layers.10.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:40,964 - INFO - Skipping layer model.layers.10.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:40,964 - INFO - Skipping layer model.layers.10.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:10:40,964 - INFO - Layer: model.layers.11.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:10:40,964 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_self_attn_q_proj.safetensors2025-04-27 23:10:40,964 - INFO - exists: True2025-04-27 23:10:40,972 - INFO - factorize_layer_kron_svd2025-04-27 23:10:42,092 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:10:43,348 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:10:44,653 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:10:45,740 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:10:46,932 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:10:48,228 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_self_attn_q_proj.safetensors TrueLayer: model.layers.11.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [2.6593389e-02 3.8675182e-05 2.0216057e-05 1.6013246e-05 1.3640273e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:11:18,131 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:11:18,131 - INFO - Replacing 'model.layers.11.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  35%|████████████████████████████████████████████████████████▏                                                                                                         | 78/225 [12:35<15:19,  6.26s/it]2025-04-27 23:11:18,131 - INFO - Skipping layer model.layers.11.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:11:18,131 - INFO - Skipping layer model.layers.11.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:11:18,131 - INFO - Layer: model.layers.11.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:11:18,132 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_self_attn_o_proj.safetensors2025-04-27 23:11:18,133 - INFO - exists: True2025-04-27 23:11:18,139 - INFO - factorize_layer_kron_svd2025-04-27 23:11:19,255 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:11:20,502 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:11:21,737 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:11:22,791 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:11:24,025 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_self_attn_o_proj.safetensors TrueLayer: model.layers.11.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.1733820e-02 3.5413963e-05 3.0874104e-05 2.7142969e-05 2.1825424e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:11:53,577 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:11:53,578 - INFO - Replacing 'model.layers.11.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)Compressing Layers:  36%|██████████████████████████████████████████████████████████▎                                                                                                       | 81/225 [13:10<17:14,  7.19s/it]2025-04-27 23:11:53,578 - INFO - Skipping layer model.layers.11.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:11:53,578 - INFO - Skipping layer model.layers.11.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:11:53,578 - INFO - Skipping layer model.layers.11.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:11:53,578 - INFO - Layer: model.layers.12.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:11:53,579 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_q_proj.safetensors2025-04-27 23:11:53,579 - INFO - exists: True2025-04-27 23:11:53,586 - INFO - factorize_layer_kron_svd2025-04-27 23:11:54,721 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:11:55,967 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:11:57,227 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:11:58,309 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:11:59,501 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:12:00,751 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_q_proj.safetensors TrueLayer: model.layers.12.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [9.15819407e-03 3.75725394e-05 2.34514755e-05 1.29712425e-05 1.20524883e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:12:30,062 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:12:30,062 - INFO - Replacing 'model.layers.12.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  38%|█████████████████████████████████████████████████████████████▏                                                                                                    | 85/225 [13:47<17:51,  7.65s/it]2025-04-27 23:12:30,063 - INFO - Layer: model.layers.12.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:12:30,068 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_k_proj.safetensors2025-04-27 23:12:30,068 - INFO - exists: True2025-04-27 23:12:30,086 - INFO - factorize_layer_kron_svd2025-04-27 23:12:31,226 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:12:32,288 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:12:33,378 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:12:34,454 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:12:35,504 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:12:36,757 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:12:37,813 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:12:38,884 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:12:39,976 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:12:41,053 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:12:42,119 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:12:43,373 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_k_proj.safetensors TrueLayer: model.layers.12.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.35002065e-05 7.74964155e-06 7.61668571e-06 7.45528405e-06 7.28192163e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:13:13,371 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:13:13,371 - INFO - Replacing 'model.layers.12.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  38%|█████████████████████████████████████████████████████████████▉                                                                                                    | 86/225 [14:30<24:16, 10.48s/it]2025-04-27 23:13:13,372 - INFO - Layer: model.layers.12.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:13:13,373 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_v_proj.safetensors2025-04-27 23:13:13,373 - INFO - exists: True2025-04-27 23:13:13,378 - INFO - factorize_layer_kron_svd2025-04-27 23:13:14,535 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:13:15,785 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:13:16,882 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:13:18,110 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:13:19,382 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_v_proj.safetensors TrueLayer: model.layers.12.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.2558317e-03 7.9526435e-05 1.6294656e-05 1.2817075e-05 1.1523963e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:13:44,356 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:13:44,357 - INFO - Replacing 'model.layers.12.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)Compressing Layers:  39%|██████████████████████████████████████████████████████████████▋                                                                                                   | 87/225 [15:01<28:54, 12.57s/it]2025-04-27 23:13:44,357 - INFO - Layer: model.layers.12.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:13:44,359 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_o_proj.safetensors2025-04-27 23:13:44,359 - INFO - exists: True2025-04-27 23:13:44,394 - INFO - factorize_layer_kron_svd2025-04-27 23:13:45,510 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:13:46,680 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:13:47,887 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:13:48,941 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:13:50,137 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:13:51,345 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_o_proj.safetensors TrueLayer: model.layers.12.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [6.9216173e-03 2.7236345e-05 2.4788929e-05 2.0528774e-05 1.6025677e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:14:13,510 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:14:13,511 - INFO - Replacing 'model.layers.12.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)Compressing Layers:  39%|███████████████████████████████████████████████████████████████▎                                                                                                  | 88/225 [15:30<33:30, 14.67s/it]2025-04-27 23:14:13,511 - INFO - Layer: model.layers.12.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:14:13,512 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_gate_proj.safetensors2025-04-27 23:14:13,512 - INFO - exists: True2025-04-27 23:14:13,523 - INFO - factorize_layer_kron_svd2025-04-27 23:14:14,730 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:14:15,776 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:14:16,812 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:14:17,844 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:14:18,878 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:14:20,148 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:14:21,775 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:14:23,634 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:14:25,456 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:14:27,257 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:14:29,057 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:14:32,617 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_gate_proj.safetensors TrueLayer: model.layers.12.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.2988171e-05 7.1681275e-06 6.5595987e-06 6.5212694e-06 6.4388596e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:15:23,425 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:15:23,425 - INFO - Replacing 'model.layers.12.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  40%|████████████████████████████████████████████████████████████████                                                                                                  | 89/225 [16:40<52:28, 23.15s/it]2025-04-27 23:15:23,426 - INFO - Layer: model.layers.12.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:15:23,427 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_up_proj.safetensors2025-04-27 23:15:23,427 - INFO - exists: True2025-04-27 23:15:23,441 - INFO - factorize_layer_kron_svd2025-04-27 23:15:24,678 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:15:25,742 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:15:26,777 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:15:27,813 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:15:28,845 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:15:30,040 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:15:31,667 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:15:33,527 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:15:35,327 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:15:37,123 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:15:38,931 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:15:41,933 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_up_proj.safetensors TrueLayer: model.layers.12.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [5.9819213e-06 4.8709753e-06 4.7432140e-06 4.6781370e-06 4.6636528e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:16:27,489 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:16:27,490 - INFO - Replacing 'model.layers.12.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  40%|████████████████████████████████████████████████████████████████                                                                                                | 90/225 [17:44<1:08:39, 30.51s/it]2025-04-27 23:16:27,490 - INFO - Skipping layer model.layers.12.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:16:27,490 - INFO - Layer: model.layers.13.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:16:27,492 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_q_proj.safetensors2025-04-27 23:16:27,492 - INFO - exists: True2025-04-27 23:16:27,504 - INFO - factorize_layer_kron_svd2025-04-27 23:16:28,630 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:16:29,828 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:16:31,084 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:16:32,153 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:16:33,384 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:16:34,655 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_q_proj.safetensors TrueLayer: model.layers.13.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.9350363e-02 5.4489075e-05 1.5019226e-05 1.3564231e-05 1.1421780e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:17:01,805 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:17:01,805 - INFO - Replacing 'model.layers.13.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  41%|██████████████████████████████████████████████████████████████████▏                                                                                               | 92/225 [18:19<57:35, 25.98s/it]2025-04-27 23:17:01,806 - INFO - Layer: model.layers.13.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:17:01,821 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_k_proj.safetensors2025-04-27 23:17:01,821 - INFO - exists: True2025-04-27 23:17:01,832 - INFO - factorize_layer_kron_svd2025-04-27 23:17:02,940 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:17:04,007 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:17:05,063 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:17:06,121 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:17:07,171 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:17:08,353 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:17:09,383 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:17:10,447 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:17:11,495 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:17:12,548 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:17:13,608 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:17:14,812 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_k_proj.safetensors TrueLayer: model.layers.13.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.3729892e-05 7.1575278e-06 7.1094173e-06 6.8093209e-06 6.5548347e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:17:40,547 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:17:40,547 - INFO - Replacing 'model.layers.13.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  41%|██████████████████████████████████████████████████████████████████▏                                                                                             | 93/225 [18:57<1:02:37, 28.47s/it]2025-04-27 23:17:40,548 - INFO - Layer: model.layers.13.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:17:40,549 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_v_proj.safetensors2025-04-27 23:17:40,549 - INFO - exists: True2025-04-27 23:17:40,555 - INFO - factorize_layer_kron_svd2025-04-27 23:17:41,684 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:17:42,932 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:17:44,018 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:17:45,230 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:17:46,487 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_v_proj.safetensors TrueLayer: model.layers.13.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [5.0983722e-03 5.0585440e-05 2.1063875e-05 1.7282286e-05 1.4230653e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:18:08,627 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:18:08,627 - INFO - Replacing 'model.layers.13.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)Compressing Layers:  42%|██████████████████████████████████████████████████████████████████▊                                                                                             | 94/225 [19:25<1:01:58, 28.38s/it]2025-04-27 23:18:08,628 - INFO - Layer: model.layers.13.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:18:08,628 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_o_proj.safetensors2025-04-27 23:18:08,628 - INFO - exists: True2025-04-27 23:18:08,637 - INFO - factorize_layer_kron_svd2025-04-27 23:18:09,746 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:18:10,894 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:18:12,108 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:18:13,179 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:18:14,349 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:18:15,565 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_o_proj.safetensors TrueLayer: model.layers.13.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.2446343e-02 3.8981681e-05 3.5099336e-05 2.8158551e-05 2.2302480e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:18:37,376 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:18:37,376 - INFO - Replacing 'model.layers.13.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)Compressing Layers:  42%|███████████████████████████████████████████████████████████████████▌                                                                                            | 95/225 [19:54<1:01:41, 28.47s/it]2025-04-27 23:18:37,377 - INFO - Skipping layer model.layers.13.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:18:37,377 - INFO - Layer: model.layers.13.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:18:37,377 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_up_proj.safetensors2025-04-27 23:18:37,377 - INFO - exists: True2025-04-27 23:18:37,383 - INFO - factorize_layer_kron_svd2025-04-27 23:18:38,616 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:18:39,747 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:18:40,948 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:18:42,554 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:18:45,209 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:18:48,252 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_up_proj.safetensors TrueLayer: model.layers.13.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [6.4682798e-03 6.1749437e-05 2.4349400e-05 2.1919277e-05 2.0116417e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:19:35,767 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:19:35,767 - INFO - Replacing 'model.layers.13.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  43%|████████████████████████████████████████████████████████████████████▉                                                                                           | 97/225 [20:53<1:01:21, 28.76s/it]2025-04-27 23:19:35,767 - INFO - Skipping layer model.layers.13.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:19:35,767 - INFO - Skipping layer model.layers.14.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:19:35,767 - INFO - Skipping layer model.layers.14.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:19:35,767 - INFO - Layer: model.layers.14.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:19:35,769 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_self_attn_v_proj.safetensors2025-04-27 23:19:35,769 - INFO - exists: True2025-04-27 23:19:35,779 - INFO - factorize_layer_kron_svd2025-04-27 23:19:36,851 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:19:38,053 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:19:39,082 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:19:40,260 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:19:41,492 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_self_attn_v_proj.safetensors TrueLayer: model.layers.14.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [2.4001515e-03 4.1852738e-05 1.8041146e-05 1.1698585e-05 1.1010628e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:20:02,969 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:20:02,969 - INFO - Replacing 'model.layers.14.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)Compressing Layers:  45%|████████████████████████████████████████████████████████████████████████▎                                                                                        | 101/225 [21:20<35:07, 16.99s/it]2025-04-27 23:20:02,970 - INFO - Layer: model.layers.14.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:20:02,970 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_self_attn_o_proj.safetensors2025-04-27 23:20:02,970 - INFO - exists: True2025-04-27 23:20:02,976 - INFO - factorize_layer_kron_svd2025-04-27 23:20:04,142 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:20:05,357 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:20:06,605 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:20:07,678 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:20:08,881 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:20:10,096 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_self_attn_o_proj.safetensors TrueLayer: model.layers.14.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [6.2927091e-03 2.8666886e-05 1.8207767e-05 1.3825013e-05 1.3153526e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:20:32,409 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:20:32,410 - INFO - Replacing 'model.layers.14.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)Compressing Layers:  45%|████████████████████████████████████████████████████████████████████████▉                                                                                        | 102/225 [21:49<38:56, 18.99s/it]2025-04-27 23:20:32,410 - INFO - Layer: model.layers.14.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:20:32,411 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_gate_proj.safetensors2025-04-27 23:20:32,411 - INFO - exists: True2025-04-27 23:20:32,416 - INFO - factorize_layer_kron_svd2025-04-27 23:20:33,651 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:20:34,821 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:20:36,053 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:20:37,777 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:20:40,937 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_gate_proj.safetensors TrueLayer: model.layers.14.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [3.2148813e-03 8.8560395e-05 2.4315630e-05 1.8305876e-05 1.6133905e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:21:26,293 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:21:26,293 - INFO - Replacing 'model.layers.14.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  46%|█████████████████████████████████████████████████████████████████████████▋                                                                                       | 103/225 [22:43<51:51, 25.51s/it]2025-04-27 23:21:26,294 - INFO - Layer: model.layers.14.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:21:26,295 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_up_proj.safetensors2025-04-27 23:21:26,295 - INFO - exists: True2025-04-27 23:21:26,320 - INFO - factorize_layer_kron_svd2025-04-27 23:21:27,511 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:21:28,660 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:21:29,872 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:21:31,583 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:21:34,254 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:21:37,309 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_up_proj.safetensors TrueLayer: model.layers.14.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [9.7202640e-03 2.5967920e-05 2.1275189e-05 1.6423986e-05 1.5875536e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:22:27,086 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:22:27,086 - INFO - Replacing 'model.layers.14.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  46%|█████████████████████████████████████████████████████████████████████████▍                                                                                     | 104/225 [23:44<1:06:25, 32.93s/it]2025-04-27 23:22:27,086 - INFO - Skipping layer model.layers.14.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:22:27,086 - INFO - Layer: model.layers.15.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:22:27,088 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_self_attn_q_proj.safetensors2025-04-27 23:22:27,088 - INFO - exists: True2025-04-27 23:22:27,098 - INFO - factorize_layer_kron_svd2025-04-27 23:22:28,191 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:22:29,409 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:22:30,692 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:22:31,786 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:22:32,953 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:22:34,188 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_self_attn_q_proj.safetensors TrueLayer: model.layers.15.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.19919665e-02 7.38329763e-05 2.44418788e-05 1.36780509e-05 1.00903217e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:23:02,283 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:23:02,284 - INFO - Replacing 'model.layers.15.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  47%|███████████████████████████████████████████████████████████████████████████▊                                                                                     | 106/225 [24:19<53:53, 27.17s/it]2025-04-27 23:23:02,285 - INFO - Skipping layer model.layers.15.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:23:02,285 - INFO - Layer: model.layers.15.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:23:02,286 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_self_attn_v_proj.safetensors2025-04-27 23:23:02,286 - INFO - exists: True2025-04-27 23:23:02,293 - INFO - factorize_layer_kron_svd2025-04-27 23:23:03,452 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:23:04,702 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:23:05,759 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:23:06,945 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:23:08,211 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_self_attn_v_proj.safetensors TrueLayer: model.layers.15.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [5.2174702e-03 3.7199647e-05 2.2397549e-05 1.4793896e-05 1.3787846e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:23:36,619 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:23:36,619 - INFO - Replacing 'model.layers.15.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)Compressing Layers:  48%|█████████████████████████████████████████████████████████████████████████████▎                                                                                   | 108/225 [24:53<46:10, 23.68s/it]2025-04-27 23:23:36,620 - INFO - Layer: model.layers.15.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:23:36,621 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_self_attn_o_proj.safetensors2025-04-27 23:23:36,621 - INFO - exists: True2025-04-27 23:23:36,626 - INFO - factorize_layer_kron_svd2025-04-27 23:23:37,745 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:23:38,930 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:23:40,183 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:23:41,278 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:23:42,484 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:23:43,764 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_self_attn_o_proj.safetensors TrueLayer: model.layers.15.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.3481039e-02 3.9970259e-05 2.5012072e-05 2.0769639e-05 1.6556156e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:24:11,879 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:24:11,879 - INFO - Replacing 'model.layers.15.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)Compressing Layers:  48%|█████████████████████████████████████████████████████████████████████████████▉                                                                                   | 109/225 [25:29<50:15, 25.99s/it]2025-04-27 23:24:11,879 - INFO - Skipping layer model.layers.15.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:11,879 - INFO - Skipping layer model.layers.15.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:11,879 - INFO - Skipping layer model.layers.15.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:11,879 - INFO - Skipping layer model.layers.16.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:11,879 - INFO - Skipping layer model.layers.16.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:11,879 - INFO - Layer: model.layers.16.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:24:11,880 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_self_attn_v_proj.safetensors2025-04-27 23:24:11,880 - INFO - exists: True2025-04-27 23:24:11,885 - INFO - factorize_layer_kron_svd2025-04-27 23:24:13,044 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:24:14,296 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:24:15,358 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:24:16,539 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:24:17,804 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_self_attn_v_proj.safetensors TrueLayer: model.layers.16.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [2.1955712e-02 2.2352620e-05 1.8913308e-05 1.6113720e-05 1.5723988e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:24:46,380 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:24:46,381 - INFO - Replacing 'model.layers.16.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)Compressing Layers:  51%|██████████████████████████████████████████████████████████████████████████████████▎                                                                              | 115/225 [26:03<24:13, 13.22s/it]2025-04-27 23:24:46,381 - INFO - Layer: model.layers.16.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:24:46,382 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_self_attn_o_proj.safetensors2025-04-27 23:24:46,382 - INFO - exists: True2025-04-27 23:24:46,389 - INFO - factorize_layer_kron_svd2025-04-27 23:24:47,564 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:24:48,703 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:24:49,996 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:24:51,104 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:24:52,347 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:24:53,640 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_self_attn_o_proj.safetensors TrueLayer: model.layers.16.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [2.5437940e-02 3.6648642e-05 3.1384960e-05 2.5638148e-05 2.1998954e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:25:21,660 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:25:21,660 - INFO - Replacing 'model.layers.16.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)Compressing Layers:  52%|███████████████████████████████████████████████████████████████████████████████████                                                                              | 116/225 [26:38<29:14, 16.10s/it]2025-04-27 23:25:21,660 - INFO - Skipping layer model.layers.16.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:25:21,660 - INFO - Skipping layer model.layers.16.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:25:21,660 - INFO - Skipping layer model.layers.16.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:25:21,660 - INFO - Layer: model.layers.17.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:25:21,661 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_self_attn_q_proj.safetensors2025-04-27 23:25:21,661 - INFO - exists: True2025-04-27 23:25:21,666 - INFO - factorize_layer_kron_svd2025-04-27 23:25:22,863 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:25:24,117 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:25:25,409 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:25:26,483 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:25:27,713 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:25:29,003 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_self_attn_q_proj.safetensors TrueLayer: model.layers.17.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.9799747e-02 3.0997286e-05 1.8318075e-05 1.5734129e-05 9.7556685e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:25:57,928 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:25:57,928 - INFO - Replacing 'model.layers.17.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  53%|█████████████████████████████████████████████████████████████████████████████████████▊                                                                           | 120/225 [27:15<22:54, 13.09s/it]2025-04-27 23:25:57,928 - INFO - Skipping layer model.layers.17.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:25:57,928 - INFO - Skipping layer model.layers.17.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:25:57,928 - INFO - Skipping layer model.layers.17.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:25:57,928 - INFO - Skipping layer model.layers.17.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:25:57,928 - INFO - Skipping layer model.layers.17.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:25:57,928 - INFO - Skipping layer model.layers.17.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:25:57,928 - INFO - Layer: model.layers.18.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:25:57,930 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_q_proj.safetensors2025-04-27 23:25:57,930 - INFO - exists: True2025-04-27 23:25:57,938 - INFO - factorize_layer_kron_svd2025-04-27 23:25:59,052 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:26:00,245 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:26:01,518 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:26:02,591 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:26:03,714 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:26:04,965 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_q_proj.safetensors TrueLayer: model.layers.18.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.0405821e-03 8.4903695e-05 8.4368939e-06 7.6583383e-06 6.2006247e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:26:34,054 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:26:34,055 - INFO - Replacing 'model.layers.18.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  56%|██████████████████████████████████████████████████████████████████████████████████████████▉                                                                      | 127/225 [27:51<14:41,  9.00s/it]2025-04-27 23:26:34,055 - INFO - Layer: model.layers.18.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:26:34,055 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_k_proj.safetensors2025-04-27 23:26:34,055 - INFO - exists: True2025-04-27 23:26:34,064 - INFO - factorize_layer_kron_svd2025-04-27 23:26:35,212 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:26:36,467 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:26:37,746 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:26:38,797 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:26:40,065 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:26:41,339 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_k_proj.safetensors TrueLayer: model.layers.18.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [7.7125463e-03 3.3378132e-05 8.2256274e-06 7.4304949e-06 6.1785499e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:27:10,240 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:27:10,241 - INFO - Replacing 'model.layers.18.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  57%|███████████████████████████████████████████████████████████████████████████████████████████▌                                                                     | 128/225 [28:27<18:44, 11.59s/it]2025-04-27 23:27:10,241 - INFO - Skipping layer model.layers.18.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:27:10,241 - INFO - Skipping layer model.layers.18.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:27:10,241 - INFO - Skipping layer model.layers.18.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:27:10,241 - INFO - Layer: model.layers.18.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:27:10,242 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_mlp_up_proj.safetensors2025-04-27 23:27:10,242 - INFO - exists: True2025-04-27 23:27:10,247 - INFO - factorize_layer_kron_svd2025-04-27 23:27:11,484 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:27:12,674 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:27:13,927 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:27:15,720 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:27:19,329 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_mlp_up_proj.safetensors TrueLayer: model.layers.18.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [5.7937209e-03 2.0253658e-05 1.6411330e-05 1.4584949e-05 1.2499403e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:28:23,385 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:28:23,385 - INFO - Replacing 'model.layers.18.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  59%|██████████████████████████████████████████████████████████████████████████████████████████████▍                                                                  | 132/225 [29:40<21:37, 13.95s/it]2025-04-27 23:28:23,385 - INFO - Skipping layer model.layers.18.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:28:23,386 - INFO - Layer: model.layers.19.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:28:23,387 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_self_attn_q_proj.safetensors2025-04-27 23:28:23,387 - INFO - exists: True2025-04-27 23:28:23,397 - INFO - factorize_layer_kron_svd2025-04-27 23:28:24,590 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:28:25,827 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:28:26,883 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:28:28,107 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_self_attn_q_proj.safetensors TrueLayer: model.layers.19.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [2.2634035e-03 7.5883843e-05 1.0972199e-05 7.9327338e-06 7.6394726e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:28:53,133 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:28:53,133 - INFO - Replacing 'model.layers.19.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  60%|███████████████████████████████████████████████████████████████████████████████████████████████▉                                                                 | 134/225 [30:10<21:26, 14.14s/it]2025-04-27 23:28:53,134 - INFO - Skipping layer model.layers.19.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:28:53,134 - INFO - Skipping layer model.layers.19.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:28:53,134 - INFO - Skipping layer model.layers.19.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:28:53,134 - INFO - Skipping layer model.layers.19.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:28:53,134 - INFO - Layer: model.layers.19.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:28:53,135 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_mlp_up_proj.safetensors2025-04-27 23:28:53,135 - INFO - exists: True2025-04-27 23:28:53,146 - INFO - factorize_layer_kron_svd2025-04-27 23:28:54,390 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:28:55,600 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:28:56,873 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:28:58,637 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:29:02,282 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_mlp_up_proj.safetensors TrueLayer: model.layers.19.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [5.9256121e-03 1.7187345e-05 1.4408997e-05 1.2730728e-05 9.7351012e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:30:04,908 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:30:04,909 - INFO - Replacing 'model.layers.19.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  62%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                                                             | 139/225 [31:22<20:23, 14.23s/it]2025-04-27 23:30:04,909 - INFO - Skipping layer model.layers.19.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:30:04,909 - INFO - Skipping layer model.layers.20.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:30:04,909 - INFO - Skipping layer model.layers.20.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:30:04,909 - INFO - Skipping layer model.layers.20.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:30:04,909 - INFO - Skipping layer model.layers.20.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:30:04,909 - INFO - Layer: model.layers.20.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:30:04,911 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_20_mlp_gate_proj.safetensors2025-04-27 23:30:04,911 - INFO - exists: True2025-04-27 23:30:04,937 - INFO - factorize_layer_kron_svd2025-04-27 23:30:06,171 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:30:07,387 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:30:08,633 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:30:10,483 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:30:14,164 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_20_mlp_gate_proj.safetensors TrueLayer: model.layers.20.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [5.6294589e-03 2.1218650e-05 1.5237835e-05 1.4524096e-05 1.3646030e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:31:17,997 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:31:17,997 - INFO - Replacing 'model.layers.20.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  64%|███████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                         | 145/225 [32:35<17:49, 13.37s/it]2025-04-27 23:31:17,998 - INFO - Skipping layer model.layers.20.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:31:17,998 - INFO - Skipping layer model.layers.20.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:31:17,998 - INFO - Layer: model.layers.21.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:31:17,999 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_self_attn_q_proj.safetensors2025-04-27 23:31:17,999 - INFO - exists: True2025-04-27 23:31:18,010 - INFO - factorize_layer_kron_svd2025-04-27 23:31:19,137 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:31:20,418 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:31:21,671 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:31:22,752 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:31:23,993 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_self_attn_q_proj.safetensors TrueLayer: model.layers.21.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.9928338e-03 1.8311075e-05 9.7191369e-06 7.7809100e-06 5.9942731e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:31:53,471 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:31:53,471 - INFO - Replacing 'model.layers.21.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  66%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                       | 148/225 [33:10<16:42, 13.02s/it]2025-04-27 23:31:53,471 - INFO - Layer: model.layers.21.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:31:53,471 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_self_attn_k_proj.safetensors2025-04-27 23:31:53,472 - INFO - exists: True2025-04-27 23:31:53,475 - INFO - factorize_layer_kron_svd2025-04-27 23:31:54,627 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:31:55,844 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:31:57,097 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:31:58,167 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:31:59,385 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_self_attn_k_proj.safetensors TrueLayer: model.layers.21.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.3128020e-03 9.2708731e-05 9.0614503e-06 4.9129476e-06 3.1025263e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:32:27,070 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:32:27,071 - INFO - Replacing 'model.layers.21.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  66%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                      | 149/225 [33:44<19:03, 15.05s/it]2025-04-27 23:32:27,071 - INFO - Layer: model.layers.21.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:32:27,073 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_self_attn_v_proj.safetensors2025-04-27 23:32:27,073 - INFO - exists: True2025-04-27 23:32:27,077 - INFO - factorize_layer_kron_svd2025-04-27 23:32:28,240 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:32:29,508 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:32:30,597 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:32:31,803 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:32:33,119 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_self_attn_v_proj.safetensors TrueLayer: model.layers.21.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [2.9529214e-03 7.7000013e-06 6.5920690e-06 6.2058962e-06 5.7856682e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:32:55,845 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:32:55,845 - INFO - Replacing 'model.layers.21.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)Compressing Layers:  67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                     | 150/225 [34:13<20:55, 16.74s/it]2025-04-27 23:32:55,845 - INFO - Skipping layer model.layers.21.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:32:55,845 - INFO - Skipping layer model.layers.21.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:32:55,846 - INFO - Layer: model.layers.21.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:32:55,846 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_up_proj.safetensors2025-04-27 23:32:55,846 - INFO - exists: True2025-04-27 23:32:55,850 - INFO - factorize_layer_kron_svd2025-04-27 23:32:57,095 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:32:58,372 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:32:59,597 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:33:01,495 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:33:05,203 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_up_proj.safetensors TrueLayer: model.layers.21.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.8695735e-03 5.5963137e-05 1.4356574e-05 1.3274203e-05 1.2030486e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:34:08,232 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:34:08,232 - INFO - Replacing 'model.layers.21.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                   | 153/225 [35:25<23:09, 19.30s/it]2025-04-27 23:34:08,233 - INFO - Layer: model.layers.21.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:34:08,234 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_down_proj.safetensors2025-04-27 23:34:08,234 - INFO - exists: True2025-04-27 23:34:08,247 - INFO - factorize_layer_kron_svd2025-04-27 23:34:09,990 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:34:12,372 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:34:16,002 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:34:17,048 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:34:18,226 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:34:19,484 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_down_proj.safetensors TrueLayer: model.layers.21.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=11008, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x11008)...  SVD complete. Singular values (top 5): [4.5640193e-02 9.2487091e-05 5.4519722e-05 4.7700971e-05 3.9835424e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,11008), (4096,1496)factorized_sequential Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))2025-04-27 23:35:30,917 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))')2025-04-27 23:35:30,917 - INFO - Replacing 'model.layers.21.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)Compressing Layers:  68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                  | 154/225 [36:48<33:26, 28.27s/it]2025-04-27 23:35:30,917 - INFO - Skipping layer model.layers.22.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:35:30,917 - INFO - Skipping layer model.layers.22.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:35:30,917 - INFO - Skipping layer model.layers.22.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:35:30,917 - INFO - Skipping layer model.layers.22.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:35:30,917 - INFO - Skipping layer model.layers.22.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:35:30,917 - INFO - Layer: model.layers.22.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:35:30,919 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_up_proj.safetensors2025-04-27 23:35:30,919 - INFO - exists: True2025-04-27 23:35:30,931 - INFO - factorize_layer_kron_svd2025-04-27 23:35:32,203 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:35:33,442 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:35:34,690 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:35:36,559 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:35:40,206 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_up_proj.safetensors TrueLayer: model.layers.22.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.2615041e-03 2.1604352e-05 1.3284305e-05 1.1028132e-05 1.0561911e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:36:44,221 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:36:44,221 - INFO - Replacing 'model.layers.22.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  71%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                              | 160/225 [38:01<21:05, 19.47s/it]2025-04-27 23:36:44,221 - INFO - Layer: model.layers.22.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:36:44,223 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_down_proj.safetensors2025-04-27 23:36:44,223 - INFO - exists: True2025-04-27 23:36:44,235 - INFO - factorize_layer_kron_svd2025-04-27 23:36:46,002 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:36:48,402 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:36:52,051 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:36:53,118 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:36:54,364 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:36:55,631 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_down_proj.safetensors TrueLayer: model.layers.22.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=11008, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x11008)...  SVD complete. Singular values (top 5): [2.3855805e-02 8.0862206e-05 5.0205126e-05 4.2752916e-05 3.9248724e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,11008), (4096,1496)factorized_sequential Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))2025-04-27 23:38:07,987 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))')2025-04-27 23:38:07,987 - INFO - Replacing 'model.layers.22.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)Compressing Layers:  72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                             | 161/225 [39:25<28:41, 26.89s/it]2025-04-27 23:38:07,987 - INFO - Skipping layer model.layers.23.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:38:07,987 - INFO - Skipping layer model.layers.23.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:38:07,987 - INFO - Skipping layer model.layers.23.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:38:07,987 - INFO - Skipping layer model.layers.23.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:38:07,987 - INFO - Layer: model.layers.23.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:38:07,991 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_gate_proj.safetensors2025-04-27 23:38:07,991 - INFO - exists: True2025-04-27 23:38:08,003 - INFO - factorize_layer_kron_svd2025-04-27 23:38:09,219 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:38:10,377 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:38:11,626 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:38:13,539 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:38:17,267 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_gate_proj.safetensors TrueLayer: model.layers.23.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.6312244e-03 1.5538668e-05 4.1256385e-06 2.1622898e-06 2.0991954e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:39:15,894 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:39:15,894 - INFO - Replacing 'model.layers.23.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  74%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                          | 166/225 [40:33<20:31, 20.88s/it]2025-04-27 23:39:15,895 - INFO - Skipping layer model.layers.23.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:39:15,895 - INFO - Layer: model.layers.23.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:39:15,897 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_down_proj.safetensors2025-04-27 23:39:15,897 - INFO - exists: True2025-04-27 23:39:15,915 - INFO - factorize_layer_kron_svd2025-04-27 23:39:17,755 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:39:20,417 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:39:24,022 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:39:25,091 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:39:26,270 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:39:27,512 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_down_proj.safetensors TrueLayer: model.layers.23.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=11008, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x11008)...  SVD complete. Singular values (top 5): [6.6739982e-03 2.3223182e-04 4.9300255e-05 3.7862912e-05 3.5104458e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,11008), (4096,1496)factorized_sequential Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))2025-04-27 23:40:25,294 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))')2025-04-27 23:40:25,294 - INFO - Replacing 'model.layers.23.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)Compressing Layers:  75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                        | 168/225 [41:42<22:31, 23.71s/it]2025-04-27 23:40:25,294 - INFO - Layer: model.layers.24.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:40:25,296 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_24_self_attn_q_proj.safetensors2025-04-27 23:40:25,296 - INFO - exists: True2025-04-27 23:40:25,309 - INFO - factorize_layer_kron_svd2025-04-27 23:40:26,405 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:40:27,573 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:40:28,757 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:40:29,806 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:40:31,035 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_24_self_attn_q_proj.safetensors TrueLayer: model.layers.24.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [9.3926415e-03 2.3708692e-05 9.1289621e-06 5.9139638e-06 5.1298866e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:40:52,852 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:40:52,853 - INFO - Replacing 'model.layers.24.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                        | 169/225 [42:10<22:35, 24.21s/it]2025-04-27 23:40:52,853 - INFO - Layer: model.layers.24.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:40:52,854 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_24_self_attn_k_proj.safetensors2025-04-27 23:40:52,854 - INFO - exists: True2025-04-27 23:40:52,859 - INFO - factorize_layer_kron_svd2025-04-27 23:40:53,977 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:40:55,238 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:40:56,312 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:40:57,544 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_24_self_attn_k_proj.safetensors TrueLayer: model.layers.24.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.0369071e-02 1.0171949e-05 5.6428921e-06 4.9444534e-06 4.1125627e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:41:27,549 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:41:27,549 - INFO - Replacing 'model.layers.24.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                       | 170/225 [42:44<23:40, 25.83s/it]2025-04-27 23:41:27,549 - INFO - Layer: model.layers.24.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:41:27,551 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_24_self_attn_v_proj.safetensors2025-04-27 23:41:27,551 - INFO - exists: True2025-04-27 23:41:27,564 - INFO - factorize_layer_kron_svd2025-04-27 23:41:28,677 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:41:29,925 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:41:30,991 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:41:32,189 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:41:33,462 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_24_self_attn_v_proj.safetensors TrueLayer: model.layers.24.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [3.9264322e-03 1.4345253e-05 8.7122116e-06 7.7657978e-06 6.5538288e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:42:03,092 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:42:03,092 - INFO - Replacing 'model.layers.24.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)Compressing Layers:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                      | 171/225 [43:20<24:49, 27.58s/it]2025-04-27 23:42:03,092 - INFO - Skipping layer model.layers.24.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:03,092 - INFO - Skipping layer model.layers.24.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:03,092 - INFO - Skipping layer model.layers.24.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:03,092 - INFO - Skipping layer model.layers.24.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:03,092 - INFO - Skipping layer model.layers.25.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:03,092 - INFO - Skipping layer model.layers.25.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:03,092 - INFO - Skipping layer model.layers.25.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:03,092 - INFO - Skipping layer model.layers.25.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:03,092 - INFO - Skipping layer model.layers.25.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:42:03,092 - INFO - Layer: model.layers.25.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:42:03,093 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_25_mlp_up_proj.safetensors2025-04-27 23:42:03,093 - INFO - exists: True2025-04-27 23:42:03,123 - INFO - factorize_layer_kron_svd2025-04-27 23:42:04,370 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:42:05,577 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:42:06,847 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:42:08,718 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:42:12,443 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_25_mlp_up_proj.safetensors TrueLayer: model.layers.25.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [5.2369884e-03 1.2808485e-05 1.0910483e-05 1.0035876e-05 9.8339870e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:43:03,694 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:43:03,694 - INFO - Replacing 'model.layers.25.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                               | 181/225 [44:20<08:51, 12.07s/it]2025-04-27 23:43:03,695 - INFO - Skipping layer model.layers.25.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:43:03,695 - INFO - Skipping layer model.layers.26.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:43:03,695 - INFO - Skipping layer model.layers.26.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:43:03,695 - INFO - Layer: model.layers.26.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:43:03,696 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_self_attn_v_proj.safetensors2025-04-27 23:43:03,696 - INFO - exists: True2025-04-27 23:43:03,708 - INFO - factorize_layer_kron_svd2025-04-27 23:43:04,801 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:43:06,011 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:43:07,071 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:43:08,219 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:43:09,417 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_self_attn_v_proj.safetensors TrueLayer: model.layers.26.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [5.9758541e-03 1.9324034e-05 9.3038707e-06 8.1034304e-06 6.7713663e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:43:31,651 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:43:31,651 - INFO - Replacing 'model.layers.26.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)Compressing Layers:  82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                            | 185/225 [44:48<07:03, 10.59s/it]2025-04-27 23:43:31,651 - INFO - Layer: model.layers.26.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:43:31,652 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_self_attn_o_proj.safetensors2025-04-27 23:43:31,652 - INFO - exists: True2025-04-27 23:43:31,657 - INFO - factorize_layer_kron_svd2025-04-27 23:43:32,782 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:43:33,959 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:43:35,187 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:43:36,248 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:43:37,436 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_self_attn_o_proj.safetensors TrueLayer: model.layers.26.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [9.7393105e-03 3.5570287e-05 1.1101330e-05 8.3519908e-06 6.8651966e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:43:59,639 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:43:59,639 - INFO - Replacing 'model.layers.26.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)Compressing Layers:  83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 186/225 [45:16<07:56, 12.23s/it]2025-04-27 23:43:59,640 - INFO - Layer: model.layers.26.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:43:59,640 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_mlp_gate_proj.safetensors2025-04-27 23:43:59,640 - INFO - exists: True2025-04-27 23:43:59,646 - INFO - factorize_layer_kron_svd2025-04-27 23:44:00,874 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:44:02,053 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:44:03,279 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:44:05,176 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:44:08,934 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_mlp_gate_proj.safetensors TrueLayer: model.layers.26.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [3.5354062e-03 1.2539586e-05 5.3919930e-06 4.6146652e-06 4.4492190e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:45:14,210 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:45:14,210 - INFO - Replacing 'model.layers.26.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                           | 187/225 [46:31<12:26, 19.63s/it]2025-04-27 23:45:14,211 - INFO - Layer: model.layers.26.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:45:14,212 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_mlp_up_proj.safetensors2025-04-27 23:45:14,212 - INFO - exists: True2025-04-27 23:45:14,233 - INFO - factorize_layer_kron_svd2025-04-27 23:45:15,445 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:45:16,681 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:45:18,567 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:45:22,215 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_mlp_up_proj.safetensors TrueLayer: model.layers.26.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [5.6883384e-04 1.2054178e-04 1.2884599e-05 1.1027682e-05 9.3955405e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:46:21,786 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:46:21,787 - INFO - Replacing 'model.layers.26.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                          | 188/225 [47:39<16:23, 26.59s/it]2025-04-27 23:46:21,787 - INFO - Skipping layer model.layers.26.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:46:21,787 - INFO - Layer: model.layers.27.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:46:21,789 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_27_self_attn_q_proj.safetensors2025-04-27 23:46:21,789 - INFO - exists: True2025-04-27 23:46:21,799 - INFO - factorize_layer_kron_svd2025-04-27 23:46:22,885 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:46:24,018 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:46:25,270 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:46:26,314 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:46:27,464 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:46:28,684 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_27_self_attn_q_proj.safetensors TrueLayer: model.layers.27.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [3.1449087e-02 1.7293913e-05 1.0841605e-05 1.0305326e-05 7.7381810e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:46:51,079 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:46:51,080 - INFO - Replacing 'model.layers.27.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                         | 190/225 [48:08<13:28, 23.09s/it]2025-04-27 23:46:51,080 - INFO - Layer: model.layers.27.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:46:51,081 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_27_self_attn_k_proj.safetensors2025-04-27 23:46:51,081 - INFO - exists: True2025-04-27 23:46:51,086 - INFO - factorize_layer_kron_svd2025-04-27 23:46:52,218 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:46:53,404 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:46:54,669 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:46:55,760 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:46:56,993 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_27_self_attn_k_proj.safetensors TrueLayer: model.layers.27.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.4964168e-02 2.2320717e-05 6.6194466e-06 5.9166182e-06 5.3742956e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:47:19,341 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:47:19,341 - INFO - Replacing 'model.layers.27.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  85%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                        | 191/225 [48:36<13:35, 23.98s/it]2025-04-27 23:47:19,341 - INFO - Skipping layer model.layers.27.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:47:19,341 - INFO - Skipping layer model.layers.27.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:47:19,341 - INFO - Skipping layer model.layers.27.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:47:19,341 - INFO - Skipping layer model.layers.27.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:47:19,341 - INFO - Skipping layer model.layers.27.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:47:19,341 - INFO - Layer: model.layers.28.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:47:19,342 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_28_self_attn_q_proj.safetensors2025-04-27 23:47:19,342 - INFO - exists: True2025-04-27 23:47:19,347 - INFO - factorize_layer_kron_svd2025-04-27 23:47:20,470 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:47:21,639 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:47:22,872 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:47:23,949 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:47:25,144 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:47:26,345 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_28_self_attn_q_proj.safetensors TrueLayer: model.layers.28.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [7.7257529e-02 2.7300872e-05 1.5574698e-05 1.3328204e-05 1.0336477e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:47:48,988 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:47:48,988 - INFO - Replacing 'model.layers.28.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                    | 197/225 [49:06<05:53, 12.61s/it]2025-04-27 23:47:48,988 - INFO - Layer: model.layers.28.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:47:48,990 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_28_self_attn_k_proj.safetensors2025-04-27 23:47:48,990 - INFO - exists: True2025-04-27 23:47:48,995 - INFO - factorize_layer_kron_svd2025-04-27 23:47:50,133 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:47:51,195 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:47:52,245 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:47:53,317 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:47:54,349 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:47:55,731 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:47:56,775 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:47:57,850 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:47:58,904 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:47:59,992 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:48:01,087 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:48:02,322 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_28_self_attn_k_proj.safetensors TrueLayer: model.layers.28.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.3403688e-05 6.0787293e-06 5.6532840e-06 5.4129196e-06 5.2258588e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:48:24,265 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:48:24,266 - INFO - Replacing 'model.layers.28.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                   | 198/225 [49:41<06:56, 15.43s/it]2025-04-27 23:48:24,267 - INFO - Skipping layer model.layers.28.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:48:24,267 - INFO - Skipping layer model.layers.28.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:48:24,267 - INFO - Skipping layer model.layers.28.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:48:24,267 - INFO - Skipping layer model.layers.28.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:48:24,267 - INFO - Skipping layer model.layers.28.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:48:24,267 - INFO - Layer: model.layers.29.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:48:24,268 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_29_self_attn_q_proj.safetensors2025-04-27 23:48:24,268 - INFO - exists: True2025-04-27 23:48:24,283 - INFO - factorize_layer_kron_svd2025-04-27 23:48:25,468 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:48:26,699 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:48:27,970 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:48:29,074 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:48:30,304 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:48:31,582 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_29_self_attn_q_proj.safetensors TrueLayer: model.layers.29.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.3079672e-02 6.6411179e-05 1.8837438e-05 1.0730346e-05 9.5193182e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:48:54,428 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:48:54,429 - INFO - Replacing 'model.layers.29.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉               | 204/225 [50:11<03:31, 10.06s/it]2025-04-27 23:48:54,429 - INFO - Layer: model.layers.29.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:48:54,430 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_29_self_attn_k_proj.safetensors2025-04-27 23:48:54,430 - INFO - exists: True2025-04-27 23:48:54,434 - INFO - factorize_layer_kron_svd2025-04-27 23:48:55,556 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:48:56,666 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:48:57,885 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:48:58,944 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:49:00,155 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_29_self_attn_k_proj.safetensors TrueLayer: model.layers.29.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.0633701e-02 6.4269574e-05 1.3669864e-05 7.6451261e-06 6.8974796e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:49:24,251 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:49:24,251 - INFO - Replacing 'model.layers.29.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋              | 205/225 [50:41<04:04, 12.22s/it]2025-04-27 23:49:24,251 - INFO - Skipping layer model.layers.29.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,251 - INFO - Skipping layer model.layers.29.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,251 - INFO - Skipping layer model.layers.29.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,251 - INFO - Skipping layer model.layers.29.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,251 - INFO - Skipping layer model.layers.29.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,251 - INFO - Skipping layer model.layers.30.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,251 - INFO - Skipping layer model.layers.30.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,251 - INFO - Skipping layer model.layers.30.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,251 - INFO - Skipping layer model.layers.30.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,251 - INFO - Skipping layer model.layers.30.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,251 - INFO - Skipping layer model.layers.30.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,251 - INFO - Skipping layer model.layers.30.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,251 - INFO - Skipping layer model.layers.31.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,251 - INFO - Skipping layer model.layers.31.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,252 - INFO - Skipping layer model.layers.31.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,252 - INFO - Skipping layer model.layers.31.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,252 - INFO - Skipping layer model.layers.31.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,252 - INFO - Skipping layer model.layers.31.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,252 - INFO - Skipping layer model.layers.31.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:49:24,252 - INFO - Skipping layer lm_head: Sensitivity ratio >= 1.0 (1.00). No compression.Compressing Layers: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [50:41<00:00, 13.52s/it]2025-04-27 23:49:24,252 - INFO - Compression finished. Processed: 69, Skipped (Ratio>=1 or No Sensitivity/Factors): 156, Failed: 02025-04-27 23:49:24,254 - INFO - Total parameters after compression: 57244979202025-04-27 23:49:24,254 - INFO - C rate : 0.84953173657135252025-04-27 23:49:24,254 - INFO - Saving compressed model to ./llama102025-04-27 23:49:24,254 - INFO - Compressed model and tokenizer saved.2025-04-27 23:49:24,265 - INFO - Evaluating on wikitext2Evaluating:   0%|                                                                                                                                                                                    | 0/21 [00:00<?, ?it/s]Evaluating:   5%|████████▏                                                                                                                                                                   | 1/21 [00:03<01:19,  3.97s/it]Evaluating:  10%|████████████████▍                                                                                                                                                           | 2/21 [00:05<00:43,  2.28s/it]Evaluating:  14%|████████████████████████▌                                                                                                                                                   | 3/21 [00:06<00:31,  1.75s/it]Evaluating:  19%|████████████████████████████████▊                                                                                                                                           | 4/21 [00:07<00:25,  1.49s/it]Evaluating:  24%|████████████████████████████████████████▉                                                                                                                                   | 5/21 [00:08<00:21,  1.35s/it]Evaluating:  29%|█████████████████████████████████████████████████▏                                                                                                                          | 6/21 [00:09<00:19,  1.27s/it]Evaluating:  33%|█████████████████████████████████████████████████████████▎                                                                                                                  | 7/21 [00:10<00:16,  1.21s/it]Evaluating:  38%|█████████████████████████████████████████████████████████████████▌                                                                                                          | 8/21 [00:11<00:15,  1.18s/it]Evaluating:  43%|█████████████████████████████████████████████████████████████████████████▋                                                                                                  | 9/21 [00:12<00:13,  1.16s/it]Evaluating:  48%|█████████████████████████████████████████████████████████████████████████████████▍                                                                                         | 10/21 [00:13<00:12,  1.14s/it]Evaluating:  52%|█████████████████████████████████████████████████████████████████████████████████████████▌                                                                                 | 11/21 [00:15<00:11,  1.13s/it]Evaluating:  57%|█████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                         | 12/21 [00:16<00:10,  1.12s/it]Evaluating:  62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                 | 13/21 [00:17<00:08,  1.12s/it]Evaluating:  67%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                         | 14/21 [00:18<00:07,  1.11s/it]Evaluating:  71%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                | 15/21 [00:19<00:06,  1.11s/it]Evaluating:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                        | 16/21 [00:20<00:05,  1.11s/it]Evaluating:  81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                | 17/21 [00:21<00:04,  1.11s/it]Evaluating:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                        | 18/21 [00:22<00:03,  1.11s/it]Evaluating:  90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                | 19/21 [00:24<00:02,  1.17s/it]Evaluating:  95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊        | 20/21 [00:25<00:01,  1.15s/it]Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:25<00:00,  1.06s/it]Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:25<00:00,  1.24s/it]2025-04-27 23:49:57,877 - INFO - wikitext2 perplexity: 12.37502025-04-27 23:49:57,877 - INFO - Evaluating on ptbnlls.shape torch.Size([16376])Mean NLL: 2.515625Evaluating:   0%|                                                                                                                                                                                     | 0/7 [00:00<?, ?it/s]Evaluating:  14%|████████████████████████▋                                                                                                                                                    | 1/7 [00:01<00:06,  1.10s/it]Evaluating:  29%|█████████████████████████████████████████████████▍                                                                                                                           | 2/7 [00:02<00:05,  1.10s/it]Evaluating:  43%|██████████████████████████████████████████████████████████████████████████▏                                                                                                  | 3/7 [00:03<00:04,  1.10s/it]Evaluating:  57%|██████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                          | 4/7 [00:04<00:03,  1.10s/it]Evaluating:  71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                 | 5/7 [00:05<00:02,  1.10s/it]Evaluating:  86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                        | 6/7 [00:06<00:01,  1.10s/it]Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:07<00:00,  1.06s/it]Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:07<00:00,  1.09s/it]2025-04-27 23:50:07,742 - INFO - ptb perplexity: 54.50002025-04-27 23:50:07,742 - INFO - Evaluation results:2025-04-27 23:50:07,742 - INFO -   wikitext2: 12.37502025-04-27 23:50:07,742 - INFO -   ptb: 54.5000nlls.shape torch.Size([16376])Mean NLL: 4.0
2025-04-27 17:10:07,717 - INFO - Loading model: unsloth/llama-2-7b-chat
model.layers.28.self_attn.k_proj 0.3
model.layers.22.mlp.gate_proj 0.4
model.layers.19.mlp.gate_proj 0.4
model.layers.19.self_attn.q_proj 0.3
model.layers.18.mlp.gate_proj 0.4
model.layers.18.self_attn.q_proj 0.3
model.layers.17.mlp.gate_proj 0.4
model.layers.17.self_attn.q_proj 0.3
model.layers.16.mlp.gate_proj 0.4
model.layers.16.self_attn.v_proj 0.3
model.layers.15.self_attn.q_proj 0.3
model.layers.14.mlp.gate_proj 0.4
model.layers.14.mlp.up_proj 0.4
model.layers.13.mlp.gate_proj 0.4
model.layers.13.mlp.up_proj 0.4
model.layers.13.self_attn.v_proj 0.3
model.layers.13.self_attn.o_proj 0.3
model.layers.12.mlp.gate_proj 0.4
model.layers.12.mlp.up_proj 0.4
model.layers.12.self_attn.q_proj 0.3
model.layers.11.mlp.gate_proj 0.4
model.layers.9.mlp.down_proj 0.4
model.layers.8.mlp.gate_proj 0.4
model.layers.8.mlp.down_proj 0.4
model.layers.8.self_attn.q_proj 0.3
model.layers.7.mlp.gate_proj 0.4
model.layers.7.mlp.down_proj 0.4
model.layers.6.mlp.down_proj 0.4
model.layers.6.self_attn.q_proj 0.3
model.layers.5.self_attn.k_proj 0.3
model.layers.4.mlp.gate_proj 0.4
model.layers.4.mlp.down_proj 0.4
model.layers.4.self_attn.q_proj 0.3
model.layers.2.mlp.gate_proj 0.4
model.layers.2.mlp.up_proj 0.4
model.layers.2.self_attn.q_proj 0.3
[2025-04-27 17:10:14,719] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Loading checkpoint shards:   0%|                                                                                                                                                                           | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|██████████████████████████████████████████████████████▎                                                                                                            | 1/3 [00:12<00:25, 12.72s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 2/3 [00:25<00:12, 12.92s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:35<00:00, 11.39s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:35<00:00, 11.78s/it]
2025-04-27 17:10:55,273 - INFO - Model loaded with dtype torch.bfloat16
2025-04-27 17:11:44,241 - INFO - Model moved to cuda:0
2025-04-27 17:11:44,243 - INFO - Total parameters before compression: 6738415616
2025-04-27 17:11:44,244 - INFO - Found 225 linear layers to potentially compress.
Compressing Layers:   0%|                                                                                                                                                                                | 0/225 [00:00<?, ?it/s]2025-04-27 17:11:44,245 - INFO - Skipping layer model.layers.0.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:11:44,245 - INFO - Skipping layer model.layers.0.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:11:44,245 - INFO - Skipping layer model.layers.0.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:11:44,245 - INFO - Skipping layer model.layers.0.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:11:44,246 - INFO - Skipping layer model.layers.0.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:11:44,246 - INFO - Skipping layer model.layers.0.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:11:44,246 - INFO - Skipping layer model.layers.0.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:11:44,246 - INFO - Skipping layer model.layers.1.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:11:44,246 - INFO - Skipping layer model.layers.1.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:11:44,246 - INFO - Skipping layer model.layers.1.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:11:44,246 - INFO - Skipping layer model.layers.1.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:11:44,246 - INFO - Skipping layer model.layers.1.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:11:44,246 - INFO - Skipping layer model.layers.1.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:11:44,246 - INFO - Skipping layer model.layers.1.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:11:44,246 - INFO - Layer: model.layers.2.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-27 17:11:44,253 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_q_proj.safetensors
2025-04-27 17:11:44,253 - INFO - exists: True
2025-04-27 17:11:44,263 - INFO - factorize_layer_kron_svd
2025-04-27 17:11:45,991 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:11:47,517 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:11:49,022 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 17:11:50,544 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 17:11:52,047 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 17:11:54,054 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-27 17:11:55,534 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:11:57,082 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:11:58,632 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 17:12:00,244 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 17:12:01,738 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 17:12:03,460 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_q_proj.safetensors True
Layer: model.layers.2.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.6124124e-05 1.1238000e-05 1.0046171e-05 9.5376463e-06 9.3023737e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-27 17:12:46,914 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-27 17:12:46,914 - INFO - Replacing 'model.layers.2.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:   7%|███████████▏                                                                                                                                                           | 15/225 [01:02<14:37,  4.18s/it]2025-04-27 17:12:46,915 - INFO - Skipping layer model.layers.2.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:12:46,915 - INFO - Skipping layer model.layers.2.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:12:46,915 - INFO - Skipping layer model.layers.2.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:12:46,915 - INFO - Layer: model.layers.2.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:12:46,918 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_gate_proj.safetensors
2025-04-27 17:12:46,918 - INFO - exists: True
2025-04-27 17:12:46,933 - INFO - factorize_layer_kron_svd
2025-04-27 17:12:48,907 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:12:50,450 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:12:51,946 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 17:12:53,433 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 17:12:55,018 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 17:12:56,748 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-27 17:12:59,379 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:13:02,006 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:13:04,479 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 17:13:07,205 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 17:13:09,850 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 17:13:14,725 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_gate_proj.safetensors True
Layer: model.layers.2.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.0536157e-05 7.8889116e-06 7.3416263e-06 6.9958951e-06 6.9346693e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-27 17:14:48,344 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-27 17:14:48,345 - INFO - Replacing 'model.layers.2.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:   8%|██████████████                                                                                                                                                         | 19/225 [03:04<39:08, 11.40s/it]2025-04-27 17:14:48,345 - INFO - Layer: model.layers.2.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:14:48,348 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_up_proj.safetensors
2025-04-27 17:14:48,348 - INFO - exists: True
2025-04-27 17:14:48,399 - INFO - factorize_layer_kron_svd
2025-04-27 17:14:50,196 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:14:51,689 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:14:53,180 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 17:14:54,666 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 17:14:56,145 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 17:14:57,959 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-27 17:15:00,361 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:15:03,061 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:15:05,737 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 17:15:08,403 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 17:15:10,984 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 17:15:15,832 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_up_proj.safetensors True
Layer: model.layers.2.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [4.5019283e-06 4.2492616e-06 4.1393623e-06 4.1020489e-06 4.0891123e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-27 17:16:49,463 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-27 17:16:49,463 - INFO - Replacing 'model.layers.2.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:   9%|██████████████▋                                                                                                                                                      | 20/225 [05:05<1:12:34, 21.24s/it]2025-04-27 17:16:49,464 - INFO - Skipping layer model.layers.2.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:16:49,464 - INFO - Skipping layer model.layers.3.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:16:49,464 - INFO - Skipping layer model.layers.3.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:16:49,464 - INFO - Skipping layer model.layers.3.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:16:49,464 - INFO - Skipping layer model.layers.3.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:16:49,464 - INFO - Skipping layer model.layers.3.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:16:49,464 - INFO - Skipping layer model.layers.3.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:16:49,464 - INFO - Skipping layer model.layers.3.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:16:49,464 - INFO - Layer: model.layers.4.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-27 17:16:49,468 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_q_proj.safetensors
2025-04-27 17:16:49,468 - INFO - exists: True
2025-04-27 17:16:49,532 - INFO - factorize_layer_kron_svd
2025-04-27 17:16:51,346 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:16:53,146 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:16:55,140 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:16:56,811 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:16:58,927 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_q_proj.safetensors True
Layer: model.layers.4.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.2226075e-02 9.3771603e-05 6.9365597e-05 3.1357977e-05 2.4107103e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-27 17:17:42,128 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-27 17:17:42,129 - INFO - Replacing 'model.layers.4.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  13%|█████████████████████▌                                                                                                                                                 | 29/225 [05:57<42:27, 13.00s/it]2025-04-27 17:17:42,129 - INFO - Skipping layer model.layers.4.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:17:42,129 - INFO - Skipping layer model.layers.4.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:17:42,129 - INFO - Skipping layer model.layers.4.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:17:42,129 - INFO - Layer: model.layers.4.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:17:42,131 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_mlp_gate_proj.safetensors
2025-04-27 17:17:42,131 - INFO - exists: True
2025-04-27 17:17:42,148 - INFO - factorize_layer_kron_svd
2025-04-27 17:17:44,942 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:17:46,420 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:17:47,910 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 17:17:49,395 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 17:17:50,851 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 17:17:52,642 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-27 17:17:54,958 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:17:57,617 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:18:00,340 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 17:18:02,905 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 17:18:05,518 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 17:18:10,643 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_mlp_gate_proj.safetensors True
Layer: model.layers.4.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.0836617e-05 6.9572156e-06 6.8608979e-06 6.6034477e-06 6.4066685e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-27 17:19:39,846 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-27 17:19:39,847 - INFO - Replacing 'model.layers.4.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  15%|████████████████████████▍                                                                                                                                              | 33/225 [07:55<54:56, 17.17s/it]2025-04-27 17:19:39,847 - INFO - Skipping layer model.layers.4.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:19:39,848 - INFO - Layer: model.layers.4.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:19:39,849 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_mlp_down_proj.safetensors
2025-04-27 17:19:39,849 - INFO - exists: True
2025-04-27 17:19:39,881 - INFO - factorize_layer_kron_svd
2025-04-27 17:19:42,917 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:19:46,972 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:19:51,994 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:19:53,444 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:19:55,111 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:19:56,813 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_mlp_down_proj.safetensors True
Layer: model.layers.4.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [2.7665207e-02 6.1080464e-05 2.8755128e-05 2.1798001e-05 1.3714681e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,11008), (4096,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)
2025-04-27 17:21:47,943 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)')
2025-04-27 17:21:47,944 - INFO - Replacing 'model.layers.4.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  16%|█████████████████████████▋                                                                                                                                           | 35/225 [10:03<1:17:08, 24.36s/it]2025-04-27 17:21:47,944 - INFO - Skipping layer model.layers.5.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:21:47,944 - INFO - Layer: model.layers.5.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-27 17:21:47,947 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_5_self_attn_k_proj.safetensors
2025-04-27 17:21:47,947 - INFO - exists: True
2025-04-27 17:21:48,005 - INFO - factorize_layer_kron_svd
2025-04-27 17:21:49,808 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:21:51,655 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-27 17:21:53,222 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:21:54,955 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_5_self_attn_k_proj.safetensors True
Layer: model.layers.5.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [4.8379018e-03 1.5238064e-04 9.5437594e-05 3.4556335e-05 3.0823277e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-27 17:22:35,839 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-27 17:22:35,840 - INFO - Replacing 'model.layers.5.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  16%|███████████████████████████▏                                                                                                                                         | 37/225 [10:51<1:16:05, 24.29s/it]2025-04-27 17:22:35,841 - INFO - Skipping layer model.layers.5.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:22:35,841 - INFO - Skipping layer model.layers.5.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:22:35,842 - INFO - Skipping layer model.layers.5.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:22:35,842 - INFO - Skipping layer model.layers.5.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:22:35,842 - INFO - Skipping layer model.layers.5.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:22:35,842 - INFO - Layer: model.layers.6.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-27 17:22:35,843 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_q_proj.safetensors
2025-04-27 17:22:35,843 - INFO - exists: True
2025-04-27 17:22:35,851 - INFO - factorize_layer_kron_svd
2025-04-27 17:22:37,583 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:22:39,205 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:22:40,657 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 17:22:42,189 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 17:22:43,656 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 17:22:45,384 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-27 17:22:46,894 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:22:48,358 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:22:49,852 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 17:22:51,334 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 17:22:52,850 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 17:22:54,541 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_q_proj.safetensors True
Layer: model.layers.6.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.2221675e-05 8.1780718e-06 7.9897936e-06 7.7061068e-06 7.4714530e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-27 17:23:36,452 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-27 17:23:36,452 - INFO - Replacing 'model.layers.6.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  19%|███████████████████████████████▉                                                                                                                                       | 43/225 [11:52<54:56, 18.11s/it]2025-04-27 17:23:36,453 - INFO - Skipping layer model.layers.6.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:23:36,453 - INFO - Skipping layer model.layers.6.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:23:36,453 - INFO - Skipping layer model.layers.6.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:23:36,453 - INFO - Skipping layer model.layers.6.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:23:36,453 - INFO - Skipping layer model.layers.6.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:23:36,453 - INFO - Layer: model.layers.6.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:23:36,459 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_mlp_down_proj.safetensors
2025-04-27 17:23:36,459 - INFO - exists: True
2025-04-27 17:23:36,467 - INFO - factorize_layer_kron_svd
2025-04-27 17:23:39,513 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:23:44,689 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-27 17:23:46,213 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:23:47,943 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:23:49,656 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_mlp_down_proj.safetensors True
Layer: model.layers.6.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [1.1730455e-02 7.5274904e-05 5.8971975e-05 3.9316310e-05 2.7261001e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,11008), (4096,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)
2025-04-27 17:25:39,051 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)')
2025-04-27 17:25:39,051 - INFO - Replacing 'model.layers.6.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  22%|████████████████████████████████████▎                                                                                                                                  | 49/225 [13:54<55:44, 19.00s/it]2025-04-27 17:25:39,052 - INFO - Skipping layer model.layers.7.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:25:39,052 - INFO - Skipping layer model.layers.7.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:25:39,052 - INFO - Skipping layer model.layers.7.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:25:39,052 - INFO - Skipping layer model.layers.7.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:25:39,052 - INFO - Layer: model.layers.7.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:25:39,055 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_gate_proj.safetensors
2025-04-27 17:25:39,055 - INFO - exists: True
2025-04-27 17:25:39,097 - INFO - factorize_layer_kron_svd
2025-04-27 17:25:40,933 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:25:42,632 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:25:44,340 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:25:46,971 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:25:51,896 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_gate_proj.safetensors True
Layer: model.layers.7.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [6.1598225e-03 4.3083266e-05 2.9594266e-05 2.0862246e-05 1.8714514e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-27 17:27:23,449 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-27 17:27:23,450 - INFO - Replacing 'model.layers.7.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  24%|████████████████████████████████████████                                                                                                                               | 54/225 [15:39<55:50, 19.59s/it]2025-04-27 17:27:23,451 - INFO - Skipping layer model.layers.7.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:27:23,451 - INFO - Layer: model.layers.7.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:27:23,453 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_down_proj.safetensors
2025-04-27 17:27:23,453 - INFO - exists: True
2025-04-27 17:27:23,495 - INFO - factorize_layer_kron_svd
2025-04-27 17:27:26,427 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:27:31,592 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-27 17:27:33,115 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:27:34,762 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:27:36,621 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_down_proj.safetensors True
Layer: model.layers.7.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [4.0638614e-03 2.6415018e-04 8.9995185e-05 2.9939682e-05 2.5749245e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,11008), (4096,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)
2025-04-27 17:29:36,670 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)')
2025-04-27 17:29:36,671 - INFO - Replacing 'model.layers.7.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  25%|█████████████████████████████████████████                                                                                                                            | 56/225 [17:52<1:15:17, 26.73s/it]2025-04-27 17:29:36,672 - INFO - Layer: model.layers.8.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-27 17:29:36,677 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_q_proj.safetensors
2025-04-27 17:29:36,677 - INFO - exists: True
2025-04-27 17:29:36,739 - INFO - factorize_layer_kron_svd
2025-04-27 17:29:38,615 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:29:40,367 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:29:42,069 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:29:43,603 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:29:45,359 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_q_proj.safetensors True
Layer: model.layers.8.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [5.8933757e-03 1.1251512e-04 4.9954931e-05 2.4490531e-05 1.9172267e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-27 17:30:30,452 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-27 17:30:30,453 - INFO - Replacing 'model.layers.8.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  25%|█████████████████████████████████████████▊                                                                                                                           | 57/225 [18:46<1:22:15, 29.38s/it]2025-04-27 17:30:30,453 - INFO - Skipping layer model.layers.8.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:30:30,453 - INFO - Skipping layer model.layers.8.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:30:30,454 - INFO - Skipping layer model.layers.8.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:30:30,454 - INFO - Layer: model.layers.8.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:30:30,456 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_mlp_gate_proj.safetensors
2025-04-27 17:30:30,456 - INFO - exists: True
2025-04-27 17:30:30,470 - INFO - factorize_layer_kron_svd
2025-04-27 17:30:32,400 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:30:34,148 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:30:35,969 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:30:38,825 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:30:44,192 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_mlp_gate_proj.safetensors True
Layer: model.layers.8.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.4417933e-03 7.4210926e-05 3.5308200e-05 2.5064986e-05 1.6473876e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-27 17:32:18,768 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-27 17:32:18,768 - INFO - Replacing 'model.layers.8.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  27%|████████████████████████████████████████████▋                                                                                                                        | 61/225 [20:34<1:18:02, 28.55s/it]2025-04-27 17:32:18,768 - INFO - Skipping layer model.layers.8.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:32:18,769 - INFO - Layer: model.layers.8.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:32:18,774 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_mlp_down_proj.safetensors
2025-04-27 17:32:18,775 - INFO - exists: True
2025-04-27 17:32:18,836 - INFO - factorize_layer_kron_svd
2025-04-27 17:32:22,851 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:32:28,079 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-27 17:32:29,543 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:32:31,225 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:32:32,943 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_mlp_down_proj.safetensors True
Layer: model.layers.8.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [1.9795836e-03 5.3476641e-04 5.7899004e-05 3.2476404e-05 3.0444553e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,11008), (4096,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)
2025-04-27 17:34:23,835 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)')
2025-04-27 17:34:23,835 - INFO - Replacing 'model.layers.8.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  28%|██████████████████████████████████████████████▏                                                                                                                      | 63/225 [22:39<1:35:48, 35.48s/it]2025-04-27 17:34:23,836 - INFO - Skipping layer model.layers.9.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:34:23,836 - INFO - Skipping layer model.layers.9.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:34:23,836 - INFO - Skipping layer model.layers.9.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:34:23,836 - INFO - Skipping layer model.layers.9.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:34:23,836 - INFO - Skipping layer model.layers.9.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:34:23,836 - INFO - Skipping layer model.layers.9.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:34:23,836 - INFO - Layer: model.layers.9.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:34:23,850 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_9_mlp_down_proj.safetensors
2025-04-27 17:34:23,850 - INFO - exists: True
2025-04-27 17:34:23,880 - INFO - factorize_layer_kron_svd
2025-04-27 17:34:26,755 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:34:30,646 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:34:35,624 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:34:37,205 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:34:39,015 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:34:40,740 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_9_mlp_down_proj.safetensors True
Layer: model.layers.9.mlp.down_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [2.8424084e-02 1.8538581e-04 1.7036285e-04 1.0448481e-04 9.5918804e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,11008), (4096,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)
2025-04-27 17:36:35,838 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=4096, bias=False)
)')
2025-04-27 17:36:35,839 - INFO - Replacing 'model.layers.9.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  31%|███████████████████████████████████████████████████▎                                                                                                                 | 70/225 [24:51<1:09:58, 27.09s/it]2025-04-27 17:36:35,839 - INFO - Skipping layer model.layers.10.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:36:35,839 - INFO - Skipping layer model.layers.10.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:36:35,839 - INFO - Skipping layer model.layers.10.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:36:35,839 - INFO - Skipping layer model.layers.10.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:36:35,839 - INFO - Skipping layer model.layers.10.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:36:35,839 - INFO - Skipping layer model.layers.10.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:36:35,839 - INFO - Skipping layer model.layers.10.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:36:35,839 - INFO - Skipping layer model.layers.11.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:36:35,839 - INFO - Skipping layer model.layers.11.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:36:35,839 - INFO - Skipping layer model.layers.11.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:36:35,839 - INFO - Skipping layer model.layers.11.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:36:35,840 - INFO - Layer: model.layers.11.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:36:35,843 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_mlp_gate_proj.safetensors
2025-04-27 17:36:35,843 - INFO - exists: True
2025-04-27 17:36:35,872 - INFO - factorize_layer_kron_svd
2025-04-27 17:36:38,932 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:36:40,543 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:36:42,267 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:36:44,641 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:36:49,437 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_mlp_gate_proj.safetensors True
Layer: model.layers.11.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [2.99881306e-03 1.06764026e-04 2.91248271e-05 2.53626113e-05
 2.07210996e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-27 17:38:18,681 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-27 17:38:18,681 - INFO - Replacing 'model.layers.11.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  36%|████████████████████████████████████████████████████████████▊                                                                                                          | 82/225 [26:34<40:09, 16.85s/it]2025-04-27 17:38:18,682 - INFO - Skipping layer model.layers.11.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:38:18,683 - INFO - Skipping layer model.layers.11.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:38:18,683 - INFO - Layer: model.layers.12.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-27 17:38:18,687 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_q_proj.safetensors
2025-04-27 17:38:18,687 - INFO - exists: True
2025-04-27 17:38:18,723 - INFO - factorize_layer_kron_svd
2025-04-27 17:38:20,688 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:38:22,434 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:38:24,179 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:38:25,673 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:38:27,427 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:38:29,261 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_q_proj.safetensors True
Layer: model.layers.12.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [9.15819407e-03 3.75725394e-05 2.34514755e-05 1.29712425e-05
 1.20524883e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-27 17:39:13,748 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-27 17:39:13,749 - INFO - Replacing 'model.layers.12.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  38%|███████████████████████████████████████████████████████████████                                                                                                        | 85/225 [27:29<39:53, 17.10s/it]2025-04-27 17:39:13,749 - INFO - Skipping layer model.layers.12.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:39:13,749 - INFO - Skipping layer model.layers.12.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:39:13,749 - INFO - Skipping layer model.layers.12.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:39:13,749 - INFO - Layer: model.layers.12.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:39:13,750 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_gate_proj.safetensors
2025-04-27 17:39:13,751 - INFO - exists: True
2025-04-27 17:39:13,758 - INFO - factorize_layer_kron_svd
2025-04-27 17:39:15,609 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:39:17,107 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:39:18,699 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 17:39:20,380 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 17:39:21,884 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 17:39:23,645 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-27 17:39:25,944 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:39:28,510 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:39:31,028 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 17:39:33,561 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 17:39:36,117 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 17:39:41,282 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_gate_proj.safetensors True
Layer: model.layers.12.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.2988171e-05 7.1681275e-06 6.5595987e-06 6.5212694e-06 6.4388596e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-27 17:41:11,933 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-27 17:41:11,934 - INFO - Replacing 'model.layers.12.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  40%|██████████████████████████████████████████████████████████████████                                                                                                     | 89/225 [29:27<45:30, 20.07s/it]2025-04-27 17:41:11,934 - INFO - Layer: model.layers.12.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:41:11,937 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_up_proj.safetensors
2025-04-27 17:41:11,937 - INFO - exists: True
2025-04-27 17:41:11,968 - INFO - factorize_layer_kron_svd
2025-04-27 17:41:13,701 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:41:15,196 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:41:16,731 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 17:41:18,242 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 17:41:19,900 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 17:41:21,737 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-27 17:41:24,004 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:41:26,576 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:41:29,138 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 17:41:31,647 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 17:41:34,180 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 17:41:39,397 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_up_proj.safetensors True
Layer: model.layers.12.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [5.9819213e-06 4.8709753e-06 4.7432140e-06 4.6781370e-06 4.6636528e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-27 17:43:10,443 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-27 17:43:10,443 - INFO - Replacing 'model.layers.12.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  40%|██████████████████████████████████████████████████████████████████                                                                                                   | 90/225 [31:26<1:02:35, 27.82s/it]2025-04-27 17:43:10,443 - INFO - Skipping layer model.layers.12.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:43:10,443 - INFO - Skipping layer model.layers.13.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:43:10,443 - INFO - Skipping layer model.layers.13.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:43:10,444 - INFO - Layer: model.layers.13.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-27 17:43:10,447 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_v_proj.safetensors
2025-04-27 17:43:10,447 - INFO - exists: True
2025-04-27 17:43:10,479 - INFO - factorize_layer_kron_svd
2025-04-27 17:43:12,133 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:43:13,847 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-27 17:43:15,327 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:43:17,030 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:43:18,877 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_v_proj.safetensors True
Layer: model.layers.13.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [5.0983722e-03 5.0585440e-05 2.1063875e-05 1.7282286e-05 1.4230653e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-27 17:43:59,123 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-27 17:43:59,124 - INFO - Replacing 'model.layers.13.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)
Compressing Layers:  42%|█████████████████████████████████████████████████████████████████████▊                                                                                                 | 94/225 [32:14<50:08, 22.96s/it]2025-04-27 17:43:59,125 - INFO - Layer: model.layers.13.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-27 17:43:59,127 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_o_proj.safetensors
2025-04-27 17:43:59,127 - INFO - exists: True
2025-04-27 17:43:59,132 - INFO - factorize_layer_kron_svd
2025-04-27 17:44:00,872 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:44:02,537 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:44:04,235 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:44:05,835 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:44:07,532 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:44:09,439 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_o_proj.safetensors True
Layer: model.layers.13.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.2446343e-02 3.8981681e-05 3.5099336e-05 2.8158551e-05 2.2302480e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-27 17:44:52,518 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-27 17:44:52,518 - INFO - Replacing 'model.layers.13.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  42%|██████████████████████████████████████████████████████████████████████▌                                                                                                | 95/225 [33:08<56:19, 26.00s/it]2025-04-27 17:44:52,519 - INFO - Layer: model.layers.13.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:44:52,522 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_gate_proj.safetensors
2025-04-27 17:44:52,522 - INFO - exists: True
2025-04-27 17:44:52,536 - INFO - factorize_layer_kron_svd
2025-04-27 17:44:54,460 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:44:56,136 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:44:57,914 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:45:00,540 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:45:05,503 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_gate_proj.safetensors True
Layer: model.layers.13.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [6.0720551e-03 4.1635503e-05 2.5478646e-05 2.1152418e-05 1.8824872e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-27 17:46:42,045 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-27 17:46:42,046 - INFO - Replacing 'model.layers.13.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  43%|██████████████████████████████████████████████████████████████████████▍                                                                                              | 96/225 [34:57<1:18:17, 36.41s/it]2025-04-27 17:46:42,047 - INFO - Layer: model.layers.13.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:46:42,060 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_up_proj.safetensors
2025-04-27 17:46:42,060 - INFO - exists: True
2025-04-27 17:46:42,085 - INFO - factorize_layer_kron_svd
2025-04-27 17:46:43,879 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:46:45,668 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:46:47,453 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:46:49,812 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:46:54,029 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:46:59,097 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_up_proj.safetensors True
Layer: model.layers.13.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [6.4682798e-03 6.1749437e-05 2.4349400e-05 2.1919277e-05 2.0116417e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-27 17:48:29,536 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-27 17:48:29,536 - INFO - Replacing 'model.layers.13.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  43%|███████████████████████████████████████████████████████████████████████▏                                                                                             | 97/225 [36:45<1:40:36, 47.16s/it]2025-04-27 17:48:29,536 - INFO - Skipping layer model.layers.13.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:48:29,536 - INFO - Skipping layer model.layers.14.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:48:29,536 - INFO - Skipping layer model.layers.14.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:48:29,536 - INFO - Skipping layer model.layers.14.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:48:29,537 - INFO - Skipping layer model.layers.14.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:48:29,537 - INFO - Layer: model.layers.14.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:48:29,540 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_gate_proj.safetensors
2025-04-27 17:48:29,540 - INFO - exists: True
2025-04-27 17:48:29,566 - INFO - factorize_layer_kron_svd
2025-04-27 17:48:31,389 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:48:33,069 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:48:34,864 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:48:37,352 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:48:42,439 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_gate_proj.safetensors True
Layer: model.layers.14.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [3.2148813e-03 8.8560395e-05 2.4315630e-05 1.8305876e-05 1.6133905e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-27 17:50:10,238 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-27 17:50:10,238 - INFO - Replacing 'model.layers.14.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  46%|███████████████████████████████████████████████████████████████████████████                                                                                         | 103/225 [38:25<1:01:01, 30.01s/it]2025-04-27 17:50:10,239 - INFO - Layer: model.layers.14.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:50:10,241 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_up_proj.safetensors
2025-04-27 17:50:10,241 - INFO - exists: True
2025-04-27 17:50:10,273 - INFO - factorize_layer_kron_svd
2025-04-27 17:50:12,015 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:50:13,720 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:50:15,534 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:50:17,911 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:50:22,520 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:50:27,540 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_up_proj.safetensors True
Layer: model.layers.14.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [9.7202640e-03 2.5967920e-05 2.1275189e-05 1.6423986e-05 1.5875536e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-27 17:51:57,565 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-27 17:51:57,566 - INFO - Replacing 'model.layers.14.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  46%|███████████████████████████████████████████████████████████████████████████▊                                                                                        | 104/225 [40:13<1:18:59, 39.17s/it]2025-04-27 17:51:57,567 - INFO - Skipping layer model.layers.14.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:51:57,567 - INFO - Layer: model.layers.15.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-27 17:51:57,569 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_self_attn_q_proj.safetensors
2025-04-27 17:51:57,569 - INFO - exists: True
2025-04-27 17:51:57,596 - INFO - factorize_layer_kron_svd
2025-04-27 17:51:59,244 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:52:01,067 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:52:02,847 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:52:04,384 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:52:06,052 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:52:07,853 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_self_attn_q_proj.safetensors True
Layer: model.layers.15.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.19919665e-02 7.38329763e-05 2.44418788e-05 1.36780509e-05
 1.00903217e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-27 17:52:51,920 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-27 17:52:51,920 - INFO - Replacing 'model.layers.15.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  47%|█████████████████████████████████████████████████████████████████████████████▎                                                                                      | 106/225 [41:07<1:11:40, 36.14s/it]2025-04-27 17:52:51,921 - INFO - Skipping layer model.layers.15.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:52:51,921 - INFO - Skipping layer model.layers.15.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:52:51,921 - INFO - Skipping layer model.layers.15.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:52:51,921 - INFO - Skipping layer model.layers.15.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:52:51,921 - INFO - Skipping layer model.layers.15.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:52:51,921 - INFO - Skipping layer model.layers.15.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:52:51,921 - INFO - Skipping layer model.layers.16.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:52:51,921 - INFO - Skipping layer model.layers.16.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:52:51,921 - INFO - Layer: model.layers.16.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-27 17:52:51,922 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_self_attn_v_proj.safetensors
2025-04-27 17:52:51,922 - INFO - exists: True
2025-04-27 17:52:51,930 - INFO - factorize_layer_kron_svd
2025-04-27 17:52:53,500 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:52:55,254 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-27 17:52:56,725 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:52:58,462 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:53:00,271 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_self_attn_v_proj.safetensors True
Layer: model.layers.16.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [2.1955712e-02 2.2352620e-05 1.8913308e-05 1.6113720e-05 1.5723988e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-27 17:53:45,721 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-27 17:53:45,721 - INFO - Replacing 'model.layers.16.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)
Compressing Layers:  51%|████████████████████████████████████████████████████████████████████████████████████▊                                                                                 | 115/225 [42:01<32:01, 17.47s/it]2025-04-27 17:53:45,721 - INFO - Skipping layer model.layers.16.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:53:45,721 - INFO - Layer: model.layers.16.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:53:45,723 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_mlp_gate_proj.safetensors
2025-04-27 17:53:45,724 - INFO - exists: True
2025-04-27 17:53:45,731 - INFO - factorize_layer_kron_svd
2025-04-27 17:53:47,698 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:53:49,353 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:53:51,152 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:53:53,507 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:53:58,805 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_mlp_gate_proj.safetensors True
Layer: model.layers.16.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [3.0645484e-03 2.1843742e-05 8.2429324e-06 3.9983265e-06 3.0144206e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-27 17:55:31,736 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-27 17:55:31,736 - INFO - Replacing 'model.layers.16.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  52%|██████████████████████████████████████████████████████████████████████████████████████▎                                                                               | 117/225 [43:47<41:56, 23.30s/it]2025-04-27 17:55:31,737 - INFO - Skipping layer model.layers.16.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:55:31,737 - INFO - Skipping layer model.layers.16.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:55:31,737 - INFO - Layer: model.layers.17.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-27 17:55:31,752 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_self_attn_q_proj.safetensors
2025-04-27 17:55:31,752 - INFO - exists: True
2025-04-27 17:55:31,777 - INFO - factorize_layer_kron_svd
2025-04-27 17:55:33,412 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:55:35,132 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:55:37,127 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:55:38,720 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:55:40,433 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:55:42,273 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_self_attn_q_proj.safetensors True
Layer: model.layers.17.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.9799747e-02 3.0997286e-05 1.8318075e-05 1.5734129e-05 9.7556685e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-27 17:56:26,809 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-27 17:56:26,809 - INFO - Replacing 'model.layers.17.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  53%|████████████████████████████████████████████████████████████████████████████████████████▌                                                                             | 120/225 [44:42<38:31, 22.02s/it]2025-04-27 17:56:26,809 - INFO - Skipping layer model.layers.17.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:56:26,809 - INFO - Skipping layer model.layers.17.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:56:26,810 - INFO - Skipping layer model.layers.17.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:56:26,810 - INFO - Layer: model.layers.17.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:56:26,810 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_mlp_gate_proj.safetensors
2025-04-27 17:56:26,811 - INFO - exists: True
2025-04-27 17:56:26,819 - INFO - factorize_layer_kron_svd
2025-04-27 17:56:28,798 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:56:30,434 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:56:32,156 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:56:34,785 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:56:40,151 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_mlp_gate_proj.safetensors True
Layer: model.layers.17.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.2734500e-02 1.9533880e-05 1.8360362e-05 1.5285072e-05 1.4013754e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-27 17:58:07,929 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-27 17:58:07,930 - INFO - Replacing 'model.layers.17.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  55%|███████████████████████████████████████████████████████████████████████████████████████████▍                                                                          | 124/225 [46:23<38:52, 23.10s/it]2025-04-27 17:58:07,930 - INFO - Skipping layer model.layers.17.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:58:07,930 - INFO - Skipping layer model.layers.17.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:58:07,930 - INFO - Layer: model.layers.18.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-27 17:58:07,934 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_q_proj.safetensors
2025-04-27 17:58:07,934 - INFO - exists: True
2025-04-27 17:58:07,956 - INFO - factorize_layer_kron_svd
2025-04-27 17:58:09,630 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:58:11,327 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:58:13,165 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:58:14,688 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:58:16,307 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:58:18,076 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_q_proj.safetensors True
Layer: model.layers.18.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [4.0405821e-03 8.4903695e-05 8.4368939e-06 7.6583383e-06 6.2006247e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-27 17:59:03,709 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-27 17:59:03,709 - INFO - Replacing 'model.layers.18.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  56%|█████████████████████████████████████████████████████████████████████████████████████████████▋                                                                        | 127/225 [47:19<35:47, 21.92s/it]2025-04-27 17:59:03,709 - INFO - Skipping layer model.layers.18.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:59:03,709 - INFO - Skipping layer model.layers.18.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:59:03,709 - INFO - Skipping layer model.layers.18.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 17:59:03,709 - INFO - Layer: model.layers.18.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 17:59:03,711 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_mlp_gate_proj.safetensors
2025-04-27 17:59:03,711 - INFO - exists: True
2025-04-27 17:59:03,726 - INFO - factorize_layer_kron_svd
2025-04-27 17:59:06,606 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:59:08,483 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 17:59:10,328 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 17:59:12,935 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 17:59:17,980 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_mlp_gate_proj.safetensors True
Layer: model.layers.18.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [8.0129793e-03 1.8024977e-05 1.6487887e-05 1.3588725e-05 1.1236772e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-27 18:00:49,228 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-27 18:00:49,228 - INFO - Replacing 'model.layers.18.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  58%|████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                     | 131/225 [49:04<36:39, 23.40s/it]2025-04-27 18:00:49,229 - INFO - Skipping layer model.layers.18.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:00:49,229 - INFO - Skipping layer model.layers.18.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:00:49,229 - INFO - Layer: model.layers.19.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-27 18:00:49,231 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_self_attn_q_proj.safetensors
2025-04-27 18:00:49,232 - INFO - exists: True
2025-04-27 18:00:49,257 - INFO - factorize_layer_kron_svd
2025-04-27 18:00:51,158 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 18:00:53,066 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-27 18:00:54,572 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 18:00:56,341 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_self_attn_q_proj.safetensors True
Layer: model.layers.19.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [2.2634035e-03 7.5883843e-05 1.0972199e-05 7.9327338e-06 7.6394726e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-27 18:01:40,606 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-27 18:01:40,606 - INFO - Replacing 'model.layers.19.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  60%|██████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                   | 134/225 [49:56<32:59, 21.75s/it]2025-04-27 18:01:40,607 - INFO - Skipping layer model.layers.19.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:01:40,607 - INFO - Skipping layer model.layers.19.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:01:40,607 - INFO - Skipping layer model.layers.19.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:01:40,607 - INFO - Layer: model.layers.19.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 18:01:40,608 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_mlp_gate_proj.safetensors
2025-04-27 18:01:40,609 - INFO - exists: True
2025-04-27 18:01:40,619 - INFO - factorize_layer_kron_svd
2025-04-27 18:01:44,112 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 18:01:45,855 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 18:01:47,731 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-27 18:01:50,408 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 18:01:56,364 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_mlp_gate_proj.safetensors True
Layer: model.layers.19.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [6.8501011e-03 1.8891638e-05 1.4157393e-05 1.3611631e-05 1.2496623e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-27 18:03:31,730 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-27 18:03:31,731 - INFO - Replacing 'model.layers.19.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  61%|█████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                | 138/225 [51:47<34:27, 23.76s/it]2025-04-27 18:03:31,731 - INFO - Skipping layer model.layers.19.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,731 - INFO - Skipping layer model.layers.19.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,731 - INFO - Skipping layer model.layers.20.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,731 - INFO - Skipping layer model.layers.20.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,731 - INFO - Skipping layer model.layers.20.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,731 - INFO - Skipping layer model.layers.20.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,731 - INFO - Skipping layer model.layers.20.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,731 - INFO - Skipping layer model.layers.20.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,731 - INFO - Skipping layer model.layers.20.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,732 - INFO - Skipping layer model.layers.21.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,732 - INFO - Skipping layer model.layers.21.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,732 - INFO - Skipping layer model.layers.21.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,732 - INFO - Skipping layer model.layers.21.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,732 - INFO - Skipping layer model.layers.21.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,732 - INFO - Skipping layer model.layers.21.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,732 - INFO - Skipping layer model.layers.21.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,732 - INFO - Skipping layer model.layers.22.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,732 - INFO - Skipping layer model.layers.22.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,732 - INFO - Skipping layer model.layers.22.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,732 - INFO - Skipping layer model.layers.22.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:03:31,732 - INFO - Layer: model.layers.22.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-27 18:03:31,735 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_gate_proj.safetensors
2025-04-27 18:03:31,735 - INFO - exists: True
2025-04-27 18:03:31,760 - INFO - factorize_layer_kron_svd
2025-04-27 18:03:33,554 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 18:03:35,024 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 18:03:36,520 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 18:03:38,075 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 18:03:39,530 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 18:03:41,272 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-27 18:03:43,502 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 18:03:46,090 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 18:03:48,570 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 18:03:51,060 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 18:03:53,853 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 18:03:59,024 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_gate_proj.safetensors True
Layer: model.layers.22.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.1882697e-05 6.5561608e-06 6.1485889e-06 6.0917669e-06 5.8484889e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-27 18:05:30,743 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-27 18:05:30,744 - INFO - Replacing 'model.layers.22.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                | 159/225 [53:46<11:55, 10.83s/it]2025-04-27 18:05:30,744 - INFO - Skipping layer model.layers.22.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,744 - INFO - Skipping layer model.layers.22.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,744 - INFO - Skipping layer model.layers.23.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,744 - INFO - Skipping layer model.layers.23.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,744 - INFO - Skipping layer model.layers.23.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,744 - INFO - Skipping layer model.layers.23.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,744 - INFO - Skipping layer model.layers.23.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,744 - INFO - Skipping layer model.layers.23.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.23.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.24.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.24.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.24.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.24.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.24.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.24.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.24.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.25.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.25.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.25.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.25.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.25.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.25.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.25.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.26.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.26.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.26.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.26.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.26.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.26.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.26.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.27.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.27.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.27.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.27.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.27.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.27.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.27.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Skipping layer model.layers.28.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:05:30,745 - INFO - Layer: model.layers.28.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-27 18:05:30,748 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_28_self_attn_k_proj.safetensors
2025-04-27 18:05:30,748 - INFO - exists: True
2025-04-27 18:05:30,769 - INFO - factorize_layer_kron_svd
2025-04-27 18:05:32,347 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 18:05:33,875 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 18:05:35,363 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 18:05:37,065 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 18:05:38,613 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 18:05:40,349 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-27 18:05:41,787 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-27 18:05:43,229 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-27 18:05:44,713 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-27 18:05:46,170 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-27 18:05:47,624 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-27 18:05:49,356 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_28_self_attn_k_proj.safetensors True
Layer: model.layers.28.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
layer module Linear(in_features=4096, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.3403688e-05 6.0787293e-06 5.6532840e-06 5.4129196e-06 5.2258588e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-27 18:06:32,826 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-27 18:06:32,827 - INFO - Replacing 'model.layers.28.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                    | 198/225 [54:48<02:09,  4.78s/it]2025-04-27 18:06:32,827 - INFO - Skipping layer model.layers.28.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,827 - INFO - Skipping layer model.layers.28.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,827 - INFO - Skipping layer model.layers.28.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,827 - INFO - Skipping layer model.layers.28.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,827 - INFO - Skipping layer model.layers.28.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,827 - INFO - Skipping layer model.layers.29.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,827 - INFO - Skipping layer model.layers.29.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,827 - INFO - Skipping layer model.layers.29.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,827 - INFO - Skipping layer model.layers.29.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,827 - INFO - Skipping layer model.layers.29.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,827 - INFO - Skipping layer model.layers.29.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,827 - INFO - Skipping layer model.layers.29.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,828 - INFO - Skipping layer model.layers.30.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,828 - INFO - Skipping layer model.layers.30.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,828 - INFO - Skipping layer model.layers.30.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,828 - INFO - Skipping layer model.layers.30.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,828 - INFO - Skipping layer model.layers.30.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,828 - INFO - Skipping layer model.layers.30.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,828 - INFO - Skipping layer model.layers.30.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,828 - INFO - Skipping layer model.layers.31.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,828 - INFO - Skipping layer model.layers.31.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,828 - INFO - Skipping layer model.layers.31.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,828 - INFO - Skipping layer model.layers.31.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,828 - INFO - Skipping layer model.layers.31.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,828 - INFO - Skipping layer model.layers.31.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,828 - INFO - Skipping layer model.layers.31.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-27 18:06:32,828 - INFO - Skipping layer lm_head: Sensitivity ratio >= 1.0 (1.00). No compression.
Compressing Layers: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [54:48<00:00, 14.62s/it]
2025-04-27 18:06:32,828 - INFO - Compression finished. Processed: 36, Skipped (Ratio>=1 or No Sensitivity/Factors): 189, Failed: 0
2025-04-27 18:06:32,830 - INFO - Total parameters after compression: 5980975104
2025-04-27 18:06:32,831 - INFO - C rate : 0.8875936785196955
2025-04-27 18:06:32,831 - INFO - Saving compressed model to ./llama10
2025-04-27 18:06:32,831 - INFO - Compressed model and tokenizer saved.
2025-04-27 18:06:32,842 - INFO - Evaluating on wikitext2
Evaluating:   0%|                                                                                                                                                                                         | 0/21 [00:00<?, ?it/s]Evaluating:   5%|████████▍                                                                                                                                                                        | 1/21 [00:02<00:59,  2.98s/it]Evaluating:  10%|████████████████▊                                                                                                                                                                | 2/21 [00:04<00:35,  1.86s/it]Evaluating:  14%|█████████████████████████▎                                                                                                                                                       | 3/21 [00:05<00:27,  1.51s/it]Evaluating:  19%|█████████████████████████████████▋                                                                                                                                               | 4/21 [00:06<00:22,  1.35s/it]Evaluating:  24%|██████████████████████████████████████████▏                                                                                                                                      | 5/21 [00:07<00:20,  1.25s/it]Evaluating:  29%|██████████████████████████████████████████████████▌                                                                                                                              | 6/21 [00:08<00:17,  1.20s/it]Evaluating:  33%|███████████████████████████████████████████████████████████                                                                                                                      | 7/21 [00:09<00:16,  1.17s/it]Evaluating:  38%|███████████████████████████████████████████████████████████████████▍                                                                                                             | 8/21 [00:10<00:14,  1.14s/it]Evaluating:  43%|███████████████████████████████████████████████████████████████████████████▊                                                                                                     | 9/21 [00:11<00:13,  1.13s/it]Evaluating:  48%|███████████████████████████████████████████████████████████████████████████████████▊                                                                                            | 10/21 [00:12<00:12,  1.12s/it]Evaluating:  52%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                   | 11/21 [00:13<00:11,  1.11s/it]Evaluating:  57%|████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                           | 12/21 [00:14<00:09,  1.11s/it]Evaluating:  62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                   | 13/21 [00:16<00:08,  1.10s/it]Evaluating:  67%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                          | 14/21 [00:17<00:07,  1.10s/it]Evaluating:  71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                  | 15/21 [00:18<00:06,  1.10s/it]Evaluating:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                          | 16/21 [00:19<00:05,  1.10s/it]Evaluating:  81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                 | 17/21 [00:20<00:04,  1.10s/it]Evaluating:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                         | 18/21 [00:21<00:03,  1.10s/it]Evaluating:  90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                | 19/21 [00:22<00:02,  1.10s/it]Evaluating:  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 20/21 [00:23<00:01,  1.10s/it]Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:24<00:00,  1.02s/it]Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:24<00:00,  1.17s/it]
2025-04-27 18:07:04,976 - INFO - wikitext2 perplexity: 10.5625
2025-04-27 18:07:04,976 - INFO - Evaluating on ptb
nlls.shape torch.Size([16376])
Mean NLL: 2.359375
Evaluating:   0%|                                                                                                                                                                                          | 0/7 [00:00<?, ?it/s]Evaluating:  14%|█████████████████████████▍                                                                                                                                                        | 1/7 [00:01<00:06,  1.12s/it]Evaluating:  29%|██████████████████████████████████████████████████▊                                                                                                                               | 2/7 [00:02<00:05,  1.10s/it]Evaluating:  43%|████████████████████████████████████████████████████████████████████████████▎                                                                                                     | 3/7 [00:03<00:04,  1.10s/it]Evaluating:  57%|█████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                            | 4/7 [00:04<00:03,  1.10s/it]Evaluating:  71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                  | 5/7 [00:05<00:02,  1.10s/it]Evaluating:  86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 6/7 [00:06<00:01,  1.10s/it]Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:07<00:00,  1.06s/it]Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:07<00:00,  1.08s/it]
2025-04-27 18:07:14,694 - INFO - ptb perplexity: 50.5000
2025-04-27 18:07:14,695 - INFO - Evaluation results:
2025-04-27 18:07:14,695 - INFO -   wikitext2: 10.5625
2025-04-27 18:07:14,695 - INFO -   ptb: 50.5000
nlls.shape torch.Size([16376])
Mean NLL: 3.921875

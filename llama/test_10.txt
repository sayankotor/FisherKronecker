2025-04-27 22:14:28,980 - INFO - Loading model: unsloth/llama-2-7b-chatmodel.layers.28.self_attn.k_proj 0.3model.layers.23.mlp.gate_proj 0.5model.layers.23.mlp.down_proj 0.5model.layers.22.mlp.up_proj 0.5model.layers.22.mlp.down_proj 0.5model.layers.21.mlp.up_proj 0.5model.layers.21.mlp.down_proj 0.5model.layers.20.mlp.gate_proj 0.5model.layers.19.mlp.up_proj 0.5model.layers.19.self_attn.q_proj 0.3model.layers.18.mlp.up_proj 0.5model.layers.18.self_attn.q_proj 0.3model.layers.18.self_attn.k_proj 0.3model.layers.17.self_attn.q_proj 0.3model.layers.16.self_attn.v_proj 0.3model.layers.15.self_attn.q_proj 0.3model.layers.14.mlp.gate_proj 0.5model.layers.14.mlp.up_proj 0.5model.layers.13.mlp.up_proj 0.5model.layers.13.self_attn.q_proj 0.3model.layers.13.self_attn.k_proj 0.3model.layers.13.self_attn.v_proj 0.3model.layers.13.self_attn.o_proj 0.3model.layers.12.mlp.gate_proj 0.5model.layers.12.mlp.up_proj 0.5model.layers.12.self_attn.q_proj 0.3model.layers.12.self_attn.k_proj 0.3model.layers.11.self_attn.q_proj 0.3model.layers.8.self_attn.q_proj 0.3model.layers.8.self_attn.k_proj 0.3model.layers.7.mlp.down_proj 0.5model.layers.7.self_attn.q_proj 0.3model.layers.7.self_attn.k_proj 0.3model.layers.6.self_attn.q_proj 0.3model.layers.6.self_attn.k_proj 0.3model.layers.5.self_attn.k_proj 0.3model.layers.4.self_attn.q_proj 0.3model.layers.4.self_attn.k_proj 0.3model.layers.3.self_attn.q_proj 0.3model.layers.3.self_attn.k_proj 0.3model.layers.2.mlp.gate_proj 0.5model.layers.2.self_attn.q_proj 0.3model.layers.2.self_attn.k_proj 0.3[2025-04-27 22:14:33,756] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Loading checkpoint shards:   0%|                                                                                                                                                                           | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|██████████████████████████████████████████████████████▎                                                                                                            | 1/3 [00:13<00:27, 13.84s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                      | 2/3 [00:28<00:14, 14.23s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:37<00:00, 12.03s/it]Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:37<00:00, 12.58s/it]2025-04-27 22:15:14,443 - INFO - Model loaded with dtype torch.bfloat162025-04-27 22:16:04,721 - INFO - Model moved to cuda:02025-04-27 22:16:04,723 - INFO - Total parameters before compression: 67384156162025-04-27 22:16:04,724 - INFO - Found 225 linear layers to potentially compress.Compressing Layers:   0%|                                                                                                                                                                                | 0/225 [00:00<?, ?it/s]2025-04-27 22:16:04,725 - INFO - Skipping layer model.layers.0.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:04,725 - INFO - Skipping layer model.layers.0.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:04,725 - INFO - Skipping layer model.layers.0.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:04,725 - INFO - Skipping layer model.layers.0.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:04,725 - INFO - Skipping layer model.layers.0.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:04,725 - INFO - Skipping layer model.layers.0.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:04,725 - INFO - Skipping layer model.layers.0.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:04,725 - INFO - Skipping layer model.layers.1.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:04,726 - INFO - Skipping layer model.layers.1.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:04,726 - INFO - Skipping layer model.layers.1.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:04,726 - INFO - Skipping layer model.layers.1.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:04,726 - INFO - Skipping layer model.layers.1.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:04,726 - INFO - Skipping layer model.layers.1.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:04,726 - INFO - Skipping layer model.layers.1.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:04,726 - INFO - Layer: model.layers.2.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:16:04,728 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_q_proj.safetensors2025-04-27 22:16:04,728 - INFO - exists: True2025-04-27 22:16:04,730 - INFO - factorize_layer_kron_svd2025-04-27 22:16:06,484 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:16:07,985 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:16:09,466 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:16:10,928 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:16:12,443 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:16:14,328 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:16:15,862 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:16:17,321 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:16:18,989 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:16:20,594 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:16:22,053 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:16:23,837 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_q_proj.safetensors TrueLayer: model.layers.2.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.6124124e-05 1.1238000e-05 1.0046171e-05 9.5376463e-06 9.3023737e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:17:06,525 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:17:06,525 - INFO - Replacing 'model.layers.2.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:   7%|███████████▏                                                                                                                                                           | 15/225 [01:01<14:25,  4.12s/it]2025-04-27 22:17:06,526 - INFO - Layer: model.layers.2.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:17:06,530 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_k_proj.safetensors2025-04-27 22:17:06,530 - INFO - exists: True2025-04-27 22:17:06,549 - INFO - factorize_layer_kron_svd2025-04-27 22:17:08,716 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:17:10,470 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:17:11,981 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:17:13,720 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_k_proj.safetensors TrueLayer: model.layers.2.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.0046256e-03 1.5359199e-04 9.4199255e-05 5.9006747e-05 4.1726034e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:17:54,224 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:17:54,224 - INFO - Replacing 'model.layers.2.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:   7%|███████████▉                                                                                                                                                           | 16/225 [01:49<27:33,  7.91s/it]2025-04-27 22:17:54,225 - INFO - Skipping layer model.layers.2.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:17:54,225 - INFO - Skipping layer model.layers.2.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:17:54,225 - INFO - Layer: model.layers.2.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:17:54,226 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_gate_proj.safetensors2025-04-27 22:17:54,227 - INFO - exists: True2025-04-27 22:17:54,238 - INFO - factorize_layer_kron_svd2025-04-27 22:17:56,079 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:17:57,552 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:17:59,122 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:18:00,607 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:18:02,117 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:18:03,824 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:18:06,144 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:18:08,742 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:18:11,286 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:18:13,799 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:18:16,314 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:18:21,294 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_gate_proj.safetensors TrueLayer: model.layers.2.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.0536157e-05 7.8889116e-06 7.3416263e-06 6.9958951e-06 6.9346693e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:19:49,095 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:19:49,095 - INFO - Replacing 'model.layers.2.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:   8%|██████████████                                                                                                                                                         | 19/225 [03:44<55:28, 16.16s/it]2025-04-27 22:19:49,096 - INFO - Skipping layer model.layers.2.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:19:49,096 - INFO - Skipping layer model.layers.2.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:19:49,096 - INFO - Layer: model.layers.3.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:19:49,100 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_q_proj.safetensors2025-04-27 22:19:49,100 - INFO - exists: True2025-04-27 22:19:49,159 - INFO - factorize_layer_kron_svd2025-04-27 22:19:51,096 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:19:52,890 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:19:54,399 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:19:56,189 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_q_proj.safetensors TrueLayer: model.layers.3.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.1026561e-02 7.6449120e-05 6.9494883e-05 4.4865315e-05 4.0528874e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:20:37,305 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:20:37,306 - INFO - Replacing 'model.layers.3.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  10%|████████████████▎                                                                                                                                                      | 22/225 [04:32<54:35, 16.13s/it]2025-04-27 22:20:37,307 - INFO - Layer: model.layers.3.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:20:37,310 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_k_proj.safetensors2025-04-27 22:20:37,310 - INFO - exists: True2025-04-27 22:20:37,325 - INFO - factorize_layer_kron_svd2025-04-27 22:20:39,327 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:20:40,806 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:20:42,288 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:20:43,738 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:20:45,240 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:20:46,964 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:20:48,408 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:20:49,880 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:20:51,324 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:20:52,776 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:20:54,257 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:20:55,859 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_k_proj.safetensors TrueLayer: model.layers.3.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.5212615e-05 7.6301594e-06 6.8745117e-06 6.6968514e-06 6.6398402e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:21:37,024 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:21:37,025 - INFO - Replacing 'model.layers.3.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  10%|████████████████▊                                                                                                                                                    | 23/225 [05:32<1:11:32, 21.25s/it]2025-04-27 22:21:37,026 - INFO - Skipping layer model.layers.3.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:21:37,026 - INFO - Skipping layer model.layers.3.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:21:37,026 - INFO - Skipping layer model.layers.3.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:21:37,027 - INFO - Skipping layer model.layers.3.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:21:37,027 - INFO - Skipping layer model.layers.3.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:21:37,027 - INFO - Layer: model.layers.4.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:21:37,029 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_q_proj.safetensors2025-04-27 22:21:37,029 - INFO - exists: True2025-04-27 22:21:37,045 - INFO - factorize_layer_kron_svd2025-04-27 22:21:38,840 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:21:40,513 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:21:42,257 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:21:43,805 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:21:45,531 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_q_proj.safetensors TrueLayer: model.layers.4.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.2226075e-02 9.3771603e-05 6.9365597e-05 3.1357977e-05 2.4107103e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:22:28,817 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:22:28,817 - INFO - Replacing 'model.layers.4.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  13%|█████████████████████▌                                                                                                                                                 | 29/225 [06:24<48:44, 14.92s/it]2025-04-27 22:22:28,818 - INFO - Layer: model.layers.4.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:22:28,824 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_k_proj.safetensors2025-04-27 22:22:28,824 - INFO - exists: True2025-04-27 22:22:28,850 - INFO - factorize_layer_kron_svd2025-04-27 22:22:30,680 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:22:32,159 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:22:33,641 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:22:35,116 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:22:36,689 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:22:38,553 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:22:39,996 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:22:41,451 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:22:42,880 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:22:44,364 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:22:45,814 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:22:47,535 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_k_proj.safetensors TrueLayer: model.layers.4.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.5640973e-05 8.6818372e-06 8.1892204e-06 8.0699401e-06 7.7858249e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:23:31,023 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:23:31,023 - INFO - Replacing 'model.layers.4.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  13%|██████████████████████                                                                                                                                               | 30/225 [07:26<1:04:53, 19.97s/it]2025-04-27 22:23:31,024 - INFO - Skipping layer model.layers.4.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:23:31,024 - INFO - Skipping layer model.layers.4.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:23:31,024 - INFO - Skipping layer model.layers.4.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:23:31,024 - INFO - Skipping layer model.layers.4.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:23:31,024 - INFO - Skipping layer model.layers.4.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:23:31,024 - INFO - Skipping layer model.layers.5.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:23:31,024 - INFO - Layer: model.layers.5.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:23:31,026 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_5_self_attn_k_proj.safetensors2025-04-27 22:23:31,026 - INFO - exists: True2025-04-27 22:23:31,034 - INFO - factorize_layer_kron_svd2025-04-27 22:23:32,817 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:23:34,552 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:23:36,085 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:23:37,984 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_5_self_attn_k_proj.safetensors TrueLayer: model.layers.5.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.8379018e-03 1.5238064e-04 9.5437594e-05 3.4556335e-05 3.0823277e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:24:19,924 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:24:19,925 - INFO - Replacing 'model.layers.5.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  16%|███████████████████████████▍                                                                                                                                           | 37/225 [08:15<41:33, 13.27s/it]2025-04-27 22:24:19,925 - INFO - Skipping layer model.layers.5.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:24:19,925 - INFO - Skipping layer model.layers.5.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:24:19,925 - INFO - Skipping layer model.layers.5.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:24:19,925 - INFO - Skipping layer model.layers.5.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:24:19,925 - INFO - Skipping layer model.layers.5.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:24:19,926 - INFO - Layer: model.layers.6.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:24:19,927 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_q_proj.safetensors2025-04-27 22:24:19,927 - INFO - exists: True2025-04-27 22:24:19,935 - INFO - factorize_layer_kron_svd2025-04-27 22:24:21,748 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:24:23,258 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:24:24,764 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:24:26,276 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:24:27,827 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:24:29,544 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:24:31,568 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:24:45,563 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:24:58,576 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:25:08,763 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:25:10,239 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:25:11,933 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_q_proj.safetensors TrueLayer: model.layers.6.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.2221675e-05 8.1780718e-06 7.9897936e-06 7.7061068e-06 7.4714530e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:25:52,920 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:25:52,921 - INFO - Replacing 'model.layers.6.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  19%|███████████████████████████████▉                                                                                                                                       | 43/225 [09:48<42:51, 14.13s/it]2025-04-27 22:25:52,921 - INFO - Layer: model.layers.6.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:25:52,926 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_k_proj.safetensors2025-04-27 22:25:52,926 - INFO - exists: True2025-04-27 22:25:52,935 - INFO - factorize_layer_kron_svd2025-04-27 22:25:54,825 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:25:56,319 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:25:57,793 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:25:59,368 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:26:00,888 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:26:02,648 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:26:04,124 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:26:05,622 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:26:07,274 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:26:08,875 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:26:10,341 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:26:12,066 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_k_proj.safetensors TrueLayer: model.layers.6.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.4503589e-05 7.8944504e-06 7.5118478e-06 7.3252472e-06 7.0036058e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:27:07,469 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:27:07,470 - INFO - Replacing 'model.layers.6.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  20%|████████████████████████████████▋                                                                                                                                      | 44/225 [11:02<58:00, 19.23s/it]2025-04-27 22:27:07,471 - INFO - Skipping layer model.layers.6.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:27:07,471 - INFO - Skipping layer model.layers.6.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:27:07,471 - INFO - Skipping layer model.layers.6.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:27:07,471 - INFO - Skipping layer model.layers.6.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:27:07,471 - INFO - Skipping layer model.layers.6.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:27:07,471 - INFO - Layer: model.layers.7.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:27:07,478 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_q_proj.safetensors2025-04-27 22:27:07,479 - INFO - exists: True2025-04-27 22:27:07,580 - INFO - factorize_layer_kron_svd2025-04-27 22:27:12,200 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:27:15,506 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:27:17,477 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:27:21,867 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_q_proj.safetensors TrueLayer: model.layers.7.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.46691271e-03 1.17237636e-04 4.46072554e-05 2.93035046e-05 2.62491722e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:28:34,167 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:28:34,167 - INFO - Replacing 'model.layers.7.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  22%|█████████████████████████████████████                                                                                                                                  | 50/225 [12:29<50:14, 17.22s/it]2025-04-27 22:28:34,167 - INFO - Layer: model.layers.7.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:28:34,172 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_k_proj.safetensors2025-04-27 22:28:34,172 - INFO - exists: True2025-04-27 22:28:34,187 - INFO - factorize_layer_kron_svd2025-04-27 22:28:38,170 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:28:40,397 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:28:43,097 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:28:46,869 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_k_proj.safetensors TrueLayer: model.layers.7.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.3513221e-03 9.5677962e-05 5.9629248e-05 3.8686332e-05 2.6624961e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:29:28,918 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:29:28,918 - INFO - Replacing 'model.layers.7.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  23%|█████████████████████████████████████▊                                                                                                                                 | 51/225 [13:24<59:50, 20.63s/it]2025-04-27 22:29:28,919 - INFO - Skipping layer model.layers.7.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:29:28,919 - INFO - Skipping layer model.layers.7.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:29:28,919 - INFO - Skipping layer model.layers.7.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:29:28,919 - INFO - Skipping layer model.layers.7.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:29:28,919 - INFO - Layer: model.layers.7.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:29:28,920 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_down_proj.safetensors2025-04-27 22:29:28,920 - INFO - exists: True2025-04-27 22:29:28,929 - INFO - factorize_layer_kron_svd2025-04-27 22:29:31,928 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:29:38,083 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:29:40,015 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:29:41,951 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:29:44,613 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_down_proj.safetensors TrueLayer: model.layers.7.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=11008, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x11008)...  SVD complete. Singular values (top 5): [4.0638614e-03 2.6415018e-04 8.9995185e-05 2.9939682e-05 2.5749245e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,11008), (4096,1496)factorized_sequential Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))2025-04-27 22:32:23,990 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))')2025-04-27 22:32:23,991 - INFO - Replacing 'model.layers.7.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)Compressing Layers:  25%|█████████████████████████████████████████                                                                                                                            | 56/225 [16:19<1:14:03, 26.29s/it]2025-04-27 22:32:23,991 - INFO - Layer: model.layers.8.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:32:23,992 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_q_proj.safetensors2025-04-27 22:32:23,992 - INFO - exists: True2025-04-27 22:32:24,044 - INFO - factorize_layer_kron_svd2025-04-27 22:32:26,013 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:32:27,843 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:32:29,743 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:32:31,296 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:32:33,124 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_q_proj.safetensors TrueLayer: model.layers.8.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [5.8933757e-03 1.1251512e-04 4.9954931e-05 2.4490531e-05 1.9172267e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:33:24,067 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:33:24,067 - INFO - Replacing 'model.layers.8.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  25%|█████████████████████████████████████████▊                                                                                                                           | 57/225 [17:19<1:23:11, 29.71s/it]2025-04-27 22:33:24,068 - INFO - Layer: model.layers.8.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:33:24,073 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_k_proj.safetensors2025-04-27 22:33:24,073 - INFO - exists: True2025-04-27 22:33:24,108 - INFO - factorize_layer_kron_svd2025-04-27 22:33:28,176 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:33:31,293 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:33:34,499 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:33:37,899 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:33:41,109 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:33:44,695 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:33:46,541 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:33:48,805 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:33:51,971 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:33:55,031 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:33:56,751 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:33:59,383 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_k_proj.safetensors TrueLayer: model.layers.8.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.4188925e-05 7.6309198e-06 7.4526779e-06 7.2449202e-06 6.7348119e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:34:59,405 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:34:59,406 - INFO - Replacing 'model.layers.8.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  26%|██████████████████████████████████████████▌                                                                                                                          | 58/225 [18:54<1:45:44, 37.99s/it]2025-04-27 22:34:59,407 - INFO - Skipping layer model.layers.8.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,407 - INFO - Skipping layer model.layers.8.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,407 - INFO - Skipping layer model.layers.8.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,407 - INFO - Skipping layer model.layers.8.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,407 - INFO - Skipping layer model.layers.8.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,407 - INFO - Skipping layer model.layers.9.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,408 - INFO - Skipping layer model.layers.9.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,408 - INFO - Skipping layer model.layers.9.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,408 - INFO - Skipping layer model.layers.9.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,408 - INFO - Skipping layer model.layers.9.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,408 - INFO - Skipping layer model.layers.9.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,408 - INFO - Skipping layer model.layers.9.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,408 - INFO - Skipping layer model.layers.10.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,408 - INFO - Skipping layer model.layers.10.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,408 - INFO - Skipping layer model.layers.10.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,408 - INFO - Skipping layer model.layers.10.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,408 - INFO - Skipping layer model.layers.10.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,408 - INFO - Skipping layer model.layers.10.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,408 - INFO - Skipping layer model.layers.10.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:59,408 - INFO - Layer: model.layers.11.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:34:59,411 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_self_attn_q_proj.safetensors2025-04-27 22:34:59,411 - INFO - exists: True2025-04-27 22:34:59,474 - INFO - factorize_layer_kron_svd2025-04-27 22:35:03,665 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:35:06,533 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:35:09,179 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:35:12,672 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:35:16,033 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:35:17,888 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_self_attn_q_proj.safetensors TrueLayer: model.layers.11.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [2.6593389e-02 3.8675182e-05 2.0216057e-05 1.6013246e-05 1.3640273e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:35:57,834 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:35:57,835 - INFO - Replacing 'model.layers.11.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  35%|█████████████████████████████████████████████████████████▉                                                                                                             | 78/225 [19:53<25:48, 10.54s/it]2025-04-27 22:35:57,836 - INFO - Skipping layer model.layers.11.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:35:57,836 - INFO - Skipping layer model.layers.11.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:35:57,836 - INFO - Skipping layer model.layers.11.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:35:57,836 - INFO - Skipping layer model.layers.11.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:35:57,836 - INFO - Skipping layer model.layers.11.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:35:57,837 - INFO - Skipping layer model.layers.11.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:35:57,837 - INFO - Layer: model.layers.12.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:35:57,840 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_q_proj.safetensors2025-04-27 22:35:57,840 - INFO - exists: True2025-04-27 22:35:57,853 - INFO - factorize_layer_kron_svd2025-04-27 22:35:59,910 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:36:01,969 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:36:05,784 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:36:09,089 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:36:12,703 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:36:15,836 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_q_proj.safetensors TrueLayer: model.layers.12.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [9.15819407e-03 3.75725394e-05 2.34514755e-05 1.29712425e-05 1.20524883e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:37:03,877 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:37:03,878 - INFO - Replacing 'model.layers.12.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  38%|███████████████████████████████████████████████████████████████                                                                                                        | 85/225 [20:59<23:51, 10.23s/it]2025-04-27 22:37:03,879 - INFO - Layer: model.layers.12.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:37:03,884 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_k_proj.safetensors2025-04-27 22:37:03,884 - INFO - exists: True2025-04-27 22:37:03,896 - INFO - factorize_layer_kron_svd2025-04-27 22:37:07,983 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:37:11,168 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:37:13,882 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:37:15,829 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:37:18,572 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:37:22,580 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:37:24,945 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:37:26,804 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:37:28,810 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:37:31,099 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:37:32,974 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:37:34,668 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_k_proj.safetensors TrueLayer: model.layers.12.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.35002065e-05 7.74964155e-06 7.61668571e-06 7.45528405e-06 7.28192163e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:38:16,286 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:38:16,287 - INFO - Replacing 'model.layers.12.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  38%|███████████████████████████████████████████████████████████████▊                                                                                                       | 86/225 [22:11<31:30, 13.60s/it]2025-04-27 22:38:16,288 - INFO - Skipping layer model.layers.12.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:38:16,288 - INFO - Skipping layer model.layers.12.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:38:16,288 - INFO - Layer: model.layers.12.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:38:16,291 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_gate_proj.safetensors2025-04-27 22:38:16,292 - INFO - exists: True2025-04-27 22:38:16,303 - INFO - factorize_layer_kron_svd2025-04-27 22:38:20,613 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:38:23,881 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:38:27,095 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:38:29,390 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:38:31,181 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:38:35,271 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:38:40,407 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:38:43,901 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:38:47,207 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:38:50,434 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:38:53,005 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:38:57,445 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_gate_proj.safetensors TrueLayer: model.layers.12.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.2988171e-05 7.1681275e-06 6.5595987e-06 6.5212694e-06 6.4388596e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:40:41,155 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:40:41,155 - INFO - Replacing 'model.layers.12.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  40%|██████████████████████████████████████████████████████████████████                                                                                                     | 89/225 [24:36<45:40, 20.15s/it]2025-04-27 22:40:41,156 - INFO - Layer: model.layers.12.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:40:41,158 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_up_proj.safetensors2025-04-27 22:40:41,158 - INFO - exists: True2025-04-27 22:40:41,187 - INFO - factorize_layer_kron_svd2025-04-27 22:40:43,511 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:40:46,403 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:40:49,599 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:40:51,304 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:40:53,297 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:40:55,038 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:40:58,870 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:41:04,697 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:41:07,771 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:41:10,836 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:41:13,669 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:41:19,675 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_up_proj.safetensors TrueLayer: model.layers.12.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [5.9819213e-06 4.8709753e-06 4.7432140e-06 4.6781370e-06 4.6636528e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:43:38,003 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:43:38,003 - INFO - Replacing 'model.layers.12.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  40%|██████████████████████████████████████████████████████████████████                                                                                                   | 90/225 [27:33<1:14:24, 33.07s/it]2025-04-27 22:43:38,004 - INFO - Skipping layer model.layers.12.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:43:38,004 - INFO - Layer: model.layers.13.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:43:38,010 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_q_proj.safetensors2025-04-27 22:43:38,010 - INFO - exists: True2025-04-27 22:43:38,107 - INFO - factorize_layer_kron_svd2025-04-27 22:43:42,683 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:43:46,569 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:43:48,315 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:43:49,879 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:43:52,886 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:43:54,748 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_q_proj.safetensors TrueLayer: model.layers.13.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.9350363e-02 5.4489075e-05 1.5019226e-05 1.3564231e-05 1.1421780e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:44:50,211 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:44:50,212 - INFO - Replacing 'model.layers.13.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  41%|███████████████████████████████████████████████████████████████████▍                                                                                                 | 92/225 [28:45<1:14:35, 33.65s/it]2025-04-27 22:44:50,212 - INFO - Layer: model.layers.13.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:44:50,215 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_k_proj.safetensors2025-04-27 22:44:50,215 - INFO - exists: True2025-04-27 22:44:50,228 - INFO - factorize_layer_kron_svd2025-04-27 22:44:52,267 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:44:54,725 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:44:56,196 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:44:57,721 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:44:59,265 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:45:00,972 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:45:02,419 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:45:04,017 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:45:05,524 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:45:07,080 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:45:08,564 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:45:10,244 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_k_proj.safetensors TrueLayer: model.layers.13.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.3729892e-05 7.1575278e-06 7.1094173e-06 6.8093209e-06 6.5548347e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:46:16,311 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:46:16,311 - INFO - Replacing 'model.layers.13.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  41%|████████████████████████████████████████████████████████████████████▏                                                                                                | 93/225 [30:11<1:27:52, 39.94s/it]2025-04-27 22:46:16,312 - INFO - Layer: model.layers.13.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:46:16,313 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_v_proj.safetensors2025-04-27 22:46:16,313 - INFO - exists: True2025-04-27 22:46:16,322 - INFO - factorize_layer_kron_svd2025-04-27 22:46:18,064 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:46:20,082 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:46:21,612 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:46:23,249 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:46:25,296 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_v_proj.safetensors TrueLayer: model.layers.13.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [5.0983722e-03 5.0585440e-05 2.1063875e-05 1.7282286e-05 1.4230653e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:47:24,880 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:47:24,881 - INFO - Replacing 'model.layers.13.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)Compressing Layers:  42%|████████████████████████████████████████████████████████████████████▉                                                                                                | 94/225 [31:20<1:36:20, 44.13s/it]2025-04-27 22:47:24,881 - INFO - Layer: model.layers.13.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:47:24,885 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_o_proj.safetensors2025-04-27 22:47:24,885 - INFO - exists: True2025-04-27 22:47:24,892 - INFO - factorize_layer_kron_svd2025-04-27 22:47:27,736 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:47:30,196 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:47:31,973 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:47:34,649 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:47:36,548 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:47:38,399 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_o_proj.safetensors TrueLayer: model.layers.13.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.2446343e-02 3.8981681e-05 3.5099336e-05 2.8158551e-05 2.2302480e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:48:34,992 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:48:34,992 - INFO - Replacing 'model.layers.13.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)Compressing Layers:  42%|█████████████████████████████████████████████████████████████████████▋                                                                                               | 95/225 [32:30<1:45:20, 48.62s/it]2025-04-27 22:48:34,993 - INFO - Skipping layer model.layers.13.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:48:34,993 - INFO - Layer: model.layers.13.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:48:34,996 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_up_proj.safetensors2025-04-27 22:48:34,996 - INFO - exists: True2025-04-27 22:48:35,009 - INFO - factorize_layer_kron_svd2025-04-27 22:48:39,175 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:48:42,793 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:48:46,576 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:48:51,509 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:49:00,097 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:49:06,872 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_up_proj.safetensors TrueLayer: model.layers.13.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [6.4682798e-03 6.1749437e-05 2.4349400e-05 2.1919277e-05 2.0116417e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:51:00,084 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:51:00,084 - INFO - Replacing 'model.layers.13.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  43%|███████████████████████████████████████████████████████████████████████▏                                                                                             | 97/225 [34:55<2:00:35, 56.53s/it]2025-04-27 22:51:00,085 - INFO - Skipping layer model.layers.13.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:51:00,085 - INFO - Skipping layer model.layers.14.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:51:00,085 - INFO - Skipping layer model.layers.14.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:51:00,085 - INFO - Skipping layer model.layers.14.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:51:00,085 - INFO - Skipping layer model.layers.14.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:51:00,085 - INFO - Layer: model.layers.14.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:51:00,087 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_gate_proj.safetensors2025-04-27 22:51:00,087 - INFO - exists: True2025-04-27 22:51:00,115 - INFO - factorize_layer_kron_svd2025-04-27 22:51:02,145 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:51:03,923 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:51:05,836 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:51:08,509 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:51:18,293 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_gate_proj.safetensors TrueLayer: model.layers.14.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [3.2148813e-03 8.8560395e-05 2.4315630e-05 1.8305876e-05 1.6133905e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:53:04,280 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:53:04,281 - INFO - Replacing 'model.layers.14.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  46%|███████████████████████████████████████████████████████████████████████████                                                                                         | 103/225 [36:59<1:12:14, 35.52s/it]2025-04-27 22:53:04,281 - INFO - Layer: model.layers.14.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:53:04,285 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_up_proj.safetensors2025-04-27 22:53:04,285 - INFO - exists: True2025-04-27 22:53:04,365 - INFO - factorize_layer_kron_svd2025-04-27 22:53:08,379 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:53:10,429 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:53:13,869 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:53:18,425 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:53:23,548 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:53:32,997 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_up_proj.safetensors TrueLayer: model.layers.14.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [9.7202640e-03 2.5967920e-05 2.1275189e-05 1.6423986e-05 1.5875536e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:55:29,383 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:55:29,383 - INFO - Replacing 'model.layers.14.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  46%|███████████████████████████████████████████████████████████████████████████▊                                                                                        | 104/225 [39:24<1:38:42, 48.95s/it]2025-04-27 22:55:29,383 - INFO - Skipping layer model.layers.14.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:55:29,383 - INFO - Layer: model.layers.15.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:55:29,387 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_self_attn_q_proj.safetensors2025-04-27 22:55:29,387 - INFO - exists: True2025-04-27 22:55:29,464 - INFO - factorize_layer_kron_svd2025-04-27 22:55:31,646 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:55:35,208 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:55:39,209 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:55:40,891 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:55:42,805 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:55:46,671 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_self_attn_q_proj.safetensors TrueLayer: model.layers.15.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.19919665e-02 7.38329763e-05 2.44418788e-05 1.36780509e-05 1.00903217e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:56:46,965 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:56:46,966 - INFO - Replacing 'model.layers.15.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  47%|█████████████████████████████████████████████████████████████████████████████▎                                                                                      | 106/225 [40:42<1:31:51, 46.31s/it]2025-04-27 22:56:46,967 - INFO - Skipping layer model.layers.15.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:56:46,967 - INFO - Skipping layer model.layers.15.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:56:46,967 - INFO - Skipping layer model.layers.15.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:56:46,967 - INFO - Skipping layer model.layers.15.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:56:46,967 - INFO - Skipping layer model.layers.15.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:56:46,967 - INFO - Skipping layer model.layers.15.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:56:46,967 - INFO - Skipping layer model.layers.16.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:56:46,967 - INFO - Skipping layer model.layers.16.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:56:46,967 - INFO - Layer: model.layers.16.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:56:46,969 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_self_attn_v_proj.safetensors2025-04-27 22:56:46,970 - INFO - exists: True2025-04-27 22:56:46,979 - INFO - factorize_layer_kron_svd2025-04-27 22:56:50,160 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:56:52,503 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:56:55,264 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:56:58,660 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:57:00,460 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_self_attn_v_proj.safetensors TrueLayer: model.layers.16.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [2.1955712e-02 2.2352620e-05 1.8913308e-05 1.6113720e-05 1.5723988e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:57:37,013 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:57:37,014 - INFO - Replacing 'model.layers.16.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)Compressing Layers:  51%|████████████████████████████████████████████████████████████████████████████████████▊                                                                                 | 115/225 [41:32<38:12, 20.84s/it]2025-04-27 22:57:37,014 - INFO - Skipping layer model.layers.16.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:57:37,014 - INFO - Skipping layer model.layers.16.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:57:37,014 - INFO - Skipping layer model.layers.16.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:57:37,014 - INFO - Skipping layer model.layers.16.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:57:37,014 - INFO - Layer: model.layers.17.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:57:37,015 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_self_attn_q_proj.safetensors2025-04-27 22:57:37,015 - INFO - exists: True2025-04-27 22:57:37,030 - INFO - factorize_layer_kron_svd2025-04-27 22:57:39,003 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:57:40,739 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:57:43,132 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:57:45,811 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:57:49,588 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:57:53,511 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_self_attn_q_proj.safetensors TrueLayer: model.layers.17.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.9799747e-02 3.0997286e-05 1.8318075e-05 1.5734129e-05 9.7556685e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:58:49,009 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:58:49,010 - INFO - Replacing 'model.layers.17.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  53%|████████████████████████████████████████████████████████████████████████████████████████▌                                                                             | 120/225 [42:44<32:44, 18.71s/it]2025-04-27 22:58:49,010 - INFO - Skipping layer model.layers.17.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:49,010 - INFO - Skipping layer model.layers.17.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:49,011 - INFO - Skipping layer model.layers.17.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:49,011 - INFO - Skipping layer model.layers.17.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:49,011 - INFO - Skipping layer model.layers.17.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:49,011 - INFO - Skipping layer model.layers.17.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:58:49,011 - INFO - Layer: model.layers.18.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:58:49,012 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_q_proj.safetensors2025-04-27 22:58:49,012 - INFO - exists: True2025-04-27 22:58:49,021 - INFO - factorize_layer_kron_svd2025-04-27 22:58:50,801 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:58:52,645 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:58:55,244 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:58:57,511 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:58:59,733 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:59:01,477 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_q_proj.safetensors TrueLayer: model.layers.18.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.0405821e-03 8.4903695e-05 8.4368939e-06 7.6583383e-06 6.2006247e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:00:17,573 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:00:17,573 - INFO - Replacing 'model.layers.18.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  56%|█████████████████████████████████████████████████████████████████████████████████████████████▋                                                                        | 127/225 [44:12<26:36, 16.29s/it]2025-04-27 23:00:17,573 - INFO - Layer: model.layers.18.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:00:17,577 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_k_proj.safetensors2025-04-27 23:00:17,577 - INFO - exists: True2025-04-27 23:00:17,603 - INFO - factorize_layer_kron_svd2025-04-27 23:00:22,612 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:00:26,413 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:00:28,262 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:00:30,676 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:00:32,420 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:00:34,348 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_k_proj.safetensors TrueLayer: model.layers.18.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [7.7125463e-03 3.3378132e-05 8.2256274e-06 7.4304949e-06 6.1785499e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:01:34,180 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:01:34,180 - INFO - Replacing 'model.layers.18.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  57%|██████████████████████████████████████████████████████████████████████████████████████████████▍                                                                       | 128/225 [45:29<33:40, 20.83s/it]2025-04-27 23:01:34,180 - INFO - Skipping layer model.layers.18.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:01:34,180 - INFO - Skipping layer model.layers.18.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:01:34,180 - INFO - Skipping layer model.layers.18.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:01:34,180 - INFO - Layer: model.layers.18.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:01:34,184 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_mlp_up_proj.safetensors2025-04-27 23:01:34,184 - INFO - exists: True2025-04-27 23:01:34,193 - INFO - factorize_layer_kron_svd2025-04-27 23:01:38,307 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:01:41,903 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:01:45,794 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:01:51,108 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:02:01,132 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_mlp_up_proj.safetensors TrueLayer: model.layers.18.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [5.7937209e-03 2.0253658e-05 1.6411330e-05 1.4584949e-05 1.2499403e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:03:59,303 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:03:59,303 - INFO - Replacing 'model.layers.18.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  59%|█████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                    | 132/225 [47:54<39:29, 25.48s/it]2025-04-27 23:03:59,304 - INFO - Skipping layer model.layers.18.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:03:59,304 - INFO - Layer: model.layers.19.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:03:59,307 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_self_attn_q_proj.safetensors2025-04-27 23:03:59,307 - INFO - exists: True2025-04-27 23:03:59,383 - INFO - factorize_layer_kron_svd2025-04-27 23:04:03,067 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:04:07,181 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 23:04:10,399 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:04:14,280 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_self_attn_q_proj.safetensors TrueLayer: model.layers.19.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [2.2634035e-03 7.5883843e-05 1.0972199e-05 7.9327338e-06 7.6394726e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:05:19,805 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:05:19,805 - INFO - Replacing 'model.layers.19.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  60%|██████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                   | 134/225 [49:15<42:36, 28.09s/it]2025-04-27 23:05:19,805 - INFO - Skipping layer model.layers.19.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:05:19,806 - INFO - Skipping layer model.layers.19.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:05:19,807 - INFO - Skipping layer model.layers.19.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:05:19,807 - INFO - Skipping layer model.layers.19.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:05:19,807 - INFO - Layer: model.layers.19.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:05:19,811 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_mlp_up_proj.safetensors2025-04-27 23:05:19,811 - INFO - exists: True2025-04-27 23:05:19,820 - INFO - factorize_layer_kron_svd2025-04-27 23:05:22,697 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:05:24,931 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:05:26,825 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:05:29,519 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:05:35,379 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_mlp_up_proj.safetensors TrueLayer: model.layers.19.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [5.9256121e-03 1.7187345e-05 1.4408997e-05 1.2730728e-05 9.7351012e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:07:24,276 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:07:24,277 - INFO - Replacing 'model.layers.19.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  62%|██████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                               | 139/225 [51:19<38:29, 26.85s/it]2025-04-27 23:07:24,277 - INFO - Skipping layer model.layers.19.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:07:24,277 - INFO - Skipping layer model.layers.20.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:07:24,277 - INFO - Skipping layer model.layers.20.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:07:24,277 - INFO - Skipping layer model.layers.20.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:07:24,277 - INFO - Skipping layer model.layers.20.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:07:24,277 - INFO - Layer: model.layers.20.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:07:24,279 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_20_mlp_gate_proj.safetensors2025-04-27 23:07:24,279 - INFO - exists: True2025-04-27 23:07:24,304 - INFO - factorize_layer_kron_svd2025-04-27 23:07:28,211 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:07:31,884 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:07:35,769 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:07:41,373 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:07:48,698 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_20_mlp_gate_proj.safetensors TrueLayer: model.layers.20.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [5.6294589e-03 2.1218650e-05 1.5237835e-05 1.4524096e-05 1.3646030e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:09:42,877 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:09:42,878 - INFO - Replacing 'model.layers.20.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  64%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                           | 145/225 [53:38<33:48, 25.36s/it]2025-04-27 23:09:42,879 - INFO - Skipping layer model.layers.20.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:09:42,879 - INFO - Skipping layer model.layers.20.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:09:42,879 - INFO - Skipping layer model.layers.21.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:09:42,879 - INFO - Skipping layer model.layers.21.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:09:42,879 - INFO - Skipping layer model.layers.21.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:09:42,879 - INFO - Skipping layer model.layers.21.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:09:42,879 - INFO - Skipping layer model.layers.21.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:09:42,879 - INFO - Layer: model.layers.21.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:09:42,882 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_up_proj.safetensors2025-04-27 23:09:42,883 - INFO - exists: True2025-04-27 23:09:42,909 - INFO - factorize_layer_kron_svd2025-04-27 23:09:45,443 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:09:47,133 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:09:48,841 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:09:51,431 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:09:56,278 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_up_proj.safetensors TrueLayer: model.layers.21.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.8695735e-03 5.5963137e-05 1.4356574e-05 1.3274203e-05 1.2030486e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:11:43,574 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:11:43,575 - INFO - Replacing 'model.layers.21.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  68%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                     | 153/225 [55:38<25:06, 20.92s/it]2025-04-27 23:11:43,575 - INFO - Layer: model.layers.21.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:11:43,587 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_down_proj.safetensors2025-04-27 23:11:43,587 - INFO - exists: True2025-04-27 23:11:43,671 - INFO - factorize_layer_kron_svd2025-04-27 23:11:46,411 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:11:49,630 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:11:55,180 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:11:56,880 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:11:58,648 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:12:01,409 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_down_proj.safetensors TrueLayer: model.layers.21.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=11008, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x11008)...  SVD complete. Singular values (top 5): [4.5640193e-02 9.2487091e-05 5.4519722e-05 4.7700971e-05 3.9835424e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,11008), (4096,1496)factorized_sequential Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))2025-04-27 23:14:39,059 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))')2025-04-27 23:14:39,060 - INFO - Replacing 'model.layers.21.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)Compressing Layers:  68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                    | 154/225 [58:34<37:50, 31.99s/it]2025-04-27 23:14:39,060 - INFO - Skipping layer model.layers.22.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:14:39,060 - INFO - Skipping layer model.layers.22.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:14:39,061 - INFO - Skipping layer model.layers.22.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:14:39,061 - INFO - Skipping layer model.layers.22.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:14:39,061 - INFO - Skipping layer model.layers.22.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:14:39,061 - INFO - Layer: model.layers.22.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:14:39,062 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_up_proj.safetensors2025-04-27 23:14:39,062 - INFO - exists: True2025-04-27 23:14:39,085 - INFO - factorize_layer_kron_svd2025-04-27 23:14:41,024 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:14:42,841 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:14:45,605 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:14:48,531 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:14:56,913 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_up_proj.safetensors TrueLayer: model.layers.22.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.2615041e-03 2.1604352e-05 1.3284305e-05 1.1028132e-05 1.0561911e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:17:04,910 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:17:04,910 - INFO - Replacing 'model.layers.22.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                               | 160/225 [1:01:00<31:29, 29.07s/it]2025-04-27 23:17:04,911 - INFO - Layer: model.layers.22.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:17:04,912 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_down_proj.safetensors2025-04-27 23:17:04,912 - INFO - exists: True2025-04-27 23:17:04,939 - INFO - factorize_layer_kron_svd2025-04-27 23:17:07,808 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:17:12,105 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:17:23,107 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:17:25,749 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:17:27,892 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:17:31,777 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_down_proj.safetensors TrueLayer: model.layers.22.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=11008, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x11008)...  SVD complete. Singular values (top 5): [2.3855805e-02 8.0862206e-05 5.0205126e-05 4.2752916e-05 3.9248724e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,11008), (4096,1496)factorized_sequential Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))2025-04-27 23:19:26,563 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))')2025-04-27 23:19:26,563 - INFO - Replacing 'model.layers.22.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)Compressing Layers:  72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                              | 161/225 [1:03:21<40:58, 38.41s/it]2025-04-27 23:19:26,564 - INFO - Skipping layer model.layers.23.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:19:26,564 - INFO - Skipping layer model.layers.23.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:19:26,564 - INFO - Skipping layer model.layers.23.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:19:26,564 - INFO - Skipping layer model.layers.23.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:19:26,564 - INFO - Layer: model.layers.23.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:19:26,565 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_gate_proj.safetensors2025-04-27 23:19:26,565 - INFO - exists: True2025-04-27 23:19:26,586 - INFO - factorize_layer_kron_svd2025-04-27 23:19:28,561 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:19:30,309 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:19:32,134 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:19:35,676 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:19:46,563 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_gate_proj.safetensors TrueLayer: model.layers.23.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.6312244e-03 1.5538668e-05 4.1256385e-06 2.1622898e-06 2.0991954e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 23:21:43,711 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 23:21:43,712 - INFO - Replacing 'model.layers.23.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 166/225 [1:05:38<33:45, 34.32s/it]2025-04-27 23:21:43,712 - INFO - Skipping layer model.layers.23.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:21:43,712 - INFO - Layer: model.layers.23.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 23:21:43,713 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_down_proj.safetensors2025-04-27 23:21:43,713 - INFO - exists: True2025-04-27 23:21:43,741 - INFO - factorize_layer_kron_svd2025-04-27 23:21:46,546 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:21:50,476 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:21:58,545 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 23:22:00,315 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:22:02,104 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:22:04,628 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_down_proj.safetensors TrueLayer: model.layers.23.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=11008, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x11008)...  SVD complete. Singular values (top 5): [6.6739982e-03 2.3223182e-04 4.9300255e-05 3.7862912e-05 3.5104458e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,11008), (4096,1496)factorized_sequential Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))2025-04-27 23:24:27,206 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))')2025-04-27 23:24:27,207 - INFO - Replacing 'model.layers.23.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)Compressing Layers:  75%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                         | 168/225 [1:08:22<40:30, 42.64s/it]2025-04-27 23:24:27,207 - INFO - Skipping layer model.layers.24.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,207 - INFO - Skipping layer model.layers.24.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,207 - INFO - Skipping layer model.layers.24.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,207 - INFO - Skipping layer model.layers.24.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,207 - INFO - Skipping layer model.layers.24.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,207 - INFO - Skipping layer model.layers.24.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,207 - INFO - Skipping layer model.layers.24.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,207 - INFO - Skipping layer model.layers.25.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,207 - INFO - Skipping layer model.layers.25.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,207 - INFO - Skipping layer model.layers.25.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,208 - INFO - Skipping layer model.layers.25.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,208 - INFO - Skipping layer model.layers.25.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,208 - INFO - Skipping layer model.layers.25.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,208 - INFO - Skipping layer model.layers.25.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,208 - INFO - Skipping layer model.layers.26.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,208 - INFO - Skipping layer model.layers.26.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,208 - INFO - Skipping layer model.layers.26.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,208 - INFO - Skipping layer model.layers.26.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,208 - INFO - Skipping layer model.layers.26.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,208 - INFO - Skipping layer model.layers.26.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,208 - INFO - Skipping layer model.layers.26.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,208 - INFO - Skipping layer model.layers.27.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,208 - INFO - Skipping layer model.layers.27.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,208 - INFO - Skipping layer model.layers.27.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,208 - INFO - Skipping layer model.layers.27.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,208 - INFO - Skipping layer model.layers.27.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,209 - INFO - Skipping layer model.layers.27.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,209 - INFO - Skipping layer model.layers.27.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,209 - INFO - Skipping layer model.layers.28.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:24:27,209 - INFO - Layer: model.layers.28.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 23:24:27,209 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_28_self_attn_k_proj.safetensors2025-04-27 23:24:27,210 - INFO - exists: True2025-04-27 23:24:27,231 - INFO - factorize_layer_kron_svd2025-04-27 23:24:29,557 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:24:31,121 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:24:32,689 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:24:35,044 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:24:37,113 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:24:39,776 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 23:24:41,261 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 23:24:43,508 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 23:24:46,812 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 23:24:49,976 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 23:24:53,212 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 23:24:57,112 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_28_self_attn_k_proj.safetensors TrueLayer: model.layers.28.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.3403688e-05 6.0787293e-06 5.6532840e-06 5.4129196e-06 5.2258588e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 23:26:00,208 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 23:26:00,208 - INFO - Replacing 'model.layers.28.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                   | 198/225 [1:09:55<05:08, 11.41s/it]2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.28.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.28.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.28.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.28.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.28.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.29.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.29.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.29.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.29.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.29.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.29.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.29.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.30.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.30.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.30.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.30.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.30.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.30.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,209 - INFO - Skipping layer model.layers.30.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,210 - INFO - Skipping layer model.layers.31.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,210 - INFO - Skipping layer model.layers.31.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,210 - INFO - Skipping layer model.layers.31.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,210 - INFO - Skipping layer model.layers.31.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,210 - INFO - Skipping layer model.layers.31.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,210 - INFO - Skipping layer model.layers.31.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,210 - INFO - Skipping layer model.layers.31.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 23:26:00,210 - INFO - Skipping layer lm_head: Sensitivity ratio >= 1.0 (1.00). No compression.Compressing Layers: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [1:09:55<00:00, 18.65s/it]2025-04-27 23:26:00,211 - INFO - Compression finished. Processed: 43, Skipped (Ratio>=1 or No Sensitivity/Factors): 182, Failed: 02025-04-27 23:26:00,212 - INFO - Total parameters after compression: 60617891842025-04-27 23:26:00,213 - INFO - C rate : 0.89958671732960672025-04-27 23:26:00,213 - INFO - Saving compressed model to ./llama102025-04-27 23:26:00,213 - INFO - Compressed model and tokenizer saved.2025-04-27 23:26:00,223 - INFO - Evaluating on wikitext2Evaluating:   0%|                                                                                                                                                                                         | 0/21 [00:00<?, ?it/s]Evaluating:   5%|████████▍                                                                                                                                                                        | 1/21 [00:03<01:07,  3.39s/it]Evaluating:  10%|████████████████▊                                                                                                                                                                | 2/21 [00:04<00:38,  2.04s/it]Evaluating:  14%|█████████████████████████▎                                                                                                                                                       | 3/21 [00:05<00:29,  1.61s/it]Evaluating:  19%|█████████████████████████████████▋                                                                                                                                               | 4/21 [00:06<00:24,  1.41s/it]Evaluating:  24%|██████████████████████████████████████████▏                                                                                                                                      | 5/21 [00:07<00:20,  1.31s/it]Evaluating:  29%|██████████████████████████████████████████████████▌                                                                                                                              | 6/21 [00:08<00:18,  1.24s/it]Evaluating:  33%|███████████████████████████████████████████████████████████                                                                                                                      | 7/21 [00:10<00:16,  1.20s/it]Evaluating:  38%|███████████████████████████████████████████████████████████████████▍                                                                                                             | 8/21 [00:11<00:15,  1.17s/it]Evaluating:  43%|███████████████████████████████████████████████████████████████████████████▊                                                                                                     | 9/21 [00:12<00:13,  1.15s/it]Evaluating:  48%|███████████████████████████████████████████████████████████████████████████████████▊                                                                                            | 10/21 [00:13<00:12,  1.14s/it]Evaluating:  52%|████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                   | 11/21 [00:14<00:11,  1.13s/it]Evaluating:  57%|████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                           | 12/21 [00:15<00:10,  1.14s/it]Evaluating:  62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                   | 13/21 [00:16<00:09,  1.13s/it]Evaluating:  67%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                          | 14/21 [00:17<00:07,  1.12s/it]Evaluating:  71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                  | 15/21 [00:18<00:06,  1.12s/it]Evaluating:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                          | 16/21 [00:20<00:05,  1.12s/it]Evaluating:  81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                 | 17/21 [00:21<00:04,  1.11s/it]Evaluating:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                         | 18/21 [00:22<00:03,  1.13s/it]Evaluating:  90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                | 19/21 [00:23<00:02,  1.12s/it]Evaluating:  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 20/21 [00:24<00:01,  1.12s/it]Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:25<00:00,  1.05s/it]Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:25<00:00,  1.21s/it]2025-04-27 23:26:33,466 - INFO - wikitext2 perplexity: 8.75002025-04-27 23:26:33,467 - INFO - Evaluating on ptbnlls.shape torch.Size([16376])Mean NLL: 2.171875Evaluating:   0%|                                                                                                                                                                                          | 0/7 [00:00<?, ?it/s]Evaluating:  14%|█████████████████████████▍                                                                                                                                                        | 1/7 [00:01<00:06,  1.16s/it]Evaluating:  29%|██████████████████████████████████████████████████▊                                                                                                                               | 2/7 [00:02<00:05,  1.13s/it]Evaluating:  43%|████████████████████████████████████████████████████████████████████████████▎                                                                                                     | 3/7 [00:03<00:04,  1.12s/it]Evaluating:  57%|█████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                            | 4/7 [00:04<00:03,  1.12s/it]Evaluating:  71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                  | 5/7 [00:05<00:02,  1.11s/it]Evaluating:  86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 6/7 [00:06<00:01,  1.11s/it]Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:07<00:00,  1.09s/it]Evaluating: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:07<00:00,  1.11s/it]2025-04-27 23:26:44,167 - INFO - ptb perplexity: 36.25002025-04-27 23:26:44,167 - INFO - Evaluation results:2025-04-27 23:26:44,167 - INFO -   wikitext2: 8.75002025-04-27 23:26:44,167 - INFO -   ptb: 36.2500nlls.shape torch.Size([16376])Mean NLL: 3.59375
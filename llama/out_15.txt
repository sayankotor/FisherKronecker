2025-04-24 22:28:44,137 - INFO - Loading model: unsloth/llama-2-7b
[2025-04-24 22:28:49,674] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Loading checkpoint shards:   0%|                                                      | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███████████████▎                              | 1/3 [00:00<00:01,  1.29it/s]Loading checkpoint shards:  67%|██████████████████████████████▋               | 2/3 [00:01<00:00,  1.28it/s]Loading checkpoint shards: 100%|██████████████████████████████████████████████| 3/3 [00:02<00:00,  1.35it/s]Loading checkpoint shards: 100%|██████████████████████████████████████████████| 3/3 [00:02<00:00,  1.33it/s]
2025-04-24 22:28:53,984 - INFO - Model loaded with dtype torch.bfloat16
2025-04-24 22:29:31,687 - INFO - Model moved to cuda:0
2025-04-24 22:29:31,688 - INFO - Found 225 linear layers to potentially compress.
sensitivity_dict {'lm_head': 1, 'model.layers.31.mlp.gate_proj': 1, 'model.layers.31.mlp.up_proj': 1, 'model.layers.31.mlp.down_proj': 1, 'model.layers.31.self_attn.q_proj': 0.5, 'model.layers.31.self_attn.k_proj': 0.5, 'model.layers.31.self_attn.v_proj': 1, 'model.layers.31.self_attn.o_proj': 0.6, 'model.layers.30.mlp.gate_proj': 1, 'model.layers.30.mlp.up_proj': 1, 'model.layers.30.mlp.down_proj': 1, 'model.layers.30.self_attn.q_proj': 0.7, 'model.layers.30.self_attn.k_proj': 0.8, 'model.layers.30.self_attn.v_proj': 1, 'model.layers.30.self_attn.o_proj': 0.8, 'model.layers.29.mlp.gate_proj': 1, 'model.layers.29.mlp.up_proj': 1, 'model.layers.29.mlp.down_proj': 1, 'model.layers.29.self_attn.q_proj': 0.4, 'model.layers.29.self_attn.k_proj': 0.6, 'model.layers.29.self_attn.v_proj': 1, 'model.layers.29.self_attn.o_proj': 0.7, 'model.layers.28.mlp.gate_proj': 0.9, 'model.layers.28.mlp.up_proj': 1, 'model.layers.28.mlp.down_proj': 1, 'model.layers.28.self_attn.q_proj': 0.6, 'model.layers.28.self_attn.k_proj': 0.7, 'model.layers.28.self_attn.v_proj': 1, 'model.layers.28.self_attn.o_proj': 0.7, 'model.layers.27.mlp.gate_proj': 1, 'model.layers.27.mlp.up_proj': 1, 'model.layers.27.mlp.down_proj': 1, 'model.layers.27.self_attn.q_proj': 0.4, 'model.layers.27.self_attn.k_proj': 0.3, 'model.layers.27.self_attn.v_proj': 0.9, 'model.layers.27.self_attn.o_proj': 0.8, 'model.layers.26.mlp.gate_proj': 1, 'model.layers.26.mlp.up_proj': 1, 'model.layers.26.mlp.down_proj': 1, 'model.layers.26.self_attn.q_proj': 0.5, 'model.layers.26.self_attn.k_proj': 0.4, 'model.layers.26.self_attn.v_proj': 1, 'model.layers.26.self_attn.o_proj': 0.6, 'model.layers.25.mlp.gate_proj': 0.9, 'model.layers.25.mlp.up_proj': 0.9, 'model.layers.25.mlp.down_proj': 1, 'model.layers.25.self_attn.q_proj': 0.8, 'model.layers.25.self_attn.k_proj': 1, 'model.layers.25.self_attn.v_proj': 1, 'model.layers.25.self_attn.o_proj': 0.7, 'model.layers.24.mlp.gate_proj': 0.7, 'model.layers.24.mlp.up_proj': 0.7, 'model.layers.24.mlp.down_proj': 0.9, 'model.layers.24.self_attn.q_proj': 0.1, 'model.layers.24.self_attn.k_proj': 0.1, 'model.layers.24.self_attn.v_proj': 0.7, 'model.layers.24.self_attn.o_proj': 0.4, 'model.layers.23.mlp.gate_proj': 1, 'model.layers.23.mlp.up_proj': 0.9, 'model.layers.23.mlp.down_proj': 1, 'model.layers.23.self_attn.q_proj': 0.5, 'model.layers.23.self_attn.k_proj': 0.5, 'model.layers.23.self_attn.v_proj': 0.8, 'model.layers.23.self_attn.o_proj': 0.8, 'model.layers.22.mlp.gate_proj': 1, 'model.layers.22.mlp.up_proj': 1, 'model.layers.22.mlp.down_proj': 1, 'model.layers.22.self_attn.q_proj': 0.7, 'model.layers.22.self_attn.k_proj': 0.6, 'model.layers.22.self_attn.v_proj': 1, 'model.layers.22.self_attn.o_proj': 0.8, 'model.layers.21.mlp.gate_proj': 1, 'model.layers.21.mlp.up_proj': 1, 'model.layers.21.mlp.down_proj': 1, 'model.layers.21.self_attn.q_proj': 0.6, 'model.layers.21.self_attn.k_proj': 0.5, 'model.layers.21.self_attn.v_proj': 0.7, 'model.layers.21.self_attn.o_proj': 0.9, 'model.layers.20.mlp.gate_proj': 1, 'model.layers.20.mlp.up_proj': 1, 'model.layers.20.mlp.down_proj': 1, 'model.layers.20.self_attn.q_proj': 0.2, 'model.layers.20.self_attn.k_proj': 0.1, 'model.layers.20.self_attn.v_proj': 0.8, 'model.layers.20.self_attn.o_proj': 0.7, 'model.layers.19.mlp.gate_proj': 1, 'model.layers.19.mlp.up_proj': 1, 'model.layers.19.mlp.down_proj': 1, 'model.layers.19.self_attn.q_proj': 0.7, 'model.layers.19.self_attn.k_proj': 0.6, 'model.layers.19.self_attn.v_proj': 1, 'model.layers.19.self_attn.o_proj': 0.9, 'model.layers.18.mlp.gate_proj': 1, 'model.layers.18.mlp.up_proj': 1, 'model.layers.18.mlp.down_proj': 1, 'model.layers.18.self_attn.q_proj': 0.5, 'model.layers.18.self_attn.k_proj': 0.7, 'model.layers.18.self_attn.v_proj': 1, 'model.layers.18.self_attn.o_proj': 0.8, 'model.layers.17.mlp.gate_proj': 1, 'model.layers.17.mlp.up_proj': 1, 'model.layers.17.mlp.down_proj': 1, 'model.layers.17.self_attn.q_proj': 0.6, 'model.layers.17.self_attn.k_proj': 0.5, 'model.layers.17.self_attn.v_proj': 1, 'model.layers.17.self_attn.o_proj': 1, 'model.layers.16.mlp.gate_proj': 1, 'model.layers.16.mlp.up_proj': 1, 'model.layers.16.mlp.down_proj': 1, 'model.layers.16.self_attn.q_proj': 0.4, 'model.layers.16.self_attn.k_proj': 0.3, 'model.layers.16.self_attn.v_proj': 1, 'model.layers.16.self_attn.o_proj': 0.9, 'model.layers.15.mlp.gate_proj': 1, 'model.layers.15.mlp.up_proj': 1, 'model.layers.15.mlp.down_proj': 1, 'model.layers.15.self_attn.q_proj': 0.5, 'model.layers.15.self_attn.k_proj': 0.7, 'model.layers.15.self_attn.v_proj': 1, 'model.layers.15.self_attn.o_proj': 1, 'model.layers.14.mlp.gate_proj': 1, 'model.layers.14.mlp.up_proj': 1, 'model.layers.14.mlp.down_proj': 1, 'model.layers.14.self_attn.q_proj': 0.5, 'model.layers.14.self_attn.k_proj': 0.6, 'model.layers.14.self_attn.v_proj': 1, 'model.layers.14.self_attn.o_proj': 1, 'model.layers.13.mlp.gate_proj': 1, 'model.layers.13.mlp.up_proj': 1, 'model.layers.13.mlp.down_proj': 1, 'model.layers.13.self_attn.q_proj': 0.6, 'model.layers.13.self_attn.k_proj': 0.7, 'model.layers.13.self_attn.v_proj': 1, 'model.layers.13.self_attn.o_proj': 0.8, 'model.layers.12.mlp.gate_proj': 0.5, 'model.layers.12.mlp.up_proj': 0.6, 'model.layers.12.mlp.down_proj': 0.9, 'model.layers.12.self_attn.q_proj': 0.3, 'model.layers.12.self_attn.k_proj': 0.5, 'model.layers.12.self_attn.v_proj': 1, 'model.layers.12.self_attn.o_proj': 0.7, 'model.layers.11.mlp.gate_proj': 1, 'model.layers.11.mlp.up_proj': 1, 'model.layers.11.mlp.down_proj': 1, 'model.layers.11.self_attn.q_proj': 0.2, 'model.layers.11.self_attn.k_proj': 0.2, 'model.layers.11.self_attn.v_proj': 1, 'model.layers.11.self_attn.o_proj': 0.8, 'model.layers.10.mlp.gate_proj': 1, 'model.layers.10.mlp.up_proj': 1, 'model.layers.10.mlp.down_proj': 1, 'model.layers.10.self_attn.q_proj': 0.5, 'model.layers.10.self_attn.k_proj': 0.6, 'model.layers.10.self_attn.v_proj': 1, 'model.layers.10.self_attn.o_proj': 1, 'model.layers.9.mlp.gate_proj': 1, 'model.layers.9.mlp.up_proj': 1, 'model.layers.9.mlp.down_proj': 1, 'model.layers.9.self_attn.q_proj': 0.5, 'model.layers.9.self_attn.k_proj': 0.5, 'model.layers.9.self_attn.v_proj': 1, 'model.layers.9.self_attn.o_proj': 0.9, 'model.layers.8.mlp.gate_proj': 1, 'model.layers.8.mlp.up_proj': 1, 'model.layers.8.mlp.down_proj': 1, 'model.layers.8.self_attn.q_proj': 0.5, 'model.layers.8.self_attn.k_proj': 0.5, 'model.layers.8.self_attn.v_proj': 1, 'model.layers.8.self_attn.o_proj': 0.7, 'model.layers.7.mlp.gate_proj': 0.9, 'model.layers.7.mlp.up_proj': 1, 'model.layers.7.mlp.down_proj': 1, 'model.layers.7.self_attn.q_proj': 0.5, 'model.layers.7.self_attn.k_proj': 0.4, 'model.layers.7.self_attn.v_proj': 1, 'model.layers.7.self_attn.o_proj': 0.7, 'model.layers.6.mlp.gate_proj': 1, 'model.layers.6.mlp.up_proj': 1, 'model.layers.6.mlp.down_proj': 1, 'model.layers.6.self_attn.q_proj': 0.3, 'model.layers.6.self_attn.k_proj': 0.3, 'model.layers.6.self_attn.v_proj': 1, 'model.layers.6.self_attn.o_proj': 0.8, 'model.layers.5.mlp.gate_proj': 1, 'model.layers.5.mlp.up_proj': 1, 'model.layers.5.mlp.down_proj': 1, 'model.layers.5.self_attn.q_proj': 0.2, 'model.layers.5.self_attn.k_proj': 0.7, 'model.layers.5.self_attn.v_proj': 1, 'model.layers.5.self_attn.o_proj': 0.5, 'model.layers.4.mlp.gate_proj': 0.9, 'model.layers.4.mlp.up_proj': 1, 'model.layers.4.mlp.down_proj': 0.9, 'model.layers.4.self_attn.q_proj': 0.4, 'model.layers.4.self_attn.k_proj': 0.3, 'model.layers.4.self_attn.v_proj': 1, 'model.layers.4.self_attn.o_proj': 0.7, 'model.layers.3.mlp.gate_proj': 0.9, 'model.layers.3.mlp.up_proj': 1, 'model.layers.3.mlp.down_proj': 0.9, 'model.layers.3.self_attn.q_proj': 0.4, 'model.layers.3.self_attn.k_proj': 0.2, 'model.layers.3.self_attn.v_proj': 1, 'model.layers.3.self_attn.o_proj': 0.5, 'model.layers.2.mlp.gate_proj': 0.9, 'model.layers.2.mlp.up_proj': 1, 'model.layers.2.mlp.down_proj': 1, 'model.layers.2.self_attn.q_proj': 0.5, 'model.layers.2.self_attn.k_proj': 0.4, 'model.layers.2.self_attn.v_proj': 0.9, 'model.layers.2.self_attn.o_proj': 0.8, 'model.layers.1.mlp.gate_proj': 0.6, 'model.layers.1.mlp.up_proj': 0.7, 'model.layers.1.mlp.down_proj': 1, 'model.layers.1.self_attn.q_proj': 0.1, 'model.layers.1.self_attn.k_proj': 0.1, 'model.layers.1.self_attn.v_proj': 0.5, 'model.layers.1.self_attn.o_proj': 0.4, 'model.layers.0.mlp.gate_proj': 0.4, 'model.layers.0.mlp.up_proj': 0.4, 'model.layers.0.mlp.down_proj': 0.6, 'model.layers.0.self_attn.q_proj': 0.1, 'model.layers.0.self_attn.k_proj': 0.1, 'model.layers.0.self_attn.v_proj': 0.3, 'model.layers.0.self_attn.o_proj': 0.1}
Compressing Layers:   0%|                                                           | 0/225 [00:00<?, ?it/s]2025-04-24 22:29:31,689 - INFO - Layer: model.layers.0.self_attn.q_proj | Ratio: 0.100 -> Target Rank: 208 (Align: 8)
2025-04-24 22:29:31,690 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_0_self_attn_q_proj.safetensors
2025-04-24 22:29:31,690 - INFO - exists: True
2025-04-24 22:29:31,692 - INFO - factorize_layer_kron_svd
2025-04-24 22:29:33,725 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:29:37,896 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:29:39,912 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:29:45,477 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_0_self_attn_q_proj.safetensors True
Layer: model.layers.0.self_attn.q_proj | Ratio: 0.100 -> Target Rank: 208 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [2.2687174e-04 1.1153770e-04 8.2521896e-05 7.4318777e-05 5.9026988e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (208,4096), (4096,208)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=208, bias=False)
  (1): Linear(in_features=208, out_features=4096, bias=False)
)
2025-04-24 22:30:13,462 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=208, bias=False)
  (1): Linear(in_features=208, out_features=4096, bias=False)
)')
2025-04-24 22:30:13,463 - INFO - Replacing 'model.layers.0.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:   0%|▏                                                | 1/225 [00:41<2:35:57, 41.77s/it]2025-04-24 22:30:13,463 - INFO - Layer: model.layers.0.self_attn.k_proj | Ratio: 0.100 -> Target Rank: 208 (Align: 8)
2025-04-24 22:30:13,464 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_0_self_attn_k_proj.safetensors
2025-04-24 22:30:13,464 - INFO - exists: True
2025-04-24 22:30:13,471 - INFO - factorize_layer_kron_svd
2025-04-24 22:30:14,719 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:30:16,049 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:30:17,143 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:30:18,483 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_0_self_attn_k_proj.safetensors True
Layer: model.layers.0.self_attn.k_proj | Ratio: 0.100 -> Target Rank: 208 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [4.0663555e-04 1.7498253e-04 9.6088203e-05 8.4267405e-05 5.7348043e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (208,4096), (4096,208)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=208, bias=False)
  (1): Linear(in_features=208, out_features=4096, bias=False)
)
2025-04-24 22:31:03,263 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=208, bias=False)
  (1): Linear(in_features=208, out_features=4096, bias=False)
)')
2025-04-24 22:31:03,263 - INFO - Replacing 'model.layers.0.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:   1%|▍                                                | 2/225 [01:31<2:52:48, 46.50s/it]2025-04-24 22:31:03,264 - INFO - Layer: model.layers.0.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-24 22:31:03,265 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_0_self_attn_v_proj.safetensors
2025-04-24 22:31:03,265 - INFO - exists: True
2025-04-24 22:31:03,327 - INFO - factorize_layer_kron_svd
2025-04-24 22:31:04,532 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:31:05,855 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:31:06,965 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:31:08,311 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_0_self_attn_v_proj.safetensors True
Layer: model.layers.0.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [9.5722213e-04 1.7444632e-04 6.2943713e-05 2.4491763e-05 1.8496392e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-24 22:32:01,072 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-24 22:32:01,072 - INFO - Replacing 'model.layers.0.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)
Compressing Layers:   1%|▋                                                | 3/225 [02:29<3:11:08, 51.66s/it]2025-04-24 22:32:01,072 - INFO - Layer: model.layers.0.self_attn.o_proj | Ratio: 0.100 -> Target Rank: 208 (Align: 8)
2025-04-24 22:32:01,073 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_0_self_attn_o_proj.safetensors
2025-04-24 22:32:01,073 - INFO - exists: True
2025-04-24 22:32:01,080 - INFO - factorize_layer_kron_svd
2025-04-24 22:32:04,910 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:32:07,329 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:32:08,470 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:32:10,638 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_0_self_attn_o_proj.safetensors True
Layer: model.layers.0.self_attn.o_proj | Ratio: 0.100 -> Target Rank: 208 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [5.2752346e-04 1.1654778e-04 7.8801531e-05 2.0858788e-05 1.5365338e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (208,4096), (4096,208)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=208, bias=False)
  (1): Linear(in_features=208, out_features=4096, bias=False)
)
2025-04-24 22:32:43,540 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=208, bias=False)
  (1): Linear(in_features=208, out_features=4096, bias=False)
)')
2025-04-24 22:32:43,540 - INFO - Replacing 'model.layers.0.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:   2%|▊                                                | 4/225 [03:11<2:56:55, 48.03s/it]2025-04-24 22:32:43,541 - INFO - Layer: model.layers.0.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-24 22:32:43,542 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_0_mlp_gate_proj.safetensors
2025-04-24 22:32:43,542 - INFO - exists: True
2025-04-24 22:32:43,554 - INFO - factorize_layer_kron_svd
2025-04-24 22:32:44,938 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:32:46,190 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 22:32:47,504 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-24 22:32:49,642 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:32:53,231 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_0_mlp_gate_proj.safetensors True
Layer: model.layers.0.mlp.gate_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.62766388e-04 8.60974760e-05 3.33992612e-05 1.43659045e-05
 1.18921280e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-24 22:34:15,998 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-24 22:34:15,999 - INFO - Replacing 'model.layers.0.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:   2%|█                                                | 5/225 [04:44<3:54:51, 64.05s/it]2025-04-24 22:34:16,000 - INFO - Layer: model.layers.0.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
2025-04-24 22:34:16,001 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_0_mlp_up_proj.safetensors
2025-04-24 22:34:16,001 - INFO - exists: True
2025-04-24 22:34:16,032 - INFO - factorize_layer_kron_svd
2025-04-24 22:34:17,767 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:34:19,022 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 22:34:20,317 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-24 22:34:22,913 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:34:26,486 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_0_mlp_up_proj.safetensors True
Layer: model.layers.0.mlp.up_proj | Ratio: 0.400 -> Target Rank: 1200 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [2.1327082e-04 7.0809328e-05 5.0785864e-05 3.5060402e-05 2.8840335e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1200,4096), (11008,1200)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)
2025-04-24 22:35:26,905 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1200, bias=False)
  (1): Linear(in_features=1200, out_features=11008, bias=False)
)')
2025-04-24 22:35:26,905 - INFO - Replacing 'model.layers.0.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:   3%|█▎                                               | 6/225 [05:55<4:02:17, 66.38s/it]2025-04-24 22:35:26,906 - INFO - Layer: model.layers.0.mlp.down_proj | Ratio: 0.600 -> Target Rank: 1792 (Align: 8)
2025-04-24 22:35:26,907 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_0_mlp_down_proj.safetensors
2025-04-24 22:35:26,907 - INFO - exists: True
2025-04-24 22:35:26,929 - INFO - factorize_layer_kron_svd
2025-04-24 22:35:29,473 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:35:33,145 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 22:35:36,874 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-24 22:35:38,070 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:35:39,362 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_0_mlp_down_proj.safetensors True
Layer: model.layers.0.mlp.down_proj | Ratio: 0.600 -> Target Rank: 1792 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [0.00360306 0.00047922 0.00020495 0.00012077 0.00011648]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1792,11008), (4096,1792)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=1792, bias=False)
  (1): Linear(in_features=1792, out_features=4096, bias=False)
)
2025-04-24 22:36:39,222 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=1792, bias=False)
  (1): Linear(in_features=1792, out_features=4096, bias=False)
)')
2025-04-24 22:36:39,223 - INFO - Replacing 'model.layers.0.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:   3%|█▌                                               | 7/225 [07:07<4:08:14, 68.32s/it]2025-04-24 22:36:39,223 - INFO - Layer: model.layers.1.self_attn.q_proj | Ratio: 0.100 -> Target Rank: 208 (Align: 8)
2025-04-24 22:36:39,224 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_1_self_attn_q_proj.safetensors
2025-04-24 22:36:39,224 - INFO - exists: True
2025-04-24 22:36:39,247 - INFO - factorize_layer_kron_svd
2025-04-24 22:36:40,479 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:36:41,734 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:36:42,831 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:36:44,078 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_1_self_attn_q_proj.safetensors True
Layer: model.layers.1.self_attn.q_proj | Ratio: 0.100 -> Target Rank: 208 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [8.43146583e-04 1.54433030e-04 1.13500086e-04 9.13513068e-05
 6.67410131e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (208,4096), (4096,208)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=208, bias=False)
  (1): Linear(in_features=208, out_features=4096, bias=False)
)
2025-04-24 22:37:06,831 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=208, bias=False)
  (1): Linear(in_features=208, out_features=4096, bias=False)
)')
2025-04-24 22:37:06,831 - INFO - Replacing 'model.layers.1.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:   4%|█▋                                               | 8/225 [07:35<3:20:13, 55.36s/it]2025-04-24 22:37:06,831 - INFO - Layer: model.layers.1.self_attn.k_proj | Ratio: 0.100 -> Target Rank: 208 (Align: 8)
2025-04-24 22:37:06,832 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_1_self_attn_k_proj.safetensors
2025-04-24 22:37:06,832 - INFO - exists: True
2025-04-24 22:37:06,838 - INFO - factorize_layer_kron_svd
2025-04-24 22:37:08,087 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:37:09,313 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:37:10,375 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:37:11,589 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_1_self_attn_k_proj.safetensors True
Layer: model.layers.1.self_attn.k_proj | Ratio: 0.100 -> Target Rank: 208 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [7.0266426e-04 1.2620199e-04 8.1441554e-05 8.0446305e-05 6.2994521e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (208,4096), (4096,208)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=208, bias=False)
  (1): Linear(in_features=208, out_features=4096, bias=False)
)
2025-04-24 22:37:39,300 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=208, bias=False)
  (1): Linear(in_features=208, out_features=4096, bias=False)
)')
2025-04-24 22:37:39,301 - INFO - Replacing 'model.layers.1.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:   4%|█▉                                               | 9/225 [08:07<2:53:32, 48.21s/it]2025-04-24 22:37:39,301 - INFO - Layer: model.layers.1.self_attn.v_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 22:37:39,302 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_1_self_attn_v_proj.safetensors
2025-04-24 22:37:39,302 - INFO - exists: True
2025-04-24 22:37:39,308 - INFO - factorize_layer_kron_svd
2025-04-24 22:37:40,502 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:37:41,763 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:37:42,837 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:37:44,058 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 22:37:45,319 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_1_self_attn_v_proj.safetensors True
Layer: model.layers.1.self_attn.v_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.3037748e-03 7.5241107e-05 4.6876405e-05 2.5140183e-05 2.1678547e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)
2025-04-24 22:38:13,219 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)')
2025-04-24 22:38:13,220 - INFO - Replacing 'model.layers.1.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)
Compressing Layers:   4%|██▏                                             | 10/225 [08:41<2:36:55, 43.79s/it]2025-04-24 22:38:13,220 - INFO - Layer: model.layers.1.self_attn.o_proj | Ratio: 0.400 -> Target Rank: 824 (Align: 8)
2025-04-24 22:38:13,221 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_1_self_attn_o_proj.safetensors
2025-04-24 22:38:13,221 - INFO - exists: True
2025-04-24 22:38:13,229 - INFO - factorize_layer_kron_svd
2025-04-24 22:38:14,360 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:38:15,403 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 22:38:16,459 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 22:38:17,501 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 22:38:18,592 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 22:38:19,864 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-24 22:38:20,918 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:38:21,960 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 22:38:23,015 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 22:38:24,068 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 22:38:25,121 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 22:38:26,333 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_1_self_attn_o_proj.safetensors True
Layer: model.layers.1.self_attn.o_proj | Ratio: 0.400 -> Target Rank: 824 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [3.8689141e-06 3.1313011e-06 2.2400243e-06 2.0631737e-06 2.0335660e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (824,4096), (4096,824)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=824, bias=False)
  (1): Linear(in_features=824, out_features=4096, bias=False)
)
2025-04-24 22:38:54,153 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=824, bias=False)
  (1): Linear(in_features=824, out_features=4096, bias=False)
)')
2025-04-24 22:38:54,153 - INFO - Replacing 'model.layers.1.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:   5%|██▎                                             | 11/225 [09:22<2:33:04, 42.92s/it]2025-04-24 22:38:54,153 - INFO - Layer: model.layers.1.mlp.gate_proj | Ratio: 0.600 -> Target Rank: 1792 (Align: 8)
2025-04-24 22:38:54,154 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_1_mlp_gate_proj.safetensors
2025-04-24 22:38:54,154 - INFO - exists: True
2025-04-24 22:38:54,160 - INFO - factorize_layer_kron_svd
2025-04-24 22:38:55,530 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:38:56,761 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 22:38:58,028 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-24 22:39:01,598 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_1_mlp_gate_proj.safetensors True
Layer: model.layers.1.mlp.gate_proj | Ratio: 0.600 -> Target Rank: 1792 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [2.5756116e-04 2.0590835e-05 1.7721311e-05 1.4897599e-05 1.3294678e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1792,4096), (11008,1792)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1792, bias=False)
  (1): Linear(in_features=1792, out_features=11008, bias=False)
)
2025-04-24 22:40:04,731 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1792, bias=False)
  (1): Linear(in_features=1792, out_features=11008, bias=False)
)')
2025-04-24 22:40:04,732 - INFO - Replacing 'model.layers.1.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:   5%|██▌                                             | 12/225 [10:33<3:02:14, 51.33s/it]2025-04-24 22:40:04,733 - INFO - Layer: model.layers.1.mlp.up_proj | Ratio: 0.700 -> Target Rank: 2096 (Align: 8)
2025-04-24 22:40:04,734 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_1_mlp_up_proj.safetensors
2025-04-24 22:40:04,734 - INFO - exists: True
2025-04-24 22:40:04,750 - INFO - factorize_layer_kron_svd
2025-04-24 22:40:05,966 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:40:07,011 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 22:40:08,055 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 22:40:09,103 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 22:40:10,148 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 22:40:11,336 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-24 22:40:13,077 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:40:15,045 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 22:40:16,958 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 22:40:19,106 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 22:40:21,491 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 22:40:26,490 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_1_mlp_up_proj.safetensors True
Layer: model.layers.1.mlp.up_proj | Ratio: 0.700 -> Target Rank: 2096 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [7.3754545e-06 4.6838541e-06 4.6829132e-06 4.5144598e-06 4.4679359e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2096,4096), (11008,2096)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2096, bias=False)
  (1): Linear(in_features=2096, out_features=11008, bias=False)
)
2025-04-24 22:41:44,303 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2096, bias=False)
  (1): Linear(in_features=2096, out_features=11008, bias=False)
)')
2025-04-24 22:41:44,303 - INFO - Replacing 'model.layers.1.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:   6%|██▊                                             | 13/225 [12:12<3:53:00, 65.95s/it]2025-04-24 22:41:44,304 - INFO - Skipping layer model.layers.1.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:41:44,304 - INFO - Layer: model.layers.2.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 22:41:44,305 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_2_self_attn_q_proj.safetensors
2025-04-24 22:41:44,306 - INFO - exists: True
2025-04-24 22:41:44,335 - INFO - factorize_layer_kron_svd
2025-04-24 22:41:46,046 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:41:47,866 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:41:49,052 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:41:50,550 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_2_self_attn_q_proj.safetensors True
Layer: model.layers.2.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.73210399e-03 1.81858835e-04 1.07483946e-04 9.33050178e-05
 6.15373719e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)
2025-04-24 22:42:20,384 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)')
2025-04-24 22:42:20,385 - INFO - Replacing 'model.layers.2.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:   7%|███▏                                            | 15/225 [12:48<2:33:01, 43.72s/it]2025-04-24 22:42:20,385 - INFO - Layer: model.layers.2.self_attn.k_proj | Ratio: 0.400 -> Target Rank: 824 (Align: 8)
2025-04-24 22:42:20,386 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_2_self_attn_k_proj.safetensors
2025-04-24 22:42:20,386 - INFO - exists: True
2025-04-24 22:42:20,392 - INFO - factorize_layer_kron_svd
2025-04-24 22:42:21,661 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:42:23,244 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:42:24,452 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:42:26,037 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_2_self_attn_k_proj.safetensors True
Layer: model.layers.2.self_attn.k_proj | Ratio: 0.400 -> Target Rank: 824 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.9114357e-03 1.4245549e-04 1.0586168e-04 7.6874050e-05 6.1492065e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (824,4096), (4096,824)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=824, bias=False)
  (1): Linear(in_features=824, out_features=4096, bias=False)
)
2025-04-24 22:43:03,937 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=824, bias=False)
  (1): Linear(in_features=824, out_features=4096, bias=False)
)')
2025-04-24 22:43:03,937 - INFO - Replacing 'model.layers.2.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:   7%|███▍                                            | 16/225 [13:32<2:32:08, 43.68s/it]2025-04-24 22:43:03,938 - INFO - Layer: model.layers.2.self_attn.v_proj | Ratio: 0.900 -> Target Rank: 1848 (Align: 8)
2025-04-24 22:43:03,939 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_2_self_attn_v_proj.safetensors
2025-04-24 22:43:03,939 - INFO - exists: True
2025-04-24 22:43:03,946 - INFO - factorize_layer_kron_svd
2025-04-24 22:43:05,322 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:43:06,855 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:43:07,999 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:43:09,243 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_2_self_attn_v_proj.safetensors True
Layer: model.layers.2.self_attn.v_proj | Ratio: 0.900 -> Target Rank: 1848 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.9212312e-04 4.9278322e-05 1.8895498e-05 1.5295736e-05 1.3105318e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1848,4096), (4096,1848)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1848, bias=False)
  (1): Linear(in_features=1848, out_features=4096, bias=False)
)
2025-04-24 22:43:52,382 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1848, bias=False)
  (1): Linear(in_features=1848, out_features=4096, bias=False)
)')
2025-04-24 22:43:52,383 - INFO - Replacing 'model.layers.2.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)
Compressing Layers:   8%|███▋                                            | 17/225 [14:20<2:35:45, 44.93s/it]2025-04-24 22:43:52,383 - INFO - Layer: model.layers.2.self_attn.o_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
2025-04-24 22:43:52,387 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_2_self_attn_o_proj.safetensors
2025-04-24 22:43:52,387 - INFO - exists: True
2025-04-24 22:43:52,397 - INFO - factorize_layer_kron_svd
2025-04-24 22:43:53,555 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:43:54,680 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 22:43:55,815 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 22:43:56,879 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 22:43:58,021 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 22:43:59,256 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-24 22:44:00,325 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:44:01,431 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 22:44:02,499 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 22:44:03,617 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 22:44:04,701 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 22:44:06,153 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_2_self_attn_o_proj.safetensors True
Layer: model.layers.2.self_attn.o_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [6.0198013e-06 2.9007442e-06 2.8069910e-06 2.7654496e-06 2.7077651e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1640,4096), (4096,1640)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)
2025-04-24 22:44:41,550 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)')
2025-04-24 22:44:41,550 - INFO - Replacing 'model.layers.2.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:   8%|███▊                                            | 18/225 [15:09<2:38:59, 46.08s/it]2025-04-24 22:44:41,550 - INFO - Layer: model.layers.2.mlp.gate_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
2025-04-24 22:44:41,551 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_2_mlp_gate_proj.safetensors
2025-04-24 22:44:41,551 - INFO - exists: True
2025-04-24 22:44:41,559 - INFO - factorize_layer_kron_svd
2025-04-24 22:44:42,825 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:44:43,977 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 22:44:45,102 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 22:44:46,218 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 22:44:47,341 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 22:44:49,129 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-24 22:44:50,883 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:44:52,897 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 22:44:54,850 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 22:44:56,840 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 22:45:00,466 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 22:45:07,348 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_2_mlp_gate_proj.safetensors True
Layer: model.layers.2.mlp.gate_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.0618421e-05 7.9699194e-06 7.3974884e-06 7.0551118e-06 6.9922344e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2688,4096), (11008,2688)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2688, bias=False)
  (1): Linear(in_features=2688, out_features=11008, bias=False)
)
2025-04-24 22:46:13,864 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2688, bias=False)
  (1): Linear(in_features=2688, out_features=11008, bias=False)
)')
2025-04-24 22:46:13,865 - INFO - Replacing 'model.layers.2.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:   8%|████                                            | 19/225 [16:42<3:22:42, 59.04s/it]2025-04-24 22:46:13,865 - INFO - Skipping layer model.layers.2.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,865 - INFO - Skipping layer model.layers.2.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,865 - INFO - Layer: model.layers.3.self_attn.q_proj | Ratio: 0.400 -> Target Rank: 824 (Align: 8)
2025-04-24 22:46:13,866 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_3_self_attn_q_proj.safetensors
2025-04-24 22:46:13,867 - INFO - exists: False
2025-04-24 22:46:13,867 - WARNING - Skipping layer model.layers.3.self_attn.q_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_3_self_attn_q_proj.safetensors
2025-04-24 22:46:13,867 - INFO - Layer: model.layers.3.self_attn.k_proj | Ratio: 0.200 -> Target Rank: 416 (Align: 8)
2025-04-24 22:46:13,867 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_3_self_attn_k_proj.safetensors
2025-04-24 22:46:13,867 - INFO - exists: False
2025-04-24 22:46:13,867 - WARNING - Skipping layer model.layers.3.self_attn.k_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_3_self_attn_k_proj.safetensors
2025-04-24 22:46:13,867 - INFO - Skipping layer model.layers.3.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,867 - INFO - Layer: model.layers.3.self_attn.o_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 22:46:13,867 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_3_self_attn_o_proj.safetensors
2025-04-24 22:46:13,867 - INFO - exists: False
2025-04-24 22:46:13,867 - WARNING - Skipping layer model.layers.3.self_attn.o_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_3_self_attn_o_proj.safetensors
2025-04-24 22:46:13,867 - INFO - Layer: model.layers.3.mlp.gate_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
2025-04-24 22:46:13,867 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_3_mlp_gate_proj.safetensors
2025-04-24 22:46:13,867 - INFO - exists: False
2025-04-24 22:46:13,867 - WARNING - Skipping layer model.layers.3.mlp.gate_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_3_mlp_gate_proj.safetensors
2025-04-24 22:46:13,867 - INFO - Skipping layer model.layers.3.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,867 - INFO - Layer: model.layers.3.mlp.down_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
2025-04-24 22:46:13,867 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_3_mlp_down_proj.safetensors
2025-04-24 22:46:13,867 - INFO - exists: False
2025-04-24 22:46:13,867 - WARNING - Skipping layer model.layers.3.mlp.down_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_3_mlp_down_proj.safetensors
2025-04-24 22:46:13,867 - INFO - Layer: model.layers.4.self_attn.q_proj | Ratio: 0.400 -> Target Rank: 824 (Align: 8)
2025-04-24 22:46:13,867 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_4_self_attn_q_proj.safetensors
2025-04-24 22:46:13,867 - INFO - exists: False
2025-04-24 22:46:13,867 - WARNING - Skipping layer model.layers.4.self_attn.q_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_4_self_attn_q_proj.safetensors
2025-04-24 22:46:13,867 - INFO - Layer: model.layers.4.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-24 22:46:13,868 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_4_self_attn_k_proj.safetensors
2025-04-24 22:46:13,868 - INFO - exists: False
2025-04-24 22:46:13,868 - WARNING - Skipping layer model.layers.4.self_attn.k_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_4_self_attn_k_proj.safetensors
2025-04-24 22:46:13,868 - INFO - Skipping layer model.layers.4.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,868 - INFO - Layer: model.layers.4.self_attn.o_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 22:46:13,868 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_4_self_attn_o_proj.safetensors
2025-04-24 22:46:13,868 - INFO - exists: False
2025-04-24 22:46:13,868 - WARNING - Skipping layer model.layers.4.self_attn.o_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_4_self_attn_o_proj.safetensors
2025-04-24 22:46:13,868 - INFO - Layer: model.layers.4.mlp.gate_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
2025-04-24 22:46:13,868 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_4_mlp_gate_proj.safetensors
2025-04-24 22:46:13,868 - INFO - exists: False
2025-04-24 22:46:13,868 - WARNING - Skipping layer model.layers.4.mlp.gate_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_4_mlp_gate_proj.safetensors
2025-04-24 22:46:13,868 - INFO - Skipping layer model.layers.4.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,868 - INFO - Layer: model.layers.4.mlp.down_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
2025-04-24 22:46:13,868 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_4_mlp_down_proj.safetensors
2025-04-24 22:46:13,868 - INFO - exists: False
2025-04-24 22:46:13,868 - WARNING - Skipping layer model.layers.4.mlp.down_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_4_mlp_down_proj.safetensors
2025-04-24 22:46:13,868 - INFO - Layer: model.layers.5.self_attn.q_proj | Ratio: 0.200 -> Target Rank: 416 (Align: 8)
2025-04-24 22:46:13,868 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_5_self_attn_q_proj.safetensors
2025-04-24 22:46:13,868 - INFO - exists: False
2025-04-24 22:46:13,868 - WARNING - Skipping layer model.layers.5.self_attn.q_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_5_self_attn_q_proj.safetensors
2025-04-24 22:46:13,868 - INFO - Layer: model.layers.5.self_attn.k_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 22:46:13,868 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_5_self_attn_k_proj.safetensors
2025-04-24 22:46:13,868 - INFO - exists: False
2025-04-24 22:46:13,868 - WARNING - Skipping layer model.layers.5.self_attn.k_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_5_self_attn_k_proj.safetensors
2025-04-24 22:46:13,869 - INFO - Skipping layer model.layers.5.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,869 - INFO - Layer: model.layers.5.self_attn.o_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 22:46:13,869 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_5_self_attn_o_proj.safetensors
2025-04-24 22:46:13,869 - INFO - exists: False
2025-04-24 22:46:13,869 - WARNING - Skipping layer model.layers.5.self_attn.o_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_5_self_attn_o_proj.safetensors
2025-04-24 22:46:13,869 - INFO - Skipping layer model.layers.5.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,869 - INFO - Skipping layer model.layers.5.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,869 - INFO - Skipping layer model.layers.5.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,869 - INFO - Layer: model.layers.6.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-24 22:46:13,869 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_6_self_attn_q_proj.safetensors
2025-04-24 22:46:13,869 - INFO - exists: False
2025-04-24 22:46:13,869 - WARNING - Skipping layer model.layers.6.self_attn.q_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_6_self_attn_q_proj.safetensors
2025-04-24 22:46:13,869 - INFO - Layer: model.layers.6.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-24 22:46:13,869 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_6_self_attn_k_proj.safetensors
2025-04-24 22:46:13,869 - INFO - exists: False
2025-04-24 22:46:13,869 - WARNING - Skipping layer model.layers.6.self_attn.k_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_6_self_attn_k_proj.safetensors
2025-04-24 22:46:13,869 - INFO - Skipping layer model.layers.6.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,869 - INFO - Layer: model.layers.6.self_attn.o_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
2025-04-24 22:46:13,869 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_6_self_attn_o_proj.safetensors
2025-04-24 22:46:13,869 - INFO - exists: False
2025-04-24 22:46:13,869 - WARNING - Skipping layer model.layers.6.self_attn.o_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_6_self_attn_o_proj.safetensors
2025-04-24 22:46:13,869 - INFO - Skipping layer model.layers.6.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,869 - INFO - Skipping layer model.layers.6.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,869 - INFO - Skipping layer model.layers.6.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,869 - INFO - Layer: model.layers.7.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 22:46:13,869 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_7_self_attn_q_proj.safetensors
2025-04-24 22:46:13,869 - INFO - exists: False
2025-04-24 22:46:13,869 - WARNING - Skipping layer model.layers.7.self_attn.q_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_7_self_attn_q_proj.safetensors
2025-04-24 22:46:13,869 - INFO - Layer: model.layers.7.self_attn.k_proj | Ratio: 0.400 -> Target Rank: 824 (Align: 8)
2025-04-24 22:46:13,870 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_7_self_attn_k_proj.safetensors
2025-04-24 22:46:13,870 - INFO - exists: False
2025-04-24 22:46:13,870 - WARNING - Skipping layer model.layers.7.self_attn.k_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_7_self_attn_k_proj.safetensors
2025-04-24 22:46:13,870 - INFO - Skipping layer model.layers.7.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,870 - INFO - Layer: model.layers.7.self_attn.o_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 22:46:13,870 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_7_self_attn_o_proj.safetensors
2025-04-24 22:46:13,870 - INFO - exists: False
2025-04-24 22:46:13,870 - WARNING - Skipping layer model.layers.7.self_attn.o_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_7_self_attn_o_proj.safetensors
2025-04-24 22:46:13,870 - INFO - Layer: model.layers.7.mlp.gate_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
2025-04-24 22:46:13,870 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_7_mlp_gate_proj.safetensors
2025-04-24 22:46:13,870 - INFO - exists: False
2025-04-24 22:46:13,870 - WARNING - Skipping layer model.layers.7.mlp.gate_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_7_mlp_gate_proj.safetensors
2025-04-24 22:46:13,870 - INFO - Skipping layer model.layers.7.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,870 - INFO - Skipping layer model.layers.7.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,870 - INFO - Layer: model.layers.8.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 22:46:13,870 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_8_self_attn_q_proj.safetensors
2025-04-24 22:46:13,870 - INFO - exists: False
2025-04-24 22:46:13,870 - WARNING - Skipping layer model.layers.8.self_attn.q_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_8_self_attn_q_proj.safetensors
2025-04-24 22:46:13,870 - INFO - Layer: model.layers.8.self_attn.k_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 22:46:13,870 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_8_self_attn_k_proj.safetensors
2025-04-24 22:46:13,870 - INFO - exists: False
2025-04-24 22:46:13,870 - WARNING - Skipping layer model.layers.8.self_attn.k_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_8_self_attn_k_proj.safetensors
2025-04-24 22:46:13,870 - INFO - Skipping layer model.layers.8.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,870 - INFO - Layer: model.layers.8.self_attn.o_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 22:46:13,870 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_8_self_attn_o_proj.safetensors
2025-04-24 22:46:13,870 - INFO - exists: False
2025-04-24 22:46:13,870 - WARNING - Skipping layer model.layers.8.self_attn.o_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_8_self_attn_o_proj.safetensors
2025-04-24 22:46:13,870 - INFO - Skipping layer model.layers.8.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,870 - INFO - Skipping layer model.layers.8.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,870 - INFO - Skipping layer model.layers.8.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,871 - INFO - Layer: model.layers.9.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 22:46:13,871 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_9_self_attn_q_proj.safetensors
2025-04-24 22:46:13,871 - INFO - exists: False
2025-04-24 22:46:13,871 - WARNING - Skipping layer model.layers.9.self_attn.q_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_9_self_attn_q_proj.safetensors
2025-04-24 22:46:13,871 - INFO - Layer: model.layers.9.self_attn.k_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 22:46:13,871 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_9_self_attn_k_proj.safetensors
2025-04-24 22:46:13,871 - INFO - exists: False
2025-04-24 22:46:13,871 - WARNING - Skipping layer model.layers.9.self_attn.k_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_9_self_attn_k_proj.safetensors
2025-04-24 22:46:13,871 - INFO - Skipping layer model.layers.9.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,871 - INFO - Layer: model.layers.9.self_attn.o_proj | Ratio: 0.900 -> Target Rank: 1848 (Align: 8)
2025-04-24 22:46:13,871 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_9_self_attn_o_proj.safetensors
2025-04-24 22:46:13,871 - INFO - exists: False
2025-04-24 22:46:13,871 - WARNING - Skipping layer model.layers.9.self_attn.o_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_9_self_attn_o_proj.safetensors
2025-04-24 22:46:13,871 - INFO - Skipping layer model.layers.9.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,871 - INFO - Skipping layer model.layers.9.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,871 - INFO - Skipping layer model.layers.9.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:46:13,871 - INFO - Layer: model.layers.10.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 22:46:13,871 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_10_self_attn_q_proj.safetensors
2025-04-24 22:46:13,871 - INFO - exists: True
2025-04-24 22:46:13,898 - INFO - factorize_layer_kron_svd
2025-04-24 22:46:15,123 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:46:16,559 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:46:17,911 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:46:19,395 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_3_self_attn_q_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_3_self_attn_k_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_3_self_attn_o_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_3_mlp_gate_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_3_mlp_down_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_4_self_attn_q_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_4_self_attn_k_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_4_self_attn_o_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_4_mlp_gate_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_4_mlp_down_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_5_self_attn_q_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_5_self_attn_k_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_5_self_attn_o_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_6_self_attn_q_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_6_self_attn_k_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_6_self_attn_o_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_7_self_attn_q_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_7_self_attn_k_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_7_self_attn_o_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_7_mlp_gate_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_8_self_attn_q_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_8_self_attn_k_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_8_self_attn_o_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_9_self_attn_q_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_9_self_attn_k_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_9_self_attn_o_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_10_self_attn_q_proj.safetensors True
Layer: model.layers.10.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [9.1147475e-04 1.1751837e-04 8.8331944e-05 4.9924896e-05 3.9312708e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)
2025-04-24 22:46:54,530 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)')
2025-04-24 22:46:54,530 - INFO - Replacing 'model.layers.10.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  32%|███████████████▊                                  | 71/225 [17:22<08:51,  3.45s/it]2025-04-24 22:46:54,531 - INFO - Layer: model.layers.10.self_attn.k_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
2025-04-24 22:46:54,531 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_10_self_attn_k_proj.safetensors
2025-04-24 22:46:54,531 - INFO - exists: True
2025-04-24 22:46:54,545 - INFO - factorize_layer_kron_svd
2025-04-24 22:46:55,945 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:46:57,163 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:46:58,305 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:46:59,586 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_10_self_attn_k_proj.safetensors True
Layer: model.layers.10.self_attn.k_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [7.4601988e-04 9.0922345e-05 6.7010224e-05 5.4932527e-05 5.0821891e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)
2025-04-24 22:47:37,008 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)')
2025-04-24 22:47:37,009 - INFO - Replacing 'model.layers.10.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  32%|████████████████                                  | 72/225 [18:05<11:20,  4.45s/it]2025-04-24 22:47:37,009 - INFO - Skipping layer model.layers.10.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:47:37,009 - INFO - Skipping layer model.layers.10.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:47:37,009 - INFO - Skipping layer model.layers.10.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:47:37,009 - INFO - Skipping layer model.layers.10.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:47:37,009 - INFO - Skipping layer model.layers.10.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:47:37,009 - INFO - Layer: model.layers.11.self_attn.q_proj | Ratio: 0.200 -> Target Rank: 416 (Align: 8)
2025-04-24 22:47:37,010 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_11_self_attn_q_proj.safetensors
2025-04-24 22:47:37,010 - INFO - exists: True
2025-04-24 22:47:37,016 - INFO - factorize_layer_kron_svd
2025-04-24 22:47:38,278 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:47:39,567 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:47:41,165 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:47:42,810 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_11_self_attn_q_proj.safetensors True
Layer: model.layers.11.self_attn.q_proj | Ratio: 0.200 -> Target Rank: 416 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [8.7986700e-04 9.9609977e-05 7.5574397e-05 5.1822190e-05 4.9105878e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (416,4096), (4096,416)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=416, bias=False)
  (1): Linear(in_features=416, out_features=4096, bias=False)
)
2025-04-24 22:48:18,964 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=416, bias=False)
  (1): Linear(in_features=416, out_features=4096, bias=False)
)')
2025-04-24 22:48:18,964 - INFO - Replacing 'model.layers.11.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  35%|█████████████████▎                                | 78/225 [18:47<12:01,  4.91s/it]2025-04-24 22:48:18,965 - INFO - Layer: model.layers.11.self_attn.k_proj | Ratio: 0.200 -> Target Rank: 416 (Align: 8)
2025-04-24 22:48:18,965 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_11_self_attn_k_proj.safetensors
2025-04-24 22:48:18,965 - INFO - exists: True
2025-04-24 22:48:18,972 - INFO - factorize_layer_kron_svd
2025-04-24 22:48:20,272 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:48:21,531 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:48:22,857 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:48:24,259 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_11_self_attn_k_proj.safetensors True
Layer: model.layers.11.self_attn.k_proj | Ratio: 0.200 -> Target Rank: 416 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [9.4792934e-04 1.2157520e-04 1.0247317e-04 7.1057482e-05 5.7551941e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (416,4096), (4096,416)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=416, bias=False)
  (1): Linear(in_features=416, out_features=4096, bias=False)
)
2025-04-24 22:48:59,972 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=416, bias=False)
  (1): Linear(in_features=416, out_features=4096, bias=False)
)')
2025-04-24 22:48:59,972 - INFO - Replacing 'model.layers.11.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  35%|█████████████████▌                                | 79/225 [19:28<15:32,  6.39s/it]2025-04-24 22:48:59,972 - INFO - Skipping layer model.layers.11.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:48:59,972 - INFO - Layer: model.layers.11.self_attn.o_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
2025-04-24 22:48:59,973 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_11_self_attn_o_proj.safetensors
2025-04-24 22:48:59,973 - INFO - exists: True
2025-04-24 22:48:59,987 - INFO - factorize_layer_kron_svd
2025-04-24 22:49:01,246 - INFO -   Factor is positive definite (alpha=1.00e-05)
2025-04-24 22:49:02,447 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_11_self_attn_o_proj.safetensors True
Layer: model.layers.11.self_attn.o_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Factor is positive definite (alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [2.2733117e-04 2.0363974e-05 1.6356025e-05 1.3161105e-05 1.1948326e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1640,4096), (4096,1640)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)
2025-04-24 22:49:40,274 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)')
2025-04-24 22:49:40,274 - INFO - Replacing 'model.layers.11.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  36%|██████████████████                                | 81/225 [20:08<18:47,  7.83s/it]2025-04-24 22:49:40,274 - INFO - Skipping layer model.layers.11.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:49:40,275 - INFO - Skipping layer model.layers.11.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:49:40,275 - INFO - Skipping layer model.layers.11.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:49:40,275 - INFO - Layer: model.layers.12.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-24 22:49:40,275 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_12_self_attn_q_proj.safetensors
2025-04-24 22:49:40,275 - INFO - exists: True
2025-04-24 22:49:40,281 - INFO - factorize_layer_kron_svd
2025-04-24 22:49:41,476 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:49:42,709 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:49:44,020 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:49:45,234 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_12_self_attn_q_proj.safetensors True
Layer: model.layers.12.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [9.7795611e-04 7.3599527e-05 5.4209992e-05 4.3944550e-05 4.0591403e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-24 22:50:22,994 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-24 22:50:22,994 - INFO - Replacing 'model.layers.12.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  38%|██████████████████▉                               | 85/225 [20:51<19:48,  8.49s/it]2025-04-24 22:50:22,995 - INFO - Layer: model.layers.12.self_attn.k_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 22:50:22,996 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_12_self_attn_k_proj.safetensors
2025-04-24 22:50:22,996 - INFO - exists: True
2025-04-24 22:50:23,026 - INFO - factorize_layer_kron_svd
2025-04-24 22:50:24,287 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:50:25,599 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:50:26,743 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:50:28,005 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_12_self_attn_k_proj.safetensors True
Layer: model.layers.12.self_attn.k_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.07608642e-03 1.05497034e-04 8.38672131e-05 7.40240066e-05
 4.44736033e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)
2025-04-24 22:50:51,360 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)')
2025-04-24 22:50:51,360 - INFO - Replacing 'model.layers.12.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  38%|███████████████████                               | 86/225 [21:19<23:09, 10.00s/it]2025-04-24 22:50:51,360 - INFO - Skipping layer model.layers.12.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:50:51,360 - INFO - Layer: model.layers.12.self_attn.o_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 22:50:51,361 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_12_self_attn_o_proj.safetensors
2025-04-24 22:50:51,361 - INFO - exists: True
2025-04-24 22:50:51,372 - INFO - factorize_layer_kron_svd
2025-04-24 22:50:52,609 - INFO -   Factor is positive definite (alpha=1.00e-05)
2025-04-24 22:50:53,813 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_12_self_attn_o_proj.safetensors True
Layer: model.layers.12.self_attn.o_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Factor is positive definite (alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.06922475e-04 2.49136410e-05 1.08429231e-05 1.04350329e-05
 9.57184784e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1440,4096), (4096,1440)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)
2025-04-24 22:51:27,895 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)')
2025-04-24 22:51:27,895 - INFO - Replacing 'model.layers.12.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  39%|███████████████████▌                              | 88/225 [21:56<26:12, 11.48s/it]2025-04-24 22:51:27,896 - INFO - Layer: model.layers.12.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)
2025-04-24 22:51:27,896 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_12_mlp_gate_proj.safetensors
2025-04-24 22:51:27,896 - INFO - exists: True
2025-04-24 22:51:27,904 - INFO - factorize_layer_kron_svd
2025-04-24 22:51:29,392 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:51:30,754 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:51:34,793 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_12_mlp_gate_proj.safetensors True
Layer: model.layers.12.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [3.4531584e-04 4.3154534e-05 2.8466011e-05 2.0477493e-05 1.5518726e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1496, bias=False)
  (1): Linear(in_features=1496, out_features=11008, bias=False)
)
2025-04-24 22:52:33,994 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1496, bias=False)
  (1): Linear(in_features=1496, out_features=11008, bias=False)
)')
2025-04-24 22:52:33,994 - INFO - Replacing 'model.layers.12.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  40%|███████████████████▊                              | 89/225 [23:02<40:01, 17.66s/it]2025-04-24 22:52:33,995 - INFO - Layer: model.layers.12.mlp.up_proj | Ratio: 0.600 -> Target Rank: 1792 (Align: 8)
2025-04-24 22:52:33,995 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_12_mlp_up_proj.safetensors
2025-04-24 22:52:33,995 - INFO - exists: True
2025-04-24 22:52:34,037 - INFO - factorize_layer_kron_svd
2025-04-24 22:52:35,626 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:52:37,039 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:52:40,622 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_12_mlp_up_proj.safetensors True
Layer: model.layers.12.mlp.up_proj | Ratio: 0.600 -> Target Rank: 1792 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [3.1828787e-04 4.3379245e-05 2.2649983e-05 1.7851607e-05 1.5423566e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1792,4096), (11008,1792)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1792, bias=False)
  (1): Linear(in_features=1792, out_features=11008, bias=False)
)
2025-04-24 22:53:34,882 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1792, bias=False)
  (1): Linear(in_features=1792, out_features=11008, bias=False)
)')
2025-04-24 22:53:34,883 - INFO - Replacing 'model.layers.12.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  40%|████████████████████                              | 90/225 [24:03<53:15, 23.67s/it]2025-04-24 22:53:34,883 - INFO - Layer: model.layers.12.mlp.down_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
2025-04-24 22:53:34,884 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_12_mlp_down_proj.safetensors
2025-04-24 22:53:34,884 - INFO - exists: True
2025-04-24 22:53:34,929 - INFO - factorize_layer_kron_svd
2025-04-24 22:53:37,767 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:53:41,942 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:53:43,313 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_12_mlp_down_proj.safetensors True
Layer: model.layers.12.mlp.down_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [1.1694148e-03 2.0377325e-04 1.2311406e-04 1.0693405e-04 8.5032843e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2688,11008), (4096,2688)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=2688, bias=False)
  (1): Linear(in_features=2688, out_features=4096, bias=False)
)
2025-04-24 22:54:45,826 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=2688, bias=False)
  (1): Linear(in_features=2688, out_features=4096, bias=False)
)')
2025-04-24 22:54:45,827 - INFO - Replacing 'model.layers.12.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  40%|███████████████████▍                            | 91/225 [25:14<1:10:21, 31.51s/it]2025-04-24 22:54:45,827 - INFO - Layer: model.layers.13.self_attn.q_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
2025-04-24 22:54:45,829 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_13_self_attn_q_proj.safetensors
2025-04-24 22:54:45,829 - INFO - exists: True
2025-04-24 22:54:45,854 - INFO - factorize_layer_kron_svd
2025-04-24 22:54:47,235 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:54:48,597 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:54:49,796 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:54:51,089 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_13_self_attn_q_proj.safetensors True
Layer: model.layers.13.self_attn.q_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.09047582e-03 1.13891496e-04 6.78460201e-05 4.42121782e-05
 3.75433629e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)
2025-04-24 22:55:14,384 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)')
2025-04-24 22:55:14,385 - INFO - Replacing 'model.layers.13.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  41%|███████████████████▋                            | 92/225 [25:42<1:08:35, 30.94s/it]2025-04-24 22:55:14,385 - INFO - Layer: model.layers.13.self_attn.k_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 22:55:14,387 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_13_self_attn_k_proj.safetensors
2025-04-24 22:55:14,387 - INFO - exists: True
2025-04-24 22:55:14,421 - INFO - factorize_layer_kron_svd
2025-04-24 22:55:15,782 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:55:16,985 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:55:18,268 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:55:19,687 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_13_self_attn_k_proj.safetensors True
Layer: model.layers.13.self_attn.k_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.1110766e-03 9.8493336e-05 7.3882249e-05 5.9176251e-05 4.1877520e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1440,4096), (4096,1440)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)
2025-04-24 22:55:40,408 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)')
2025-04-24 22:55:40,409 - INFO - Replacing 'model.layers.13.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  41%|███████████████████▊                            | 93/225 [26:08<1:05:44, 29.89s/it]2025-04-24 22:55:40,409 - INFO - Skipping layer model.layers.13.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:55:40,409 - INFO - Layer: model.layers.13.self_attn.o_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
2025-04-24 22:55:40,426 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_13_self_attn_o_proj.safetensors
2025-04-24 22:55:40,426 - INFO - exists: True
2025-04-24 22:55:40,438 - INFO - factorize_layer_kron_svd
2025-04-24 22:55:42,268 - INFO -   Factor is positive definite (alpha=1.00e-05)
2025-04-24 22:55:43,633 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_13_self_attn_o_proj.safetensors True
Layer: model.layers.13.self_attn.o_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Factor is positive definite (alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.67980761e-04 3.03513152e-05 1.29702248e-05 1.20683135e-05
 1.04485744e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1640,4096), (4096,1640)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)
2025-04-24 22:56:25,836 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)')
2025-04-24 22:56:25,837 - INFO - Replacing 'model.layers.13.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  42%|█████████████████████                             | 95/225 [26:54<58:50, 27.16s/it]2025-04-24 22:56:25,837 - INFO - Skipping layer model.layers.13.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:56:25,837 - INFO - Skipping layer model.layers.13.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:56:25,837 - INFO - Skipping layer model.layers.13.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:56:25,837 - INFO - Layer: model.layers.14.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 22:56:25,839 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_14_self_attn_q_proj.safetensors
2025-04-24 22:56:25,839 - INFO - exists: True
2025-04-24 22:56:25,849 - INFO - factorize_layer_kron_svd
2025-04-24 22:56:27,013 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:56:28,301 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:56:29,483 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:56:30,709 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_14_self_attn_q_proj.safetensors True
Layer: model.layers.14.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [8.67754861e-04 1.08093715e-04 5.89062947e-05 5.34788778e-05
 4.27111954e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)
2025-04-24 22:56:59,008 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)')
2025-04-24 22:56:59,008 - INFO - Replacing 'model.layers.14.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  44%|██████████████████████                            | 99/225 [27:27<36:24, 17.33s/it]2025-04-24 22:56:59,008 - INFO - Layer: model.layers.14.self_attn.k_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
2025-04-24 22:56:59,010 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_14_self_attn_k_proj.safetensors
2025-04-24 22:56:59,010 - INFO - exists: True
2025-04-24 22:56:59,030 - INFO - factorize_layer_kron_svd
2025-04-24 22:57:00,397 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:57:01,792 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:57:03,070 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:57:04,366 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_14_self_attn_k_proj.safetensors True
Layer: model.layers.14.self_attn.k_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [9.7232516e-04 1.0987472e-04 7.0545255e-05 5.8294750e-05 4.7337810e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)
2025-04-24 22:57:35,193 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)')
2025-04-24 22:57:35,193 - INFO - Replacing 'model.layers.14.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  44%|█████████████████████▊                           | 100/225 [28:03<42:16, 20.29s/it]2025-04-24 22:57:35,193 - INFO - Skipping layer model.layers.14.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:57:35,193 - INFO - Skipping layer model.layers.14.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:57:35,193 - INFO - Skipping layer model.layers.14.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:57:35,194 - INFO - Skipping layer model.layers.14.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:57:35,194 - INFO - Skipping layer model.layers.14.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:57:35,194 - INFO - Layer: model.layers.15.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 22:57:35,195 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_15_self_attn_q_proj.safetensors
2025-04-24 22:57:35,195 - INFO - exists: True
2025-04-24 22:57:35,204 - INFO - factorize_layer_kron_svd
2025-04-24 22:57:36,441 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:57:37,697 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:57:39,116 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:57:40,346 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_15_self_attn_q_proj.safetensors True
Layer: model.layers.15.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [8.6131087e-04 8.2904990e-05 6.6847955e-05 5.3464220e-05 4.0176823e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)
2025-04-24 22:58:19,463 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)')
2025-04-24 22:58:19,463 - INFO - Replacing 'model.layers.15.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  47%|███████████████████████                          | 106/225 [28:47<25:33, 12.89s/it]2025-04-24 22:58:19,463 - INFO - Layer: model.layers.15.self_attn.k_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 22:58:19,465 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_15_self_attn_k_proj.safetensors
2025-04-24 22:58:19,465 - INFO - exists: True
2025-04-24 22:58:19,474 - INFO - factorize_layer_kron_svd
2025-04-24 22:58:20,681 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:58:22,224 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:58:23,611 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:58:25,062 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_15_self_attn_k_proj.safetensors True
Layer: model.layers.15.self_attn.k_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [8.8808837e-04 1.1092378e-04 6.3344865e-05 3.6479836e-05 3.4227403e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1440,4096), (4096,1440)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)
2025-04-24 22:58:53,996 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)')
2025-04-24 22:58:53,997 - INFO - Replacing 'model.layers.15.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  48%|███████████████████████▎                         | 107/225 [29:22<30:27, 15.49s/it]2025-04-24 22:58:53,997 - INFO - Skipping layer model.layers.15.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:58:53,997 - INFO - Skipping layer model.layers.15.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:58:53,997 - INFO - Skipping layer model.layers.15.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:58:53,997 - INFO - Skipping layer model.layers.15.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:58:53,997 - INFO - Skipping layer model.layers.15.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 22:58:53,997 - INFO - Layer: model.layers.16.self_attn.q_proj | Ratio: 0.400 -> Target Rank: 824 (Align: 8)
2025-04-24 22:58:53,998 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_16_self_attn_q_proj.safetensors
2025-04-24 22:58:53,998 - INFO - exists: True
2025-04-24 22:58:54,010 - INFO - factorize_layer_kron_svd
2025-04-24 22:58:55,505 - INFO -   Factor is positive definite (alpha=1.00e-05)
2025-04-24 22:58:56,895 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_16_self_attn_q_proj.safetensors True
Layer: model.layers.16.self_attn.q_proj | Ratio: 0.400 -> Target Rank: 824 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Factor is positive definite (alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [8.8457525e-04 7.0841801e-05 5.7777223e-05 5.5076001e-05 4.2950800e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (824,4096), (4096,824)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=824, bias=False)
  (1): Linear(in_features=824, out_features=4096, bias=False)
)
2025-04-24 22:59:36,393 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=824, bias=False)
  (1): Linear(in_features=824, out_features=4096, bias=False)
)')
2025-04-24 22:59:36,393 - INFO - Replacing 'model.layers.16.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  50%|████████████████████████▌                        | 113/225 [30:04<20:56, 11.21s/it]2025-04-24 22:59:36,393 - INFO - Layer: model.layers.16.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-24 22:59:36,423 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_16_self_attn_k_proj.safetensors
2025-04-24 22:59:36,423 - INFO - exists: True
2025-04-24 22:59:36,430 - INFO - factorize_layer_kron_svd
2025-04-24 22:59:38,002 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 22:59:39,582 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 22:59:41,006 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_16_self_attn_k_proj.safetensors True
Layer: model.layers.16.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.0770942e-03 8.0462807e-05 6.1351449e-05 4.7102651e-05 3.8980666e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-24 23:00:10,734 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-24 23:00:10,734 - INFO - Replacing 'model.layers.16.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  51%|████████████████████████▊                        | 114/225 [30:39<25:21, 13.71s/it]2025-04-24 23:00:10,735 - INFO - Skipping layer model.layers.16.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:00:10,735 - INFO - Layer: model.layers.16.self_attn.o_proj | Ratio: 0.900 -> Target Rank: 1848 (Align: 8)
2025-04-24 23:00:10,736 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_16_self_attn_o_proj.safetensors
2025-04-24 23:00:10,736 - INFO - exists: True
2025-04-24 23:00:10,748 - INFO - factorize_layer_kron_svd
2025-04-24 23:00:11,906 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:00:13,055 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:00:14,231 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 23:00:15,400 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 23:00:16,562 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 23:00:17,985 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-24 23:00:19,079 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:00:20,232 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:00:21,379 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 23:00:22,482 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 23:00:23,635 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 23:00:24,850 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_16_self_attn_o_proj.safetensors True
Layer: model.layers.16.self_attn.o_proj | Ratio: 0.900 -> Target Rank: 1848 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [5.0113722e-06 2.6841703e-06 2.6340790e-06 2.5708566e-06 2.5678978e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1848,4096), (4096,1848)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1848, bias=False)
  (1): Linear(in_features=1848, out_features=4096, bias=False)
)
2025-04-24 23:00:57,510 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1848, bias=False)
  (1): Linear(in_features=1848, out_features=4096, bias=False)
)')
2025-04-24 23:00:57,510 - INFO - Replacing 'model.layers.16.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  52%|█████████████████████████▎                       | 116/225 [31:25<29:02, 15.99s/it]2025-04-24 23:00:57,511 - INFO - Skipping layer model.layers.16.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:00:57,511 - INFO - Skipping layer model.layers.16.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:00:57,511 - INFO - Skipping layer model.layers.16.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:00:57,511 - INFO - Layer: model.layers.17.self_attn.q_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
2025-04-24 23:00:57,512 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_17_self_attn_q_proj.safetensors
2025-04-24 23:00:57,512 - INFO - exists: True
2025-04-24 23:00:57,527 - INFO - factorize_layer_kron_svd
2025-04-24 23:00:58,708 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:00:59,914 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:01:01,334 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_17_self_attn_q_proj.safetensors True
Layer: model.layers.17.self_attn.q_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [7.689794e-04 7.188844e-05 6.298116e-05 4.230161e-05 4.018409e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)
2025-04-24 23:01:28,523 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)')
2025-04-24 23:01:28,524 - INFO - Replacing 'model.layers.17.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  53%|██████████████████████████▏                      | 120/225 [31:56<22:10, 12.68s/it]2025-04-24 23:01:28,524 - INFO - Layer: model.layers.17.self_attn.k_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 23:01:28,525 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_17_self_attn_k_proj.safetensors
2025-04-24 23:01:28,525 - INFO - exists: True
2025-04-24 23:01:28,536 - INFO - factorize_layer_kron_svd
2025-04-24 23:01:29,739 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:01:30,949 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:01:32,118 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:01:33,320 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_17_self_attn_k_proj.safetensors True
Layer: model.layers.17.self_attn.k_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [9.0933678e-04 7.7465673e-05 6.2697633e-05 4.3592729e-05 3.4580888e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)
2025-04-24 23:01:57,891 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)')
2025-04-24 23:01:57,891 - INFO - Replacing 'model.layers.17.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  54%|██████████████████████████▎                      | 121/225 [32:26<25:36, 14.77s/it]2025-04-24 23:01:57,891 - INFO - Skipping layer model.layers.17.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:01:57,891 - INFO - Skipping layer model.layers.17.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:01:57,892 - INFO - Skipping layer model.layers.17.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:01:57,892 - INFO - Skipping layer model.layers.17.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:01:57,892 - INFO - Skipping layer model.layers.17.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:01:57,892 - INFO - Layer: model.layers.18.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 23:01:57,893 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_18_self_attn_q_proj.safetensors
2025-04-24 23:01:57,893 - INFO - exists: True
2025-04-24 23:01:57,902 - INFO - factorize_layer_kron_svd
2025-04-24 23:01:59,040 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:02:00,227 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:02:01,381 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_18_self_attn_q_proj.safetensors True
Layer: model.layers.18.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [9.4509847e-04 6.3694992e-05 5.3031312e-05 3.3028406e-05 3.0152803e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)
2025-04-24 23:02:32,176 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)')
2025-04-24 23:02:32,176 - INFO - Replacing 'model.layers.18.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  56%|███████████████████████████▋                     | 127/225 [33:00<16:27, 10.08s/it]2025-04-24 23:02:32,177 - INFO - Layer: model.layers.18.self_attn.k_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 23:02:32,177 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_18_self_attn_k_proj.safetensors
2025-04-24 23:02:32,177 - INFO - exists: True
2025-04-24 23:02:32,183 - INFO - factorize_layer_kron_svd
2025-04-24 23:02:33,415 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:02:34,675 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:02:35,876 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:02:37,100 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_18_self_attn_k_proj.safetensors True
Layer: model.layers.18.self_attn.k_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [9.9182804e-04 7.6477787e-05 3.8036465e-05 3.0466859e-05 2.4186118e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1440,4096), (4096,1440)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)
2025-04-24 23:03:14,887 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)')
2025-04-24 23:03:14,887 - INFO - Replacing 'model.layers.18.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  57%|███████████████████████████▉                     | 128/225 [33:43<22:05, 13.66s/it]2025-04-24 23:03:14,888 - INFO - Skipping layer model.layers.18.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:03:14,888 - INFO - Layer: model.layers.18.self_attn.o_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
2025-04-24 23:03:14,888 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_18_self_attn_o_proj.safetensors
2025-04-24 23:03:14,888 - INFO - exists: True
2025-04-24 23:03:14,898 - INFO - factorize_layer_kron_svd
2025-04-24 23:03:16,010 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:03:17,122 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:03:18,254 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 23:03:19,406 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 23:03:20,570 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 23:03:21,979 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-24 23:03:23,086 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:03:24,231 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:03:25,383 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 23:03:26,540 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 23:03:27,782 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 23:03:29,025 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_18_self_attn_o_proj.safetensors True
Layer: model.layers.18.self_attn.o_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [5.4370907e-06 2.7923429e-06 2.7048741e-06 2.5941874e-06 2.5748666e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1640,4096), (4096,1640)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)
2025-04-24 23:03:53,286 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)')
2025-04-24 23:03:53,286 - INFO - Replacing 'model.layers.18.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  58%|████████████████████████████▎                    | 130/225 [34:21<23:43, 14.98s/it]2025-04-24 23:03:53,287 - INFO - Skipping layer model.layers.18.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:03:53,287 - INFO - Skipping layer model.layers.18.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:03:53,287 - INFO - Skipping layer model.layers.18.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:03:53,287 - INFO - Layer: model.layers.19.self_attn.q_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 23:03:53,287 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_19_self_attn_q_proj.safetensors
2025-04-24 23:03:53,287 - INFO - exists: True
2025-04-24 23:03:53,298 - INFO - factorize_layer_kron_svd
2025-04-24 23:03:54,658 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:03:56,088 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:03:57,465 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_19_self_attn_q_proj.safetensors True
Layer: model.layers.19.self_attn.q_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [9.1692264e-04 5.3431831e-05 4.5319226e-05 3.9285813e-05 3.2373766e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1440,4096), (4096,1440)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)
2025-04-24 23:04:22,007 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)')
2025-04-24 23:04:22,007 - INFO - Replacing 'model.layers.19.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  60%|█████████████████████████████▏                   | 134/225 [34:50<17:55, 11.82s/it]2025-04-24 23:04:22,007 - INFO - Layer: model.layers.19.self_attn.k_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
2025-04-24 23:04:22,009 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_19_self_attn_k_proj.safetensors
2025-04-24 23:04:22,009 - INFO - exists: True
2025-04-24 23:04:22,021 - INFO - factorize_layer_kron_svd
2025-04-24 23:04:23,742 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:04:24,985 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:04:26,378 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_19_self_attn_k_proj.safetensors True
Layer: model.layers.19.self_attn.k_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [9.4402843e-04 5.6601450e-05 4.7169488e-05 3.6125271e-05 3.3897668e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)
2025-04-24 23:04:48,174 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)')
2025-04-24 23:04:48,174 - INFO - Replacing 'model.layers.19.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  60%|█████████████████████████████▍                   | 135/225 [35:16<20:27, 13.63s/it]2025-04-24 23:04:48,174 - INFO - Skipping layer model.layers.19.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:04:48,174 - INFO - Layer: model.layers.19.self_attn.o_proj | Ratio: 0.900 -> Target Rank: 1848 (Align: 8)
2025-04-24 23:04:48,174 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_19_self_attn_o_proj.safetensors
2025-04-24 23:04:48,174 - INFO - exists: True
2025-04-24 23:04:48,186 - INFO - factorize_layer_kron_svd
2025-04-24 23:04:49,356 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:04:50,491 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:04:51,625 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 23:04:52,743 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 23:04:53,885 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 23:04:55,333 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-24 23:04:56,434 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:04:57,571 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:04:58,715 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 23:04:59,889 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 23:05:01,014 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 23:05:02,385 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_19_self_attn_o_proj.safetensors True
Layer: model.layers.19.self_attn.o_proj | Ratio: 0.900 -> Target Rank: 1848 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [5.3624221e-06 2.7126407e-06 2.6424659e-06 2.5899076e-06 2.4992921e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1848,4096), (4096,1848)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1848, bias=False)
  (1): Linear(in_features=1848, out_features=4096, bias=False)
)
2025-04-24 23:05:32,115 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1848, bias=False)
  (1): Linear(in_features=1848, out_features=4096, bias=False)
)')
2025-04-24 23:05:32,115 - INFO - Replacing 'model.layers.19.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  61%|█████████████████████████████▊                   | 137/225 [36:00<23:14, 15.85s/it]2025-04-24 23:05:32,116 - INFO - Skipping layer model.layers.19.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:05:32,116 - INFO - Skipping layer model.layers.19.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:05:32,116 - INFO - Skipping layer model.layers.19.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:05:32,116 - INFO - Layer: model.layers.20.self_attn.q_proj | Ratio: 0.200 -> Target Rank: 416 (Align: 8)
2025-04-24 23:05:32,116 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_20_self_attn_q_proj.safetensors
2025-04-24 23:05:32,116 - INFO - exists: True
2025-04-24 23:05:32,132 - INFO - factorize_layer_kron_svd
2025-04-24 23:05:33,601 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:05:34,826 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:05:36,208 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_20_self_attn_q_proj.safetensors True
Layer: model.layers.20.self_attn.q_proj | Ratio: 0.200 -> Target Rank: 416 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [8.0140831e-04 6.1302264e-05 5.0301820e-05 3.8353595e-05 2.8814045e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (416,4096), (4096,416)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=416, bias=False)
  (1): Linear(in_features=416, out_features=4096, bias=False)
)
2025-04-24 23:05:59,826 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=416, bias=False)
  (1): Linear(in_features=416, out_features=4096, bias=False)
)')
2025-04-24 23:05:59,826 - INFO - Replacing 'model.layers.20.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  63%|██████████████████████████████▋                  | 141/225 [36:28<16:47, 12.00s/it]2025-04-24 23:05:59,826 - INFO - Layer: model.layers.20.self_attn.k_proj | Ratio: 0.100 -> Target Rank: 208 (Align: 8)
2025-04-24 23:05:59,827 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_20_self_attn_k_proj.safetensors
2025-04-24 23:05:59,827 - INFO - exists: True
2025-04-24 23:05:59,835 - INFO - factorize_layer_kron_svd
2025-04-24 23:06:01,298 - INFO -   Factor is positive definite (alpha=1.00e-05)
2025-04-24 23:06:02,636 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_20_self_attn_k_proj.safetensors True
Layer: model.layers.20.self_attn.k_proj | Ratio: 0.100 -> Target Rank: 208 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Factor is positive definite (alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.0406852e-03 5.4494500e-05 4.0110652e-05 2.9760553e-05 2.5743462e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (208,4096), (4096,208)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=208, bias=False)
  (1): Linear(in_features=208, out_features=4096, bias=False)
)
2025-04-24 23:06:24,438 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=208, bias=False)
  (1): Linear(in_features=208, out_features=4096, bias=False)
)')
2025-04-24 23:06:24,438 - INFO - Replacing 'model.layers.20.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  63%|██████████████████████████████▉                  | 142/225 [36:52<18:55, 13.68s/it]2025-04-24 23:06:24,439 - INFO - Layer: model.layers.20.self_attn.v_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
2025-04-24 23:06:24,440 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_20_self_attn_v_proj.safetensors
2025-04-24 23:06:24,440 - INFO - exists: True
2025-04-24 23:06:24,450 - INFO - factorize_layer_kron_svd
2025-04-24 23:06:25,702 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:06:26,905 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:06:28,003 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:06:29,169 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_20_self_attn_v_proj.safetensors True
Layer: model.layers.20.self_attn.v_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [9.2318274e-05 2.0415315e-05 1.2823558e-05 8.6249993e-06 6.7299879e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1640,4096), (4096,1640)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)
2025-04-24 23:06:58,304 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)')
2025-04-24 23:06:58,304 - INFO - Replacing 'model.layers.20.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)
Compressing Layers:  64%|███████████████████████████████▏                 | 143/225 [37:26<23:07, 16.92s/it]2025-04-24 23:06:58,304 - INFO - Layer: model.layers.20.self_attn.o_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 23:06:58,305 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_20_self_attn_o_proj.safetensors
2025-04-24 23:06:58,305 - INFO - exists: True
2025-04-24 23:06:58,323 - INFO - factorize_layer_kron_svd
2025-04-24 23:06:59,472 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:07:00,567 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:07:01,668 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 23:07:02,792 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 23:07:03,938 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 23:07:05,169 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-24 23:07:06,279 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:07:07,375 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:07:08,458 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 23:07:09,613 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 23:07:10,826 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 23:07:12,139 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_20_self_attn_o_proj.safetensors True
Layer: model.layers.20.self_attn.o_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [6.5827680e-06 2.8515849e-06 2.6935977e-06 2.5656159e-06 2.4738813e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1440,4096), (4096,1440)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)
2025-04-24 23:07:40,567 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)')
2025-04-24 23:07:40,567 - INFO - Replacing 'model.layers.20.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  64%|███████████████████████████████▎                 | 144/225 [38:08<29:12, 21.64s/it]2025-04-24 23:07:40,568 - INFO - Skipping layer model.layers.20.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:07:40,568 - INFO - Skipping layer model.layers.20.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:07:40,568 - INFO - Skipping layer model.layers.20.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:07:40,568 - INFO - Layer: model.layers.21.self_attn.q_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
2025-04-24 23:07:40,568 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_21_self_attn_q_proj.safetensors
2025-04-24 23:07:40,568 - INFO - exists: True
2025-04-24 23:07:40,581 - INFO - factorize_layer_kron_svd
2025-04-24 23:07:41,796 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:07:43,199 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:07:44,553 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_21_self_attn_q_proj.safetensors True
Layer: model.layers.21.self_attn.q_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [7.1362458e-04 7.4350537e-05 3.7766848e-05 2.4498802e-05 2.1550906e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)
2025-04-24 23:08:10,097 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)')
2025-04-24 23:08:10,097 - INFO - Replacing 'model.layers.21.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  66%|████████████████████████████████▏                | 148/225 [38:38<18:20, 14.29s/it]2025-04-24 23:08:10,097 - INFO - Layer: model.layers.21.self_attn.k_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 23:08:10,098 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_21_self_attn_k_proj.safetensors
2025-04-24 23:08:10,098 - INFO - exists: True
2025-04-24 23:08:10,110 - INFO - factorize_layer_kron_svd
2025-04-24 23:08:11,449 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:08:12,834 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:08:14,178 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:08:15,400 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_21_self_attn_k_proj.safetensors True
Layer: model.layers.21.self_attn.k_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [2.5919633e-04 9.5758223e-05 7.8373805e-06 6.2292274e-06 5.4841680e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)
2025-04-24 23:08:36,737 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)')
2025-04-24 23:08:36,737 - INFO - Replacing 'model.layers.21.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  66%|████████████████████████████████▍                | 149/225 [39:05<20:31, 16.21s/it]2025-04-24 23:08:36,737 - INFO - Layer: model.layers.21.self_attn.v_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 23:08:36,738 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_21_self_attn_v_proj.safetensors
2025-04-24 23:08:36,738 - INFO - exists: True
2025-04-24 23:08:36,743 - INFO - factorize_layer_kron_svd
2025-04-24 23:08:38,050 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:08:39,396 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:08:40,581 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:08:41,800 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_21_self_attn_v_proj.safetensors True
Layer: model.layers.21.self_attn.v_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.3663064e-04 1.4290518e-05 9.3747267e-06 6.9539688e-06 4.9605360e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1440,4096), (4096,1440)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)
2025-04-24 23:09:05,220 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)')
2025-04-24 23:09:05,221 - INFO - Replacing 'model.layers.21.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)
Compressing Layers:  67%|████████████████████████████████▋                | 150/225 [39:33<23:02, 18.44s/it]2025-04-24 23:09:05,221 - INFO - Layer: model.layers.21.self_attn.o_proj | Ratio: 0.900 -> Target Rank: 1848 (Align: 8)
2025-04-24 23:09:05,222 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_21_self_attn_o_proj.safetensors
2025-04-24 23:09:05,222 - INFO - exists: True
2025-04-24 23:09:05,230 - INFO - factorize_layer_kron_svd
2025-04-24 23:09:06,460 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:09:07,605 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:09:08,758 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 23:09:09,826 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 23:09:10,964 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 23:09:12,191 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-24 23:09:13,301 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:09:14,433 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:09:15,516 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 23:09:16,618 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 23:09:17,745 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 23:09:18,928 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_21_self_attn_o_proj.safetensors True
Layer: model.layers.21.self_attn.o_proj | Ratio: 0.900 -> Target Rank: 1848 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [5.6280055e-06 2.9603489e-06 2.6979048e-06 2.4875860e-06 2.4380076e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1848,4096), (4096,1848)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1848, bias=False)
  (1): Linear(in_features=1848, out_features=4096, bias=False)
)
2025-04-24 23:09:55,007 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1848, bias=False)
  (1): Linear(in_features=1848, out_features=4096, bias=False)
)')
2025-04-24 23:09:55,008 - INFO - Replacing 'model.layers.21.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  67%|████████████████████████████████▉                | 151/225 [40:23<30:42, 24.90s/it]2025-04-24 23:09:55,008 - INFO - Skipping layer model.layers.21.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:09:55,008 - INFO - Skipping layer model.layers.21.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:09:55,008 - INFO - Skipping layer model.layers.21.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:09:55,008 - INFO - Layer: model.layers.22.self_attn.q_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 23:09:55,009 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_22_self_attn_q_proj.safetensors
2025-04-24 23:09:55,009 - INFO - exists: True
2025-04-24 23:09:55,027 - INFO - factorize_layer_kron_svd
2025-04-24 23:09:56,296 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:09:57,616 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:09:58,829 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_22_self_attn_q_proj.safetensors True
Layer: model.layers.22.self_attn.q_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [7.1551203e-04 5.6577930e-05 3.6369638e-05 2.8548533e-05 2.6242335e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1440,4096), (4096,1440)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)
2025-04-24 23:10:36,961 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)')
2025-04-24 23:10:36,961 - INFO - Replacing 'model.layers.22.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  69%|█████████████████████████████████▊               | 155/225 [41:05<19:57, 17.11s/it]2025-04-24 23:10:36,961 - INFO - Layer: model.layers.22.self_attn.k_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
2025-04-24 23:10:36,962 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_22_self_attn_k_proj.safetensors
2025-04-24 23:10:36,962 - INFO - exists: True
2025-04-24 23:10:36,971 - INFO - factorize_layer_kron_svd
2025-04-24 23:10:38,223 - INFO -   Factor is positive definite (alpha=1.00e-05)
2025-04-24 23:10:39,419 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_22_self_attn_k_proj.safetensors True
Layer: model.layers.22.self_attn.k_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Factor is positive definite (alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [8.8471733e-04 3.7389949e-05 3.0549065e-05 2.7955735e-05 2.3029401e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)
2025-04-24 23:11:09,180 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)')
2025-04-24 23:11:09,180 - INFO - Replacing 'model.layers.22.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  69%|█████████████████████████████████▉               | 156/225 [41:37<22:29, 19.55s/it]2025-04-24 23:11:09,180 - INFO - Skipping layer model.layers.22.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:11:09,180 - INFO - Layer: model.layers.22.self_attn.o_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
2025-04-24 23:11:09,183 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_22_self_attn_o_proj.safetensors
2025-04-24 23:11:09,183 - INFO - exists: True
2025-04-24 23:11:09,209 - INFO - factorize_layer_kron_svd
2025-04-24 23:11:10,403 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:11:11,605 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:11:12,864 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-24 23:11:14,062 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:11:15,306 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_22_self_attn_o_proj.safetensors True
Layer: model.layers.22.self_attn.o_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.4139143e-05 4.4393823e-06 1.9341273e-06 8.7018583e-07 8.0446426e-07]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1640,4096), (4096,1640)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)
2025-04-24 23:11:44,410 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)')
2025-04-24 23:11:44,410 - INFO - Replacing 'model.layers.22.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  70%|██████████████████████████████████▍              | 158/225 [42:12<21:08, 18.94s/it]2025-04-24 23:11:44,410 - INFO - Skipping layer model.layers.22.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:11:44,410 - INFO - Skipping layer model.layers.22.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:11:44,410 - INFO - Skipping layer model.layers.22.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:11:44,410 - INFO - Layer: model.layers.23.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 23:11:44,410 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_23_self_attn_q_proj.safetensors
2025-04-24 23:11:44,410 - INFO - exists: True
2025-04-24 23:11:44,439 - INFO - factorize_layer_kron_svd
2025-04-24 23:11:45,934 - INFO -   Factor is positive definite (alpha=1.00e-05)
2025-04-24 23:11:47,375 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_23_self_attn_q_proj.safetensors True
Layer: model.layers.23.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Factor is positive definite (alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [7.4219279e-04 3.7877617e-05 2.6261388e-05 2.2600490e-05 2.1597491e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)
2025-04-24 23:12:11,610 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)')
2025-04-24 23:12:11,610 - INFO - Replacing 'model.layers.23.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  72%|███████████████████████████████████▎             | 162/225 [42:39<13:50, 13.18s/it]2025-04-24 23:12:11,610 - INFO - Layer: model.layers.23.self_attn.k_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 23:12:11,612 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_23_self_attn_k_proj.safetensors
2025-04-24 23:12:11,612 - INFO - exists: True
2025-04-24 23:12:11,638 - INFO - factorize_layer_kron_svd
2025-04-24 23:12:12,886 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:12:14,376 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:12:15,762 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_23_self_attn_k_proj.safetensors True
Layer: model.layers.23.self_attn.k_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [8.7510236e-04 3.6314970e-05 2.5059404e-05 2.1486116e-05 2.0177757e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)
2025-04-24 23:12:41,612 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)')
2025-04-24 23:12:41,612 - INFO - Replacing 'model.layers.23.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  72%|███████████████████████████████████▍             | 163/225 [43:09<16:08, 15.62s/it]2025-04-24 23:12:41,612 - INFO - Layer: model.layers.23.self_attn.v_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
2025-04-24 23:12:41,613 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_23_self_attn_v_proj.safetensors
2025-04-24 23:12:41,613 - INFO - exists: True
2025-04-24 23:12:41,631 - INFO - factorize_layer_kron_svd
2025-04-24 23:12:42,818 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:12:44,049 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:12:45,214 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:12:46,586 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_23_self_attn_v_proj.safetensors True
Layer: model.layers.23.self_attn.v_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [5.7369165e-05 1.2404687e-05 8.0052805e-06 5.7160964e-06 4.7268454e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1640,4096), (4096,1640)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)
2025-04-24 23:13:10,288 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)')
2025-04-24 23:13:10,289 - INFO - Replacing 'model.layers.23.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)
Compressing Layers:  73%|███████████████████████████████████▋             | 164/225 [43:38<18:09, 17.86s/it]2025-04-24 23:13:10,289 - INFO - Layer: model.layers.23.self_attn.o_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
2025-04-24 23:13:10,289 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_23_self_attn_o_proj.safetensors
2025-04-24 23:13:10,289 - INFO - exists: True
2025-04-24 23:13:10,301 - INFO - factorize_layer_kron_svd
2025-04-24 23:13:11,451 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:13:12,634 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:13:13,838 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-24 23:13:14,981 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:13:16,436 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_23_self_attn_o_proj.safetensors True
Layer: model.layers.23.self_attn.o_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [2.7224842e-05 1.7550238e-06 1.0574929e-06 7.5971263e-07 6.0233413e-07]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1640,4096), (4096,1640)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)
2025-04-24 23:13:40,045 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)')
2025-04-24 23:13:40,045 - INFO - Replacing 'model.layers.23.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  73%|███████████████████████████████████▉             | 165/225 [44:08<20:11, 20.20s/it]2025-04-24 23:13:40,046 - INFO - Skipping layer model.layers.23.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:13:40,046 - INFO - Layer: model.layers.23.mlp.up_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
2025-04-24 23:13:40,050 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_23_mlp_up_proj.safetensors
2025-04-24 23:13:40,050 - INFO - exists: True
2025-04-24 23:13:40,065 - INFO - factorize_layer_kron_svd
2025-04-24 23:13:41,416 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:13:42,632 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:13:44,088 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-24 23:13:47,038 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:13:51,297 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_23_mlp_up_proj.safetensors True
Layer: model.layers.23.mlp.up_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [3.0700947e-05 1.0850134e-05 3.0478623e-06 2.7577912e-06 1.0493320e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2688,4096), (11008,2688)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2688, bias=False)
  (1): Linear(in_features=2688, out_features=11008, bias=False)
)
2025-04-24 23:14:53,849 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2688, bias=False)
  (1): Linear(in_features=2688, out_features=11008, bias=False)
)')
2025-04-24 23:14:53,849 - INFO - Replacing 'model.layers.23.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  74%|████████████████████████████████████▎            | 167/225 [45:22<25:20, 26.21s/it]2025-04-24 23:14:53,849 - INFO - Skipping layer model.layers.23.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:14:53,849 - INFO - Layer: model.layers.24.self_attn.q_proj | Ratio: 0.100 -> Target Rank: 208 (Align: 8)
2025-04-24 23:14:53,850 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_24_self_attn_q_proj.safetensors
2025-04-24 23:14:53,850 - INFO - exists: True
2025-04-24 23:14:53,891 - INFO - factorize_layer_kron_svd
2025-04-24 23:14:55,109 - INFO -   Factor is positive definite (alpha=1.00e-05)
2025-04-24 23:14:56,292 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_24_self_attn_q_proj.safetensors True
Layer: model.layers.24.self_attn.q_proj | Ratio: 0.100 -> Target Rank: 208 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Factor is positive definite (alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [4.7672872e-04 6.1251303e-05 3.5211855e-05 2.2390554e-05 2.0775149e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (208,4096), (4096,208)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=208, bias=False)
  (1): Linear(in_features=208, out_features=4096, bias=False)
)
2025-04-24 23:15:27,778 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=208, bias=False)
  (1): Linear(in_features=208, out_features=4096, bias=False)
)')
2025-04-24 23:15:27,778 - INFO - Replacing 'model.layers.24.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  75%|████████████████████████████████████▊            | 169/225 [45:56<21:31, 23.07s/it]2025-04-24 23:15:27,779 - INFO - Layer: model.layers.24.self_attn.k_proj | Ratio: 0.100 -> Target Rank: 208 (Align: 8)
2025-04-24 23:15:27,779 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_24_self_attn_k_proj.safetensors
2025-04-24 23:15:27,779 - INFO - exists: True
2025-04-24 23:15:27,788 - INFO - factorize_layer_kron_svd
2025-04-24 23:15:29,287 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:15:30,539 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:15:31,735 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_24_self_attn_k_proj.safetensors True
Layer: model.layers.24.self_attn.k_proj | Ratio: 0.100 -> Target Rank: 208 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [5.5969006e-04 4.6440331e-05 2.8826469e-05 1.8546911e-05 1.6199194e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (208,4096), (4096,208)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=208, bias=False)
  (1): Linear(in_features=208, out_features=4096, bias=False)
)
2025-04-24 23:15:56,347 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=208, bias=False)
  (1): Linear(in_features=208, out_features=4096, bias=False)
)')
2025-04-24 23:15:56,347 - INFO - Replacing 'model.layers.24.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  76%|█████████████████████████████████████            | 170/225 [46:24<22:07, 24.14s/it]2025-04-24 23:15:56,347 - INFO - Layer: model.layers.24.self_attn.v_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 23:15:56,347 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_24_self_attn_v_proj.safetensors
2025-04-24 23:15:56,347 - INFO - exists: True
2025-04-24 23:15:56,354 - INFO - factorize_layer_kron_svd
2025-04-24 23:15:57,786 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:15:59,316 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:16:00,507 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:16:01,961 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_24_self_attn_v_proj.safetensors True
Layer: model.layers.24.self_attn.v_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [6.32314041e-05 1.17357795e-05 8.59317061e-06 5.70959173e-06
 4.66277879e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1440,4096), (4096,1440)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)
2025-04-24 23:16:33,782 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)')
2025-04-24 23:16:33,782 - INFO - Replacing 'model.layers.24.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)
Compressing Layers:  76%|█████████████████████████████████████▏           | 171/225 [47:02<24:20, 27.04s/it]2025-04-24 23:16:33,783 - INFO - Layer: model.layers.24.self_attn.o_proj | Ratio: 0.400 -> Target Rank: 824 (Align: 8)
2025-04-24 23:16:33,783 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_24_self_attn_o_proj.safetensors
2025-04-24 23:16:33,784 - INFO - exists: True
2025-04-24 23:16:33,795 - INFO - factorize_layer_kron_svd
2025-04-24 23:16:35,350 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:16:36,577 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:16:37,807 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:16:39,046 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_24_self_attn_o_proj.safetensors True
Layer: model.layers.24.self_attn.o_proj | Ratio: 0.400 -> Target Rank: 824 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [3.2492968e-05 1.0363568e-05 1.5397670e-06 1.1765046e-06 8.8089394e-07]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (824,4096), (4096,824)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=824, bias=False)
  (1): Linear(in_features=824, out_features=4096, bias=False)
)
2025-04-24 23:17:09,731 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=824, bias=False)
  (1): Linear(in_features=824, out_features=4096, bias=False)
)')
2025-04-24 23:17:09,731 - INFO - Replacing 'model.layers.24.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  76%|█████████████████████████████████████▍           | 172/225 [47:38<25:45, 29.16s/it]2025-04-24 23:17:09,731 - INFO - Layer: model.layers.24.mlp.gate_proj | Ratio: 0.700 -> Target Rank: 2096 (Align: 8)
2025-04-24 23:17:09,731 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_24_mlp_gate_proj.safetensors
2025-04-24 23:17:09,731 - INFO - exists: True
2025-04-24 23:17:09,741 - INFO - factorize_layer_kron_svd
2025-04-24 23:17:11,303 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:17:12,710 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:17:14,140 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-24 23:17:16,694 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:17:20,060 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_24_mlp_gate_proj.safetensors True
Layer: model.layers.24.mlp.gate_proj | Ratio: 0.700 -> Target Rank: 2096 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [4.4121687e-05 1.3112584e-05 3.2457190e-06 2.4400804e-06 1.0232656e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2096,4096), (11008,2096)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2096, bias=False)
  (1): Linear(in_features=2096, out_features=11008, bias=False)
)
2025-04-24 23:18:19,161 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2096, bias=False)
  (1): Linear(in_features=2096, out_features=11008, bias=False)
)')
2025-04-24 23:18:19,161 - INFO - Replacing 'model.layers.24.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  77%|█████████████████████████████████████▋           | 173/225 [48:47<34:06, 39.36s/it]2025-04-24 23:18:19,161 - INFO - Layer: model.layers.24.mlp.up_proj | Ratio: 0.700 -> Target Rank: 2096 (Align: 8)
2025-04-24 23:18:19,162 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_24_mlp_up_proj.safetensors
2025-04-24 23:18:19,162 - INFO - exists: True
2025-04-24 23:18:19,189 - INFO - factorize_layer_kron_svd
2025-04-24 23:18:20,684 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:18:21,863 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:18:23,254 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-24 23:18:25,364 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:18:28,800 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_24_mlp_up_proj.safetensors True
Layer: model.layers.24.mlp.up_proj | Ratio: 0.700 -> Target Rank: 2096 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [3.5628200e-05 5.7007760e-06 3.7646712e-06 2.6412317e-06 1.1728241e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2096,4096), (11008,2096)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2096, bias=False)
  (1): Linear(in_features=2096, out_features=11008, bias=False)
)
2025-04-24 23:19:27,128 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2096, bias=False)
  (1): Linear(in_features=2096, out_features=11008, bias=False)
)')
2025-04-24 23:19:27,128 - INFO - Replacing 'model.layers.24.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  77%|█████████████████████████████████████▉           | 174/225 [49:55<39:55, 46.96s/it]2025-04-24 23:19:27,128 - INFO - Layer: model.layers.24.mlp.down_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
2025-04-24 23:19:27,130 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_24_mlp_down_proj.safetensors
2025-04-24 23:19:27,130 - INFO - exists: True
2025-04-24 23:19:27,149 - INFO - factorize_layer_kron_svd
2025-04-24 23:19:30,322 - INFO -   Factor is positive definite (alpha=1.00e-05)
2025-04-24 23:19:31,467 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_24_mlp_down_proj.safetensors True
Layer: model.layers.24.mlp.down_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Factor is positive definite (alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [7.60391820e-04 1.16251285e-04 8.95536141e-05 6.90299203e-05
 6.11170253e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2688,11008), (4096,2688)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=2688, bias=False)
  (1): Linear(in_features=2688, out_features=4096, bias=False)
)
2025-04-24 23:20:44,124 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=2688, bias=False)
  (1): Linear(in_features=2688, out_features=4096, bias=False)
)')
2025-04-24 23:20:44,125 - INFO - Replacing 'model.layers.24.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  78%|██████████████████████████████████████           | 175/225 [51:12<46:01, 55.23s/it]2025-04-24 23:20:44,125 - INFO - Layer: model.layers.25.self_attn.q_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
2025-04-24 23:20:44,126 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_25_self_attn_q_proj.safetensors
2025-04-24 23:20:44,126 - INFO - exists: True
2025-04-24 23:20:44,164 - INFO - factorize_layer_kron_svd
2025-04-24 23:20:45,410 - INFO -   Factor is positive definite (alpha=1.00e-05)
2025-04-24 23:20:46,563 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_25_self_attn_q_proj.safetensors True
Layer: model.layers.25.self_attn.q_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Factor is positive definite (alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [7.3310739e-04 4.5168257e-05 2.6283642e-05 2.2007003e-05 1.9362484e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1640,4096), (4096,1640)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)
2025-04-24 23:21:15,221 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)')
2025-04-24 23:21:15,221 - INFO - Replacing 'model.layers.25.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  78%|██████████████████████████████████████▎          | 176/225 [51:43<39:32, 48.42s/it]2025-04-24 23:21:15,221 - INFO - Skipping layer model.layers.25.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:21:15,221 - INFO - Skipping layer model.layers.25.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:21:15,221 - INFO - Layer: model.layers.25.self_attn.o_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 23:21:15,222 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_25_self_attn_o_proj.safetensors
2025-04-24 23:21:15,222 - INFO - exists: True
2025-04-24 23:21:15,237 - INFO - factorize_layer_kron_svd
2025-04-24 23:21:16,382 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:21:17,576 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:21:18,901 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-24 23:21:20,206 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:21:21,854 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_25_self_attn_o_proj.safetensors True
Layer: model.layers.25.self_attn.o_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.5150558e-05 2.7744670e-06 7.5447014e-07 4.9762866e-07 4.8306060e-07]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1440,4096), (4096,1440)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)
2025-04-24 23:21:54,085 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)')
2025-04-24 23:21:54,085 - INFO - Replacing 'model.layers.25.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  80%|██████████████████████████████████████▉          | 179/225 [52:22<22:14, 29.01s/it]2025-04-24 23:21:54,085 - INFO - Layer: model.layers.25.mlp.gate_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
2025-04-24 23:21:54,086 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_25_mlp_gate_proj.safetensors
2025-04-24 23:21:54,086 - INFO - exists: True
2025-04-24 23:21:54,092 - INFO - factorize_layer_kron_svd
2025-04-24 23:21:55,680 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:21:57,169 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:21:58,631 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-24 23:22:01,311 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:22:05,733 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_25_mlp_gate_proj.safetensors True
Layer: model.layers.25.mlp.gate_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [5.2119361e-05 1.5998472e-05 4.2029083e-06 1.9503864e-06 1.1962488e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2688,4096), (11008,2688)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2688, bias=False)
  (1): Linear(in_features=2688, out_features=11008, bias=False)
)
2025-04-24 23:23:02,697 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2688, bias=False)
  (1): Linear(in_features=2688, out_features=11008, bias=False)
)')
2025-04-24 23:23:02,697 - INFO - Replacing 'model.layers.25.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  80%|███████████████████████████████████████▏         | 180/225 [53:31<27:53, 37.20s/it]2025-04-24 23:23:02,698 - INFO - Layer: model.layers.25.mlp.up_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
2025-04-24 23:23:02,698 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_25_mlp_up_proj.safetensors
2025-04-24 23:23:02,698 - INFO - exists: True
2025-04-24 23:23:02,735 - INFO - factorize_layer_kron_svd
2025-04-24 23:23:04,182 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:23:05,548 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:23:06,933 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-24 23:23:09,638 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:23:13,471 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_25_mlp_up_proj.safetensors True
Layer: model.layers.25.mlp.up_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [3.1324053e-05 1.3509146e-05 3.6688521e-06 2.5376346e-06 1.6996315e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2688,4096), (11008,2688)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2688, bias=False)
  (1): Linear(in_features=2688, out_features=11008, bias=False)
)
2025-04-24 23:24:10,848 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2688, bias=False)
  (1): Linear(in_features=2688, out_features=11008, bias=False)
)')
2025-04-24 23:24:10,848 - INFO - Replacing 'model.layers.25.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  80%|███████████████████████████████████████▍         | 181/225 [54:39<32:27, 44.25s/it]2025-04-24 23:24:10,849 - INFO - Skipping layer model.layers.25.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:24:10,849 - INFO - Layer: model.layers.26.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 23:24:10,850 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_26_self_attn_q_proj.safetensors
2025-04-24 23:24:10,850 - INFO - exists: True
2025-04-24 23:24:10,891 - INFO - factorize_layer_kron_svd
2025-04-24 23:24:12,352 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:24:13,778 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:24:15,211 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_26_self_attn_q_proj.safetensors True
Layer: model.layers.26.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [6.1266200e-04 5.1251598e-05 3.0445932e-05 2.3706018e-05 2.2058506e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1024,4096), (4096,1024)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)
2025-04-24 23:24:45,883 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1024, bias=False)
  (1): Linear(in_features=1024, out_features=4096, bias=False)
)')
2025-04-24 23:24:45,883 - INFO - Replacing 'model.layers.26.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  81%|███████████████████████████████████████▊         | 183/225 [55:14<23:35, 33.71s/it]2025-04-24 23:24:45,883 - INFO - Layer: model.layers.26.self_attn.k_proj | Ratio: 0.400 -> Target Rank: 824 (Align: 8)
2025-04-24 23:24:45,884 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_26_self_attn_k_proj.safetensors
2025-04-24 23:24:45,884 - INFO - exists: True
2025-04-24 23:24:45,900 - INFO - factorize_layer_kron_svd
2025-04-24 23:24:47,193 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:24:48,480 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:24:49,723 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:24:51,028 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_26_self_attn_k_proj.safetensors True
Layer: model.layers.26.self_attn.k_proj | Ratio: 0.400 -> Target Rank: 824 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [8.2989095e-04 3.5860310e-05 2.9987032e-05 1.9561858e-05 1.6020722e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (824,4096), (4096,824)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=824, bias=False)
  (1): Linear(in_features=824, out_features=4096, bias=False)
)
2025-04-24 23:25:18,618 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=824, bias=False)
  (1): Linear(in_features=824, out_features=4096, bias=False)
)')
2025-04-24 23:25:18,618 - INFO - Replacing 'model.layers.26.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  82%|████████████████████████████████████████         | 184/225 [55:46<22:53, 33.49s/it]2025-04-24 23:25:18,618 - INFO - Skipping layer model.layers.26.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:25:18,618 - INFO - Layer: model.layers.26.self_attn.o_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
2025-04-24 23:25:18,620 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_26_self_attn_o_proj.safetensors
2025-04-24 23:25:18,620 - INFO - exists: True
2025-04-24 23:25:18,630 - INFO - factorize_layer_kron_svd
2025-04-24 23:25:20,007 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:25:21,203 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:25:22,475 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-24 23:25:23,679 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_26_self_attn_o_proj.safetensors True
Layer: model.layers.26.self_attn.o_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [2.3415269e-04 1.1255141e-05 1.0018451e-05 7.2952666e-06 5.6233293e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)
2025-04-24 23:25:52,194 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)')
2025-04-24 23:25:52,194 - INFO - Replacing 'model.layers.26.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  83%|████████████████████████████████████████▌        | 186/225 [56:20<17:34, 27.05s/it]2025-04-24 23:25:52,194 - INFO - Skipping layer model.layers.26.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:25:52,194 - INFO - Skipping layer model.layers.26.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:25:52,194 - INFO - Skipping layer model.layers.26.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:25:52,194 - INFO - Layer: model.layers.27.self_attn.q_proj | Ratio: 0.400 -> Target Rank: 824 (Align: 8)
2025-04-24 23:25:52,195 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_27_self_attn_q_proj.safetensors
2025-04-24 23:25:52,195 - INFO - exists: True
2025-04-24 23:25:52,207 - INFO - factorize_layer_kron_svd
2025-04-24 23:25:53,430 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:25:54,674 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:25:55,949 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-24 23:25:57,102 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:25:58,356 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_27_self_attn_q_proj.safetensors True
Layer: model.layers.27.self_attn.q_proj | Ratio: 0.400 -> Target Rank: 824 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [6.2065467e-04 3.6923717e-05 1.8709072e-05 1.2459362e-05 1.2122801e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (824,4096), (4096,824)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=824, bias=False)
  (1): Linear(in_features=824, out_features=4096, bias=False)
)
2025-04-24 23:26:26,332 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=824, bias=False)
  (1): Linear(in_features=824, out_features=4096, bias=False)
)')
2025-04-24 23:26:26,332 - INFO - Replacing 'model.layers.27.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  84%|█████████████████████████████████████████▍       | 190/225 [56:54<10:06, 17.34s/it]2025-04-24 23:26:26,332 - INFO - Layer: model.layers.27.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
2025-04-24 23:26:26,333 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_27_self_attn_k_proj.safetensors
2025-04-24 23:26:26,333 - INFO - exists: True
2025-04-24 23:26:26,338 - INFO - factorize_layer_kron_svd
2025-04-24 23:26:27,557 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:26:28,714 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:26:29,936 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-24 23:26:31,056 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:26:32,279 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_27_self_attn_k_proj.safetensors True
Layer: model.layers.27.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [8.5674645e-04 2.3621626e-05 1.5614280e-05 1.0000707e-05 9.1756147e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (616,4096), (4096,616)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)
2025-04-24 23:26:54,088 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=616, bias=False)
  (1): Linear(in_features=616, out_features=4096, bias=False)
)')
2025-04-24 23:26:54,089 - INFO - Replacing 'model.layers.27.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  85%|█████████████████████████████████████████▌       | 191/225 [57:22<10:45, 18.98s/it]2025-04-24 23:26:54,089 - INFO - Layer: model.layers.27.self_attn.v_proj | Ratio: 0.900 -> Target Rank: 1848 (Align: 8)
2025-04-24 23:26:54,090 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_27_self_attn_v_proj.safetensors
2025-04-24 23:26:54,090 - INFO - exists: True
2025-04-24 23:26:54,122 - INFO - factorize_layer_kron_svd
2025-04-24 23:26:55,360 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:26:56,614 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:26:57,699 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:26:58,927 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_27_self_attn_v_proj.safetensors True
Layer: model.layers.27.self_attn.v_proj | Ratio: 0.900 -> Target Rank: 1848 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.2262762e-04 1.2386496e-05 8.2811475e-06 5.6460581e-06 5.2539558e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1848,4096), (4096,1848)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1848, bias=False)
  (1): Linear(in_features=1848, out_features=4096, bias=False)
)
2025-04-24 23:27:27,610 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1848, bias=False)
  (1): Linear(in_features=1848, out_features=4096, bias=False)
)')
2025-04-24 23:27:27,610 - INFO - Replacing 'model.layers.27.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)
Compressing Layers:  85%|█████████████████████████████████████████▊       | 192/225 [57:55<11:54, 21.66s/it]2025-04-24 23:27:27,611 - INFO - Layer: model.layers.27.self_attn.o_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
2025-04-24 23:27:27,612 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_27_self_attn_o_proj.safetensors
2025-04-24 23:27:27,612 - INFO - exists: True
2025-04-24 23:27:27,625 - INFO - factorize_layer_kron_svd
2025-04-24 23:27:28,667 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:27:29,721 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:27:30,783 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 23:27:31,830 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 23:27:32,883 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 23:27:34,109 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-24 23:27:35,157 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:27:36,216 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:27:37,266 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 23:27:38,339 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 23:27:39,447 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 23:27:40,704 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_27_self_attn_o_proj.safetensors True
Layer: model.layers.27.self_attn.o_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [5.6467215e-06 3.7391610e-06 3.0175340e-06 2.9309579e-06 2.8120503e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1640,4096), (4096,1640)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)
2025-04-24 23:28:07,207 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1640, bias=False)
  (1): Linear(in_features=1640, out_features=4096, bias=False)
)')
2025-04-24 23:28:07,207 - INFO - Replacing 'model.layers.27.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  86%|██████████████████████████████████████████       | 193/225 [58:35<13:32, 25.39s/it]2025-04-24 23:28:07,207 - INFO - Skipping layer model.layers.27.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:28:07,207 - INFO - Skipping layer model.layers.27.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:28:07,207 - INFO - Skipping layer model.layers.27.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:28:07,207 - INFO - Layer: model.layers.28.self_attn.q_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
2025-04-24 23:28:07,208 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_28_self_attn_q_proj.safetensors
2025-04-24 23:28:07,208 - INFO - exists: True
2025-04-24 23:28:07,218 - INFO - factorize_layer_kron_svd
2025-04-24 23:28:08,406 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:28:09,675 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:28:10,794 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:28:12,084 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_28_self_attn_q_proj.safetensors True
Layer: model.layers.28.self_attn.q_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.3855560e-03 3.4289329e-05 2.7094886e-05 1.8852646e-05 1.6840988e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)
2025-04-24 23:28:41,065 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)')
2025-04-24 23:28:41,066 - INFO - Replacing 'model.layers.28.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  88%|██████████████████████████████████████████▉      | 197/225 [59:09<07:33, 16.20s/it]2025-04-24 23:28:41,066 - INFO - Layer: model.layers.28.self_attn.k_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 23:28:41,067 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_28_self_attn_k_proj.safetensors
2025-04-24 23:28:41,067 - INFO - exists: True
2025-04-24 23:28:41,078 - INFO - factorize_layer_kron_svd
2025-04-24 23:28:42,256 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:28:43,535 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:28:44,660 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:28:45,930 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_28_self_attn_k_proj.safetensors True
Layer: model.layers.28.self_attn.k_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [1.9324456e-03 3.1512136e-05 2.3065544e-05 1.8104505e-05 1.3718930e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1440,4096), (4096,1440)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)
2025-04-24 23:29:15,467 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)')
2025-04-24 23:29:15,467 - INFO - Replacing 'model.layers.28.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  88%|███████████████████████████████████████████      | 198/225 [59:43<08:37, 19.15s/it]2025-04-24 23:29:15,468 - INFO - Skipping layer model.layers.28.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:29:15,468 - INFO - Layer: model.layers.28.self_attn.o_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 23:29:15,469 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_28_self_attn_o_proj.safetensors
2025-04-24 23:29:15,469 - INFO - exists: True
2025-04-24 23:29:15,480 - INFO - factorize_layer_kron_svd
2025-04-24 23:29:16,584 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:29:17,654 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:29:18,723 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 23:29:19,839 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 23:29:20,932 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 23:29:22,171 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-24 23:29:23,259 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:29:24,327 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:29:25,397 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 23:29:26,469 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 23:29:27,586 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 23:29:28,836 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_28_self_attn_o_proj.safetensors True
Layer: model.layers.28.self_attn.o_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [6.0813322e-06 3.6334923e-06 3.2903934e-06 3.1669369e-06 3.0863382e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1440,4096), (4096,1440)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)
2025-04-24 23:29:58,447 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)')
2025-04-24 23:29:58,447 - INFO - Replacing 'model.layers.28.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  89%|█████████████████████████████████████████▊     | 200/225 [1:00:26<08:17, 19.90s/it]2025-04-24 23:29:58,448 - INFO - Layer: model.layers.28.mlp.gate_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
2025-04-24 23:29:58,449 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_28_mlp_gate_proj.safetensors
2025-04-24 23:29:58,449 - INFO - exists: True
2025-04-24 23:29:58,454 - INFO - factorize_layer_kron_svd
2025-04-24 23:29:59,639 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:30:00,718 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:30:01,788 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 23:30:02,859 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 23:30:03,939 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 23:30:05,165 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-24 23:30:06,861 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:30:08,775 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:30:10,661 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-24 23:30:12,592 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-24 23:30:14,772 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-24 23:30:18,500 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_28_mlp_gate_proj.safetensors True
Layer: model.layers.28.mlp.gate_proj | Ratio: 0.900 -> Target Rank: 2688 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.1478479e-05 8.4232897e-06 6.2309559e-06 5.6626295e-06 5.4233547e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2688,4096), (11008,2688)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2688, bias=False)
  (1): Linear(in_features=2688, out_features=11008, bias=False)
)
2025-04-24 23:31:18,845 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2688, bias=False)
  (1): Linear(in_features=2688, out_features=11008, bias=False)
)')
2025-04-24 23:31:18,845 - INFO - Replacing 'model.layers.28.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  89%|█████████████████████████████████████████▉     | 201/225 [1:01:47<12:25, 31.07s/it]2025-04-24 23:31:18,845 - INFO - Skipping layer model.layers.28.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:31:18,845 - INFO - Skipping layer model.layers.28.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:31:18,845 - INFO - Layer: model.layers.29.self_attn.q_proj | Ratio: 0.400 -> Target Rank: 824 (Align: 8)
2025-04-24 23:31:18,847 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_29_self_attn_q_proj.safetensors
2025-04-24 23:31:18,847 - INFO - exists: True
2025-04-24 23:31:18,875 - INFO - factorize_layer_kron_svd
2025-04-24 23:31:20,151 - INFO -   Factor is positive definite (alpha=1.00e-05)
2025-04-24 23:31:21,372 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_29_self_attn_q_proj.safetensors True
Layer: model.layers.29.self_attn.q_proj | Ratio: 0.400 -> Target Rank: 824 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Factor is positive definite (alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [5.0433504e-04 4.4244676e-05 2.8946495e-05 1.8910010e-05 1.3421754e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (824,4096), (4096,824)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=824, bias=False)
  (1): Linear(in_features=824, out_features=4096, bias=False)
)
2025-04-24 23:31:43,239 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=824, bias=False)
  (1): Linear(in_features=824, out_features=4096, bias=False)
)')
2025-04-24 23:31:43,240 - INFO - Replacing 'model.layers.29.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)
Compressing Layers:  91%|██████████████████████████████████████████▌    | 204/225 [1:02:11<07:19, 20.93s/it]2025-04-24 23:31:43,240 - INFO - Layer: model.layers.29.self_attn.k_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
2025-04-24 23:31:43,241 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_29_self_attn_k_proj.safetensors
2025-04-24 23:31:43,241 - INFO - exists: True
2025-04-24 23:31:43,251 - INFO - factorize_layer_kron_svd
2025-04-24 23:31:44,535 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:31:45,809 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-24 23:31:47,046 - INFO -   Factor is positive definite (alpha=1.00e-05)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_29_self_attn_k_proj.safetensors True
Layer: model.layers.29.self_attn.k_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-05)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [3.3663202e-04 4.4211527e-05 2.5250516e-05 1.9198609e-05 1.3092749e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1232,4096), (4096,1232)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)
2025-04-24 23:32:09,054 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1232, bias=False)
  (1): Linear(in_features=1232, out_features=4096, bias=False)
)')
2025-04-24 23:32:09,055 - INFO - Replacing 'model.layers.29.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)
Compressing Layers:  91%|██████████████████████████████████████████▊    | 205/225 [1:02:37<07:15, 21.78s/it]2025-04-24 23:32:09,055 - INFO - Skipping layer model.layers.29.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:32:09,056 - INFO - Layer: model.layers.29.self_attn.o_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 23:32:09,056 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_29_self_attn_o_proj.safetensors
2025-04-24 23:32:09,056 - INFO - exists: True
2025-04-24 23:32:09,067 - INFO - factorize_layer_kron_svd
2025-04-24 23:32:10,290 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:32:11,481 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-24 23:32:12,672 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-24 23:32:13,831 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-24 23:32:15,093 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_29_self_attn_o_proj.safetensors True
Layer: model.layers.29.self_attn.o_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x4096)...
  SVD complete. Singular values (top 5): [5.9945382e-05 8.5975926e-06 2.6044404e-06 1.4789388e-06 1.0568257e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (1440,4096), (4096,1440)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)
2025-04-24 23:32:37,534 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=1440, bias=False)
  (1): Linear(in_features=1440, out_features=4096, bias=False)
)')
2025-04-24 23:32:37,534 - INFO - Replacing 'model.layers.29.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)
Compressing Layers:  92%|███████████████████████████████████████████▏   | 207/225 [1:03:05<05:47, 19.28s/it]2025-04-24 23:32:37,535 - INFO - Skipping layer model.layers.29.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:32:37,535 - INFO - Skipping layer model.layers.29.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:32:37,535 - INFO - Skipping layer model.layers.29.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:32:37,535 - INFO - Layer: model.layers.30.self_attn.q_proj | Ratio: 0.700 -> Target Rank: 1440 (Align: 8)
2025-04-24 23:32:37,536 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_30_self_attn_q_proj.safetensors
2025-04-24 23:32:37,536 - INFO - exists: False
2025-04-24 23:32:37,536 - WARNING - Skipping layer model.layers.30.self_attn.q_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_30_self_attn_q_proj.safetensors
2025-04-24 23:32:37,536 - INFO - Layer: model.layers.30.self_attn.k_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
2025-04-24 23:32:37,536 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_30_self_attn_k_proj.safetensors
2025-04-24 23:32:37,536 - INFO - exists: False
2025-04-24 23:32:37,536 - WARNING - Skipping layer model.layers.30.self_attn.k_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_30_self_attn_k_proj.safetensors
2025-04-24 23:32:37,536 - INFO - Skipping layer model.layers.30.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:32:37,536 - INFO - Layer: model.layers.30.self_attn.o_proj | Ratio: 0.800 -> Target Rank: 1640 (Align: 8)
2025-04-24 23:32:37,537 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_30_self_attn_o_proj.safetensors
2025-04-24 23:32:37,537 - INFO - exists: False
2025-04-24 23:32:37,537 - WARNING - Skipping layer model.layers.30.self_attn.o_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_30_self_attn_o_proj.safetensors
2025-04-24 23:32:37,537 - INFO - Skipping layer model.layers.30.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:32:37,537 - INFO - Skipping layer model.layers.30.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:32:37,537 - INFO - Skipping layer model.layers.30.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:32:37,537 - INFO - Layer: model.layers.31.self_attn.q_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 23:32:37,537 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_31_self_attn_q_proj.safetensors
2025-04-24 23:32:37,537 - INFO - exists: False
2025-04-24 23:32:37,537 - WARNING - Skipping layer model.layers.31.self_attn.q_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_31_self_attn_q_proj.safetensors
2025-04-24 23:32:37,537 - INFO - Layer: model.layers.31.self_attn.k_proj | Ratio: 0.500 -> Target Rank: 1024 (Align: 8)
2025-04-24 23:32:37,538 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_31_self_attn_k_proj.safetensors
2025-04-24 23:32:37,538 - INFO - exists: False
2025-04-24 23:32:37,538 - WARNING - Skipping layer model.layers.31.self_attn.k_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_31_self_attn_k_proj.safetensors
2025-04-24 23:32:37,538 - INFO - Skipping layer model.layers.31.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:32:37,538 - INFO - Layer: model.layers.31.self_attn.o_proj | Ratio: 0.600 -> Target Rank: 1232 (Align: 8)
2025-04-24 23:32:37,538 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_31_self_attn_o_proj.safetensors
2025-04-24 23:32:37,538 - INFO - exists: False
2025-04-24 23:32:37,538 - WARNING - Skipping layer model.layers.31.self_attn.o_proj: Factor file not found at /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_31_self_attn_o_proj.safetensors
2025-04-24 23:32:37,538 - INFO - Skipping layer model.layers.31.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:32:37,538 - INFO - Skipping layer model.layers.31.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:32:37,538 - INFO - Skipping layer model.layers.31.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-24 23:32:37,538 - INFO - Skipping layer lm_head: Sensitivity ratio >= 1.0 (1.00). No compression.
Compressing Layers: 100%|███████████████████████████████████████████████| 225/225 [1:03:05<00:00, 16.83s/it]
2025-04-24 23:32:37,538 - INFO - Compression finished. Processed: 88, Skipped (Ratio>=1 or No Sensitivity/Factors): 137, Failed: 0
2025-04-24 23:32:37,539 - INFO - Saving compressed model to ./llama15
2025-04-24 23:33:00,069 - INFO - Compressed model and tokenizer saved.
2025-04-24 23:33:00,077 - INFO - Evaluating on wikitext2
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_30_self_attn_q_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_30_self_attn_k_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_30_self_attn_o_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_31_self_attn_q_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_31_self_attn_k_proj.safetensors False
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b/fisher_factors_output_1404/model_layers_31_self_attn_o_proj.safetensors False
Evaluating:   0%|                                                                    | 0/21 [00:00<?, ?it/s]Evaluating:   5%|██▊                                                         | 1/21 [00:04<01:30,  4.53s/it]Evaluating:  10%|█████▋                                                      | 2/21 [00:05<00:48,  2.53s/it]Evaluating:  14%|████████▌                                                   | 3/21 [00:06<00:34,  1.89s/it]Evaluating:  19%|███████████▍                                                | 4/21 [00:07<00:27,  1.59s/it]Evaluating:  24%|██████████████▎                                             | 5/21 [00:09<00:22,  1.43s/it]Evaluating:  29%|█████████████████▏                                          | 6/21 [00:10<00:19,  1.33s/it]Evaluating:  33%|████████████████████                                        | 7/21 [00:11<00:17,  1.27s/it]Evaluating:  38%|██████████████████████▊                                     | 8/21 [00:12<00:15,  1.22s/it]Evaluating:  43%|█████████████████████████▋                                  | 9/21 [00:13<00:14,  1.20s/it]Evaluating:  48%|████████████████████████████                               | 10/21 [00:14<00:12,  1.18s/it]Evaluating:  52%|██████████████████████████████▉                            | 11/21 [00:15<00:11,  1.16s/it]Evaluating:  57%|█████████████████████████████████▋                         | 12/21 [00:17<00:10,  1.16s/it]Evaluating:  62%|████████████████████████████████████▌                      | 13/21 [00:18<00:09,  1.15s/it]Evaluating:  67%|███████████████████████████████████████▎                   | 14/21 [00:19<00:08,  1.15s/it]Evaluating:  71%|██████████████████████████████████████████▏                | 15/21 [00:20<00:06,  1.14s/it]Evaluating:  76%|████████████████████████████████████████████▉              | 16/21 [00:21<00:05,  1.14s/it]Evaluating:  81%|███████████████████████████████████████████████▊           | 17/21 [00:22<00:04,  1.14s/it]Evaluating:  86%|██████████████████████████████████████████████████▌        | 18/21 [00:23<00:03,  1.14s/it]Evaluating:  90%|█████████████████████████████████████████████████████▍     | 19/21 [00:24<00:02,  1.14s/it]Evaluating:  95%|████████████████████████████████████████████████████████▏  | 20/21 [00:26<00:01,  1.14s/it]Evaluating: 100%|███████████████████████████████████████████████████████████| 21/21 [00:26<00:00,  1.06s/it]Evaluating: 100%|███████████████████████████████████████████████████████████| 21/21 [00:26<00:00,  1.29s/it]
2025-04-24 23:33:34,738 - ERROR - Failed to evaluate on wikitext2: exp(): argument 'input' (position 1) must be Tensor, not float
2025-04-24 23:33:34,738 - INFO - Evaluating on ptb
Evaluating:   0%|                                                                     | 0/7 [00:00<?, ?it/s]Evaluating:  14%|████████▋                                                    | 1/7 [00:01<00:06,  1.13s/it]Evaluating:  29%|█████████████████▍                                           | 2/7 [00:02<00:05,  1.13s/it]Evaluating:  43%|██████████████████████████▏                                  | 3/7 [00:03<00:04,  1.13s/it]Evaluating:  57%|██████████████████████████████████▊                          | 4/7 [00:04<00:03,  1.14s/it]Evaluating:  71%|███████████████████████████████████████████▌                 | 5/7 [00:05<00:02,  1.14s/it]Evaluating:  86%|████████████████████████████████████████████████████▎        | 6/7 [00:06<00:01,  1.14s/it]Evaluating: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:07<00:00,  1.09s/it]Evaluating: 100%|█████████████████████████████████████████████████████████████| 7/7 [00:07<00:00,  1.12s/it]
2025-04-24 23:33:44,623 - ERROR - Failed to evaluate on ptb: exp(): argument 'input' (position 1) must be Tensor, not float
2025-04-24 23:33:44,623 - INFO - Evaluation results:
2025-04-24 23:33:44,623 - INFO -   wikitext2: nan
2025-04-24 23:33:44,623 - INFO -   ptb: nan

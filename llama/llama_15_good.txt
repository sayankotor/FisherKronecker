2025-04-27 22:04:16,993 - INFO - Loading model: unsloth/llama-2-7b-chatmodel.layers.28.self_attn.k_proj 0.3model.layers.26.mlp.gate_proj 0.5model.layers.26.mlp.up_proj 0.5model.layers.25.mlp.up_proj 0.5model.layers.23.mlp.gate_proj 0.5model.layers.23.mlp.down_proj 0.5model.layers.22.mlp.up_proj 0.5model.layers.22.mlp.down_proj 0.5model.layers.21.mlp.up_proj 0.5model.layers.21.mlp.down_proj 0.5model.layers.20.mlp.gate_proj 0.5model.layers.19.mlp.up_proj 0.5model.layers.19.self_attn.q_proj 0.3model.layers.18.mlp.up_proj 0.5model.layers.18.self_attn.q_proj 0.3model.layers.18.self_attn.k_proj 0.3model.layers.17.self_attn.q_proj 0.3model.layers.16.self_attn.v_proj 0.3model.layers.15.self_attn.q_proj 0.3model.layers.14.mlp.gate_proj 0.5model.layers.14.mlp.up_proj 0.5model.layers.13.mlp.up_proj 0.5model.layers.13.self_attn.q_proj 0.3model.layers.13.self_attn.k_proj 0.3model.layers.13.self_attn.v_proj 0.3model.layers.13.self_attn.o_proj 0.3model.layers.12.mlp.gate_proj 0.5model.layers.12.mlp.up_proj 0.5model.layers.12.self_attn.q_proj 0.3model.layers.12.self_attn.k_proj 0.3model.layers.11.self_attn.q_proj 0.3model.layers.8.self_attn.q_proj 0.3model.layers.8.self_attn.k_proj 0.3model.layers.7.mlp.down_proj 0.5model.layers.7.self_attn.q_proj 0.3model.layers.7.self_attn.k_proj 0.3model.layers.6.self_attn.q_proj 0.3model.layers.6.self_attn.k_proj 0.3model.layers.5.self_attn.k_proj 0.3model.layers.4.self_attn.q_proj 0.3model.layers.4.self_attn.k_proj 0.3model.layers.3.self_attn.q_proj 0.3model.layers.3.self_attn.k_proj 0.3model.layers.2.mlp.gate_proj 0.5model.layers.2.self_attn.q_proj 0.3model.layers.2.self_attn.k_proj 0.3[2025-04-27 22:04:21,935] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.Loading checkpoint shards:   0%|                                                                                                                                                            | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|█████████████████████████████████████████████████▎                                                                                                  | 1/3 [00:00<00:01,  1.21it/s]Loading checkpoint shards:  67%|██████████████████████████████████████████████████████████████████████████████████████████████████▋                                                 | 2/3 [00:01<00:00,  1.37it/s]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.66it/s]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:01<00:00,  1.55it/s]2025-04-27 22:04:26,040 - INFO - Model loaded with dtype torch.bfloat162025-04-27 22:04:59,092 - INFO - Model moved to cuda:02025-04-27 22:04:59,093 - INFO - Total parameters before compression: 67384156162025-04-27 22:04:59,093 - INFO - Found 225 linear layers to potentially compress.Compressing Layers:   0%|                                                                                                                                                                 | 0/225 [00:00<?, ?it/s]2025-04-27 22:04:59,094 - INFO - Skipping layer model.layers.0.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:04:59,094 - INFO - Skipping layer model.layers.0.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:04:59,094 - INFO - Skipping layer model.layers.0.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:04:59,094 - INFO - Skipping layer model.layers.0.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:04:59,094 - INFO - Skipping layer model.layers.0.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:04:59,094 - INFO - Skipping layer model.layers.0.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:04:59,094 - INFO - Skipping layer model.layers.0.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:04:59,094 - INFO - Skipping layer model.layers.1.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:04:59,094 - INFO - Skipping layer model.layers.1.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:04:59,094 - INFO - Skipping layer model.layers.1.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:04:59,094 - INFO - Skipping layer model.layers.1.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:04:59,094 - INFO - Skipping layer model.layers.1.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:04:59,094 - INFO - Skipping layer model.layers.1.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:04:59,094 - INFO - Skipping layer model.layers.1.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:04:59,095 - INFO - Layer: model.layers.2.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:04:59,096 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_q_proj.safetensors2025-04-27 22:04:59,096 - INFO - exists: True2025-04-27 22:04:59,097 - INFO - factorize_layer_kron_svd2025-04-27 22:05:00,233 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:05:01,348 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:05:02,476 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:05:03,604 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:05:04,698 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:05:06,001 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:05:07,125 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:05:08,259 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:05:09,352 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:05:10,455 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:05:11,566 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:05:12,838 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_q_proj.safetensors TrueLayer: model.layers.2.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.6124124e-05 1.1238000e-05 1.0046171e-05 9.5376463e-06 9.3023737e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:05:33,991 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:05:33,992 - INFO - Replacing 'model.layers.2.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:   7%|██████████▏                                                                                                                                             | 15/225 [00:34<08:08,  2.33s/it]2025-04-27 22:05:33,992 - INFO - Layer: model.layers.2.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:05:33,993 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_k_proj.safetensors2025-04-27 22:05:33,993 - INFO - exists: True2025-04-27 22:05:33,997 - INFO - factorize_layer_kron_svd2025-04-27 22:05:35,884 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:05:37,385 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:05:38,497 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:05:39,698 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_self_attn_k_proj.safetensors TrueLayer: model.layers.2.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.0046256e-03 1.5359199e-04 9.4199255e-05 5.9006747e-05 4.1726034e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:06:01,166 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:06:01,167 - INFO - Replacing 'model.layers.2.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:   7%|██████████▊                                                                                                                                             | 16/225 [01:02<15:37,  4.49s/it]2025-04-27 22:06:01,167 - INFO - Skipping layer model.layers.2.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:06:01,167 - INFO - Skipping layer model.layers.2.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:06:01,167 - INFO - Layer: model.layers.2.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:06:01,168 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_gate_proj.safetensors2025-04-27 22:06:01,168 - INFO - exists: True2025-04-27 22:06:01,173 - INFO - factorize_layer_kron_svd2025-04-27 22:06:02,412 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:06:03,535 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:06:04,630 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:06:05,731 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:06:06,835 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:06:08,057 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:06:09,835 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:06:11,770 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:06:13,720 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:06:15,680 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:06:17,645 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:06:20,781 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_gate_proj.safetensors TrueLayer: model.layers.2.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.0536157e-05 7.8889116e-06 7.3416263e-06 6.9958951e-06 6.9346693e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:07:06,303 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:07:06,303 - INFO - Replacing 'model.layers.2.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:   8%|████████████▊                                                                                                                                           | 19/225 [02:07<31:27,  9.16s/it]2025-04-27 22:07:06,304 - INFO - Skipping layer model.layers.2.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:07:06,304 - INFO - Skipping layer model.layers.2.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:07:06,304 - INFO - Layer: model.layers.3.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:07:06,305 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_q_proj.safetensors2025-04-27 22:07:06,305 - INFO - exists: True2025-04-27 22:07:06,318 - INFO - factorize_layer_kron_svd2025-04-27 22:07:07,488 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:07:08,760 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:07:09,894 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:07:11,155 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_q_proj.safetensors TrueLayer: model.layers.3.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.1026561e-02 7.6449120e-05 6.9494883e-05 4.4865315e-05 4.0528874e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:07:33,689 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:07:33,689 - INFO - Replacing 'model.layers.3.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  10%|██████████████▊                                                                                                                                         | 22/225 [02:34<30:58,  9.15s/it]2025-04-27 22:07:33,689 - INFO - Layer: model.layers.3.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:07:33,690 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_k_proj.safetensors2025-04-27 22:07:33,690 - INFO - exists: True2025-04-27 22:07:33,695 - INFO - factorize_layer_kron_svd2025-04-27 22:07:34,829 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:07:35,920 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:07:37,014 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:07:38,106 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:07:39,186 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:07:40,396 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:07:41,469 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:07:42,547 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:07:43,650 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:07:44,738 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:07:45,838 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:07:47,045 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_self_attn_k_proj.safetensors TrueLayer: model.layers.3.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.5212615e-05 7.6301594e-06 6.8745117e-06 6.6968514e-06 6.6398402e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:08:08,428 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:08:08,428 - INFO - Replacing 'model.layers.3.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  10%|███████████████▌                                                                                                                                        | 23/225 [03:09<40:56, 12.16s/it]2025-04-27 22:08:08,428 - INFO - Skipping layer model.layers.3.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:08:08,428 - INFO - Skipping layer model.layers.3.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:08:08,428 - INFO - Skipping layer model.layers.3.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:08:08,428 - INFO - Skipping layer model.layers.3.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:08:08,428 - INFO - Skipping layer model.layers.3.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:08:08,428 - INFO - Layer: model.layers.4.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:08:08,430 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_q_proj.safetensors2025-04-27 22:08:08,430 - INFO - exists: True2025-04-27 22:08:08,437 - INFO - factorize_layer_kron_svd2025-04-27 22:08:09,627 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:08:10,843 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:08:12,071 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:08:13,147 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:08:14,372 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_q_proj.safetensors TrueLayer: model.layers.4.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.2226075e-02 9.3771603e-05 6.9365597e-05 3.1357977e-05 2.4107103e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:08:36,070 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:08:36,071 - INFO - Replacing 'model.layers.4.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  13%|███████████████████▌                                                                                                                                    | 29/225 [03:36<27:20,  8.37s/it]2025-04-27 22:08:36,071 - INFO - Layer: model.layers.4.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:08:36,071 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_k_proj.safetensors2025-04-27 22:08:36,071 - INFO - exists: True2025-04-27 22:08:36,076 - INFO - factorize_layer_kron_svd2025-04-27 22:08:37,260 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:08:38,375 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:08:39,467 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:08:40,550 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:08:41,633 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:08:42,832 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:08:43,948 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:08:45,068 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:08:46,154 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:08:47,241 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:08:48,344 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:08:49,662 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_self_attn_k_proj.safetensors TrueLayer: model.layers.4.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.5640973e-05 8.6818372e-06 8.1892204e-06 8.0699401e-06 7.7858249e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:09:12,163 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:09:12,164 - INFO - Replacing 'model.layers.4.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  13%|████████████████████▎                                                                                                                                   | 30/225 [04:13<36:49, 11.33s/it]2025-04-27 22:09:12,164 - INFO - Skipping layer model.layers.4.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:09:12,164 - INFO - Skipping layer model.layers.4.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:09:12,164 - INFO - Skipping layer model.layers.4.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:09:12,164 - INFO - Skipping layer model.layers.4.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:09:12,164 - INFO - Skipping layer model.layers.4.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:09:12,165 - INFO - Skipping layer model.layers.5.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:09:12,165 - INFO - Layer: model.layers.5.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:09:12,166 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_5_self_attn_k_proj.safetensors2025-04-27 22:09:12,166 - INFO - exists: True2025-04-27 22:09:12,171 - INFO - factorize_layer_kron_svd2025-04-27 22:09:13,390 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:09:14,654 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:09:15,765 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:09:17,004 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_5_self_attn_k_proj.safetensors TrueLayer: model.layers.5.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.8379018e-03 1.5238064e-04 9.5437594e-05 3.4556335e-05 3.0823277e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:09:39,463 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:09:39,464 - INFO - Replacing 'model.layers.5.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  16%|████████████████████████▉                                                                                                                               | 37/225 [04:40<23:28,  7.49s/it]2025-04-27 22:09:39,464 - INFO - Skipping layer model.layers.5.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:09:39,464 - INFO - Skipping layer model.layers.5.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:09:39,464 - INFO - Skipping layer model.layers.5.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:09:39,464 - INFO - Skipping layer model.layers.5.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:09:39,464 - INFO - Skipping layer model.layers.5.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:09:39,464 - INFO - Layer: model.layers.6.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:09:39,465 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_q_proj.safetensors2025-04-27 22:09:39,465 - INFO - exists: True2025-04-27 22:09:39,470 - INFO - factorize_layer_kron_svd2025-04-27 22:09:40,646 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:09:41,762 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:09:42,869 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:09:43,997 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:09:45,089 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:09:46,331 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:09:47,439 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:09:48,593 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:09:49,735 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:09:50,926 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:09:52,150 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:09:53,837 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_q_proj.safetensors TrueLayer: model.layers.6.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.2221675e-05 8.1780718e-06 7.9897936e-06 7.7061068e-06 7.4714530e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:10:46,070 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:10:46,070 - INFO - Replacing 'model.layers.6.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  19%|█████████████████████████████                                                                                                                           | 43/225 [05:46<26:58,  8.89s/it]2025-04-27 22:10:46,071 - INFO - Layer: model.layers.6.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:10:46,072 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_k_proj.safetensors2025-04-27 22:10:46,072 - INFO - exists: True2025-04-27 22:10:46,076 - INFO - factorize_layer_kron_svd2025-04-27 22:10:47,275 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:10:48,424 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:10:49,550 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:10:50,717 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:10:51,985 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:10:53,834 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:10:55,004 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:10:56,170 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:10:57,350 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:10:58,548 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:10:59,594 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:11:01,267 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_self_attn_k_proj.safetensors TrueLayer: model.layers.6.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.4503589e-05 7.8944504e-06 7.5118478e-06 7.3252472e-06 7.0036058e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:11:39,166 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:11:39,167 - INFO - Replacing 'model.layers.6.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  20%|█████████████████████████████▋                                                                                                                          | 44/225 [06:40<38:04, 12.62s/it]2025-04-27 22:11:39,167 - INFO - Skipping layer model.layers.6.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:11:39,167 - INFO - Skipping layer model.layers.6.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:11:39,167 - INFO - Skipping layer model.layers.6.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:11:39,167 - INFO - Skipping layer model.layers.6.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:11:39,167 - INFO - Skipping layer model.layers.6.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:11:39,168 - INFO - Layer: model.layers.7.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:11:39,185 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_q_proj.safetensors2025-04-27 22:11:39,185 - INFO - exists: True2025-04-27 22:11:39,211 - INFO - factorize_layer_kron_svd2025-04-27 22:11:40,571 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:11:42,047 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:11:43,244 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:11:44,691 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_q_proj.safetensors TrueLayer: model.layers.7.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.46691271e-03 1.17237636e-04 4.46072554e-05 2.93035046e-05 2.62491722e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:12:12,252 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:12:12,252 - INFO - Replacing 'model.layers.7.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  22%|█████████████████████████████████▊                                                                                                                      | 50/225 [07:13<28:06,  9.64s/it]2025-04-27 22:12:12,253 - INFO - Layer: model.layers.7.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:12:12,253 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_k_proj.safetensors2025-04-27 22:12:12,253 - INFO - exists: True2025-04-27 22:12:12,263 - INFO - factorize_layer_kron_svd2025-04-27 22:12:13,554 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:12:15,030 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:12:16,216 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:12:17,611 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_self_attn_k_proj.safetensors TrueLayer: model.layers.7.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.3513221e-03 9.5677962e-05 5.9629248e-05 3.8686332e-05 2.6624961e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:12:44,631 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:12:44,632 - INFO - Replacing 'model.layers.7.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  23%|██████████████████████████████████▍                                                                                                                     | 51/225 [07:45<33:56, 11.71s/it]2025-04-27 22:12:44,632 - INFO - Skipping layer model.layers.7.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:12:44,632 - INFO - Skipping layer model.layers.7.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:12:44,632 - INFO - Skipping layer model.layers.7.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:12:44,632 - INFO - Skipping layer model.layers.7.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:12:44,632 - INFO - Layer: model.layers.7.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:12:44,635 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_down_proj.safetensors2025-04-27 22:12:44,635 - INFO - exists: True2025-04-27 22:12:44,646 - INFO - factorize_layer_kron_svd2025-04-27 22:12:46,585 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:12:49,707 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:12:50,881 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:12:52,304 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:12:54,033 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_down_proj.safetensors TrueLayer: model.layers.7.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=11008, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x11008)...  SVD complete. Singular values (top 5): [4.0638614e-03 2.6415018e-04 8.9995185e-05 2.9939682e-05 2.5749245e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,11008), (4096,1496)factorized_sequential Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))2025-04-27 22:14:16,495 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))')2025-04-27 22:14:16,495 - INFO - Replacing 'model.layers.7.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)Compressing Layers:  25%|█████████████████████████████████████▊                                                                                                                  | 56/225 [09:17<40:21, 14.33s/it]2025-04-27 22:14:16,495 - INFO - Layer: model.layers.8.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:14:16,497 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_q_proj.safetensors2025-04-27 22:14:16,497 - INFO - exists: True2025-04-27 22:14:16,511 - INFO - factorize_layer_kron_svd2025-04-27 22:14:17,840 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:14:19,057 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:14:20,632 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:14:21,733 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:14:23,100 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_q_proj.safetensors TrueLayer: model.layers.8.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [5.8933757e-03 1.1251512e-04 4.9954931e-05 2.4490531e-05 1.9172267e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:14:56,564 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:14:56,564 - INFO - Replacing 'model.layers.8.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  25%|██████████████████████████████████████▌                                                                                                                 | 57/225 [09:57<47:24, 16.93s/it]2025-04-27 22:14:56,565 - INFO - Layer: model.layers.8.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:14:56,566 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_k_proj.safetensors2025-04-27 22:14:56,566 - INFO - exists: True2025-04-27 22:14:56,570 - INFO - factorize_layer_kron_svd2025-04-27 22:14:57,766 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:14:58,897 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:15:00,033 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:15:01,153 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:15:02,254 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:15:03,463 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:15:04,548 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:15:05,648 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:15:06,755 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:15:07,863 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:15:08,961 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:15:10,158 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_self_attn_k_proj.safetensors TrueLayer: model.layers.8.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.4188925e-05 7.6309198e-06 7.4526779e-06 7.2449202e-06 6.7348119e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:15:43,774 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:15:43,775 - INFO - Replacing 'model.layers.8.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  26%|███████████████████████████████████████▏                                                                                                                | 58/225 [10:44<57:45, 20.75s/it]2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.8.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.8.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.8.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.8.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.8.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.9.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.9.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.9.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.9.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.9.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.9.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.9.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.10.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.10.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.10.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.10.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.10.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.10.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Skipping layer model.layers.10.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:15:43,775 - INFO - Layer: model.layers.11.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:15:43,776 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_self_attn_q_proj.safetensors2025-04-27 22:15:43,776 - INFO - exists: True2025-04-27 22:15:43,783 - INFO - factorize_layer_kron_svd2025-04-27 22:15:44,923 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:15:46,135 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:15:47,337 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:15:48,401 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:15:49,566 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:15:50,776 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_self_attn_q_proj.safetensors TrueLayer: model.layers.11.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [2.6593389e-02 3.8675182e-05 2.0216057e-05 1.6013246e-05 1.3640273e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:16:16,271 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:16:16,272 - INFO - Replacing 'model.layers.11.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  35%|████████████████████████████████████████████████████▋                                                                                                   | 78/225 [11:17<14:09,  5.78s/it]2025-04-27 22:16:16,272 - INFO - Skipping layer model.layers.11.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:16,272 - INFO - Skipping layer model.layers.11.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:16,272 - INFO - Skipping layer model.layers.11.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:16,272 - INFO - Skipping layer model.layers.11.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:16,272 - INFO - Skipping layer model.layers.11.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:16,272 - INFO - Skipping layer model.layers.11.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:16:16,272 - INFO - Layer: model.layers.12.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:16:16,273 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_q_proj.safetensors2025-04-27 22:16:16,273 - INFO - exists: True2025-04-27 22:16:16,280 - INFO - factorize_layer_kron_svd2025-04-27 22:16:17,605 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:16:18,830 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:16:20,095 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:16:21,169 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:16:22,409 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:16:23,653 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_q_proj.safetensors TrueLayer: model.layers.12.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [9.15819407e-03 3.75725394e-05 2.34514755e-05 1.29712425e-05 1.20524883e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:16:54,388 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:16:54,388 - INFO - Replacing 'model.layers.12.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  38%|█████████████████████████████████████████████████████████▍                                                                                              | 85/225 [11:55<13:15,  5.68s/it]2025-04-27 22:16:54,388 - INFO - Layer: model.layers.12.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:16:54,390 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_k_proj.safetensors2025-04-27 22:16:54,390 - INFO - exists: True2025-04-27 22:16:54,396 - INFO - factorize_layer_kron_svd2025-04-27 22:16:55,734 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:16:56,830 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:16:57,957 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:16:59,066 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:17:00,160 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:17:01,404 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:17:02,483 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:17:03,607 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:17:04,709 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:17:05,835 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:17:06,964 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:17:08,407 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_self_attn_k_proj.safetensors TrueLayer: model.layers.12.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.35002065e-05 7.74964155e-06 7.61668571e-06 7.45528405e-06 7.28192163e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:17:42,476 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:17:42,477 - INFO - Replacing 'model.layers.12.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  38%|██████████████████████████████████████████████████████████                                                                                              | 86/225 [12:43<18:30,  7.99s/it]2025-04-27 22:17:42,478 - INFO - Skipping layer model.layers.12.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:17:42,478 - INFO - Skipping layer model.layers.12.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:17:42,478 - INFO - Layer: model.layers.12.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:17:42,478 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_gate_proj.safetensors2025-04-27 22:17:42,478 - INFO - exists: True2025-04-27 22:17:42,521 - INFO - factorize_layer_kron_svd2025-04-27 22:17:43,957 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:17:45,068 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:17:46,176 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:17:47,321 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:17:48,462 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:17:50,217 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:17:52,162 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:17:54,465 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:17:56,682 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:17:58,710 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:18:00,699 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:18:06,312 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_gate_proj.safetensors TrueLayer: model.layers.12.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.2988171e-05 7.1681275e-06 6.5595987e-06 6.5212694e-06 6.4388596e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:18:56,080 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:18:56,080 - INFO - Replacing 'model.layers.12.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  40%|████████████████████████████████████████████████████████████                                                                                            | 89/225 [13:56<25:11, 11.11s/it]2025-04-27 22:18:56,081 - INFO - Layer: model.layers.12.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:18:56,082 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_up_proj.safetensors2025-04-27 22:18:56,082 - INFO - exists: True2025-04-27 22:18:56,105 - INFO - factorize_layer_kron_svd2025-04-27 22:18:57,330 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:18:58,449 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:18:59,557 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:19:00,672 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:19:01,814 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:19:03,026 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:19:04,751 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:19:06,747 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:19:08,693 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:19:10,661 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:19:12,640 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:19:16,756 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_up_proj.safetensors TrueLayer: model.layers.12.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [5.9819213e-06 4.8709753e-06 4.7432140e-06 4.6781370e-06 4.6636528e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:20:08,249 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:20:08,250 - INFO - Replacing 'model.layers.12.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  40%|████████████████████████████████████████████████████████████▊                                                                                           | 90/225 [15:09<36:19, 16.15s/it]2025-04-27 22:20:08,250 - INFO - Skipping layer model.layers.12.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:20:08,250 - INFO - Layer: model.layers.13.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:20:08,251 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_q_proj.safetensors2025-04-27 22:20:08,251 - INFO - exists: True2025-04-27 22:20:08,263 - INFO - factorize_layer_kron_svd2025-04-27 22:20:09,531 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:20:10,898 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:20:12,351 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:20:13,475 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:20:14,796 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:20:16,326 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_q_proj.safetensors TrueLayer: model.layers.13.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.9350363e-02 5.4489075e-05 1.5019226e-05 1.3564231e-05 1.1421780e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:20:40,671 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:20:40,672 - INFO - Replacing 'model.layers.13.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  41%|██████████████████████████████████████████████████████████████▏                                                                                         | 92/225 [15:41<35:49, 16.16s/it]2025-04-27 22:20:40,672 - INFO - Layer: model.layers.13.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:20:40,672 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_k_proj.safetensors2025-04-27 22:20:40,673 - INFO - exists: True2025-04-27 22:20:40,677 - INFO - factorize_layer_kron_svd2025-04-27 22:20:41,766 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:20:42,839 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:20:43,931 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:20:45,058 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:20:46,138 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:20:47,931 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:20:49,046 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:20:50,178 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:20:51,312 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:20:52,444 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:20:53,575 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:20:55,044 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_k_proj.safetensors TrueLayer: model.layers.13.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.3729892e-05 7.1575278e-06 7.1094173e-06 6.8093209e-06 6.5548347e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:21:25,374 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:21:25,374 - INFO - Replacing 'model.layers.13.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  41%|██████████████████████████████████████████████████████████████▊                                                                                         | 93/225 [16:26<43:04, 19.58s/it]2025-04-27 22:21:25,375 - INFO - Layer: model.layers.13.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:21:25,375 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_v_proj.safetensors2025-04-27 22:21:25,375 - INFO - exists: True2025-04-27 22:21:25,381 - INFO - factorize_layer_kron_svd2025-04-27 22:21:26,606 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:21:28,053 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:21:29,201 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:21:30,831 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:21:32,654 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_v_proj.safetensors TrueLayer: model.layers.13.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [5.0983722e-03 5.0585440e-05 2.1063875e-05 1.7282286e-05 1.4230653e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:22:07,091 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:22:07,091 - INFO - Replacing 'model.layers.13.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)Compressing Layers:  42%|███████████████████████████████████████████████████████████████▌                                                                                        | 94/225 [17:07<49:49, 22.82s/it]2025-04-27 22:22:07,092 - INFO - Layer: model.layers.13.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:22:07,092 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_o_proj.safetensors2025-04-27 22:22:07,092 - INFO - exists: True2025-04-27 22:22:07,097 - INFO - factorize_layer_kron_svd2025-04-27 22:22:08,289 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:22:09,465 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:22:10,694 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:22:11,813 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:22:12,979 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:22:14,283 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_self_attn_o_proj.safetensors TrueLayer: model.layers.13.self_attn.o_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.2446343e-02 3.8981681e-05 3.5099336e-05 2.8158551e-05 2.2302480e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:22:41,548 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:22:41,548 - INFO - Replacing 'model.layers.13.self_attn.o_proj' (attribute 'o_proj' within parent LlamaAttention)Compressing Layers:  42%|████████████████████████████████████████████████████████████████▏                                                                                       | 95/225 [17:42<53:47, 24.83s/it]2025-04-27 22:22:41,548 - INFO - Skipping layer model.layers.13.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:22:41,548 - INFO - Layer: model.layers.13.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:22:41,550 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_up_proj.safetensors2025-04-27 22:22:41,550 - INFO - exists: True2025-04-27 22:22:41,554 - INFO - factorize_layer_kron_svd2025-04-27 22:22:42,983 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:22:44,356 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:22:45,807 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:22:47,533 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:22:50,178 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:22:53,543 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_up_proj.safetensors TrueLayer: model.layers.13.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [6.4682798e-03 6.1749437e-05 2.4349400e-05 2.1919277e-05 2.0116417e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:23:49,780 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:23:49,780 - INFO - Replacing 'model.layers.13.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  43%|█████████████████████████████████████████████████████████████████▌                                                                                      | 97/225 [18:50<59:31, 27.90s/it]2025-04-27 22:23:49,781 - INFO - Skipping layer model.layers.13.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:23:49,781 - INFO - Skipping layer model.layers.14.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:23:49,781 - INFO - Skipping layer model.layers.14.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:23:49,781 - INFO - Skipping layer model.layers.14.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:23:49,781 - INFO - Skipping layer model.layers.14.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:23:49,781 - INFO - Layer: model.layers.14.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:23:49,782 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_gate_proj.safetensors2025-04-27 22:23:49,782 - INFO - exists: True2025-04-27 22:23:49,800 - INFO - factorize_layer_kron_svd2025-04-27 22:23:51,119 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:23:52,509 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:23:54,105 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:23:56,012 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:24:00,294 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_gate_proj.safetensors TrueLayer: model.layers.14.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [3.2148813e-03 8.8560395e-05 2.4315630e-05 1.8305876e-05 1.6133905e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:25:00,881 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:25:00,881 - INFO - Replacing 'model.layers.14.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  46%|█████████████████████████████████████████████████████████████████████                                                                                  | 103/225 [20:01<37:35, 18.49s/it]2025-04-27 22:25:00,882 - INFO - Layer: model.layers.14.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:25:00,883 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_up_proj.safetensors2025-04-27 22:25:00,883 - INFO - exists: True2025-04-27 22:25:00,895 - INFO - factorize_layer_kron_svd2025-04-27 22:25:02,327 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:25:03,548 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:25:04,961 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:25:06,730 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:25:09,439 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:25:12,448 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_up_proj.safetensors TrueLayer: model.layers.14.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [9.7202640e-03 2.5967920e-05 2.1275189e-05 1.6423986e-05 1.5875536e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:26:08,417 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:26:08,417 - INFO - Replacing 'model.layers.14.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  46%|█████████████████████████████████████████████████████████████████████▊                                                                                 | 104/225 [21:09<49:24, 24.50s/it]2025-04-27 22:26:08,417 - INFO - Skipping layer model.layers.14.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:08,417 - INFO - Layer: model.layers.15.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:26:08,423 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_self_attn_q_proj.safetensors2025-04-27 22:26:08,423 - INFO - exists: True2025-04-27 22:26:08,439 - INFO - factorize_layer_kron_svd2025-04-27 22:26:09,674 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:26:11,022 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:26:12,381 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:26:13,564 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:26:14,862 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:26:16,134 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_self_attn_q_proj.safetensors TrueLayer: model.layers.15.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.19919665e-02 7.38329763e-05 2.44418788e-05 1.36780509e-05 1.00903217e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:26:40,986 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:26:40,987 - INFO - Replacing 'model.layers.15.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  47%|███████████████████████████████████████████████████████████████████████▏                                                                               | 106/225 [21:41<44:21, 22.37s/it]2025-04-27 22:26:40,987 - INFO - Skipping layer model.layers.15.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:40,987 - INFO - Skipping layer model.layers.15.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:40,987 - INFO - Skipping layer model.layers.15.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:40,987 - INFO - Skipping layer model.layers.15.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:40,987 - INFO - Skipping layer model.layers.15.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:40,987 - INFO - Skipping layer model.layers.15.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:40,987 - INFO - Skipping layer model.layers.16.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:40,987 - INFO - Skipping layer model.layers.16.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:26:40,987 - INFO - Layer: model.layers.16.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:26:40,987 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_self_attn_v_proj.safetensors2025-04-27 22:26:40,987 - INFO - exists: True2025-04-27 22:26:40,992 - INFO - factorize_layer_kron_svd2025-04-27 22:26:42,137 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:26:43,380 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:26:44,522 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:26:46,049 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:26:47,919 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_self_attn_v_proj.safetensors TrueLayer: model.layers.16.self_attn.v_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [2.1955712e-02 2.2352620e-05 1.8913308e-05 1.6113720e-05 1.5723988e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:27:14,959 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:27:14,959 - INFO - Replacing 'model.layers.16.self_attn.v_proj' (attribute 'v_proj' within parent LlamaAttention)Compressing Layers:  51%|█████████████████████████████████████████████████████████████████████████████▏                                                                         | 115/225 [22:15<19:42, 10.75s/it]2025-04-27 22:27:14,960 - INFO - Skipping layer model.layers.16.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:27:14,960 - INFO - Skipping layer model.layers.16.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:27:14,960 - INFO - Skipping layer model.layers.16.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:27:14,960 - INFO - Skipping layer model.layers.16.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:27:14,960 - INFO - Layer: model.layers.17.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:27:14,960 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_self_attn_q_proj.safetensors2025-04-27 22:27:14,960 - INFO - exists: True2025-04-27 22:27:14,965 - INFO - factorize_layer_kron_svd2025-04-27 22:27:16,119 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:27:17,284 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:27:18,495 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:27:19,605 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:27:20,785 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:27:22,175 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_self_attn_q_proj.safetensors TrueLayer: model.layers.17.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.9799747e-02 3.0997286e-05 1.8318075e-05 1.5734129e-05 9.7556685e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:27:48,083 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:27:48,083 - INFO - Replacing 'model.layers.17.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  53%|████████████████████████████████████████████████████████████████████████████████▌                                                                      | 120/225 [22:48<16:25,  9.38s/it]2025-04-27 22:27:48,083 - INFO - Skipping layer model.layers.17.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:27:48,083 - INFO - Skipping layer model.layers.17.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:27:48,083 - INFO - Skipping layer model.layers.17.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:27:48,083 - INFO - Skipping layer model.layers.17.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:27:48,083 - INFO - Skipping layer model.layers.17.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:27:48,083 - INFO - Skipping layer model.layers.17.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:27:48,083 - INFO - Layer: model.layers.18.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:27:48,084 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_q_proj.safetensors2025-04-27 22:27:48,084 - INFO - exists: True2025-04-27 22:27:48,088 - INFO - factorize_layer_kron_svd2025-04-27 22:27:49,232 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:27:50,400 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:27:51,631 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:27:52,771 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:27:54,002 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:27:55,429 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_q_proj.safetensors TrueLayer: model.layers.18.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [4.0405821e-03 8.4903695e-05 8.4368939e-06 7.6583383e-06 6.2006247e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:28:22,997 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:28:22,997 - INFO - Replacing 'model.layers.18.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  56%|█████████████████████████████████████████████████████████████████████████████████████▏                                                                 | 127/225 [23:23<12:27,  7.63s/it]2025-04-27 22:28:22,997 - INFO - Layer: model.layers.18.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:28:22,997 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_k_proj.safetensors2025-04-27 22:28:22,997 - INFO - exists: True2025-04-27 22:28:23,002 - INFO - factorize_layer_kron_svd2025-04-27 22:28:24,153 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:28:25,342 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:28:26,548 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:28:27,635 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:28:28,804 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:28:30,033 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_self_attn_k_proj.safetensors TrueLayer: model.layers.18.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [7.7125463e-03 3.3378132e-05 8.2256274e-06 7.4304949e-06 6.1785499e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:29:04,664 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:29:04,664 - INFO - Replacing 'model.layers.18.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  57%|█████████████████████████████████████████████████████████████████████████████████████▉                                                                 | 128/225 [24:05<16:28, 10.19s/it]2025-04-27 22:29:04,664 - INFO - Skipping layer model.layers.18.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:29:04,664 - INFO - Skipping layer model.layers.18.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:29:04,664 - INFO - Skipping layer model.layers.18.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:29:04,664 - INFO - Layer: model.layers.18.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:29:04,665 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_mlp_up_proj.safetensors2025-04-27 22:29:04,665 - INFO - exists: True2025-04-27 22:29:04,669 - INFO - factorize_layer_kron_svd2025-04-27 22:29:06,023 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:29:07,214 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:29:08,424 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:29:10,255 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:29:13,267 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_mlp_up_proj.safetensors TrueLayer: model.layers.18.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [5.7937209e-03 2.0253658e-05 1.6411330e-05 1.4584949e-05 1.2499403e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:29:54,357 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:29:54,357 - INFO - Replacing 'model.layers.18.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  59%|████████████████████████████████████████████████████████████████████████████████████████▌                                                              | 132/225 [24:55<16:50, 10.86s/it]2025-04-27 22:29:54,357 - INFO - Skipping layer model.layers.18.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:29:54,357 - INFO - Layer: model.layers.19.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:29:54,358 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_self_attn_q_proj.safetensors2025-04-27 22:29:54,358 - INFO - exists: True2025-04-27 22:29:54,371 - INFO - factorize_layer_kron_svd2025-04-27 22:29:55,530 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:29:56,755 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:29:57,834 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:29:59,042 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_self_attn_q_proj.safetensors TrueLayer: model.layers.19.self_attn.q_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [2.2634035e-03 7.5883843e-05 1.0972199e-05 7.9327338e-06 7.6394726e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:30:19,852 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:30:19,852 - INFO - Replacing 'model.layers.19.self_attn.q_proj' (attribute 'q_proj' within parent LlamaAttention)Compressing Layers:  60%|█████████████████████████████████████████████████████████████████████████████████████████▉                                                             | 134/225 [25:20<16:58, 11.20s/it]2025-04-27 22:30:19,853 - INFO - Skipping layer model.layers.19.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:30:19,853 - INFO - Skipping layer model.layers.19.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:30:19,853 - INFO - Skipping layer model.layers.19.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:30:19,853 - INFO - Skipping layer model.layers.19.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:30:19,853 - INFO - Layer: model.layers.19.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:30:19,853 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_mlp_up_proj.safetensors2025-04-27 22:30:19,853 - INFO - exists: True2025-04-27 22:30:19,858 - INFO - factorize_layer_kron_svd2025-04-27 22:30:21,100 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:30:22,264 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:30:23,491 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:30:25,314 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:30:28,398 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_mlp_up_proj.safetensors TrueLayer: model.layers.19.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [5.9256121e-03 1.7187345e-05 1.4408997e-05 1.2730728e-05 9.7351012e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:31:11,685 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:31:11,685 - INFO - Replacing 'model.layers.19.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  62%|█████████████████████████████████████████████████████████████████████████████████████████████▎                                                         | 139/225 [26:12<15:35, 10.87s/it]2025-04-27 22:31:11,685 - INFO - Skipping layer model.layers.19.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:31:11,685 - INFO - Skipping layer model.layers.20.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:31:11,685 - INFO - Skipping layer model.layers.20.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:31:11,685 - INFO - Skipping layer model.layers.20.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:31:11,685 - INFO - Skipping layer model.layers.20.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:31:11,685 - INFO - Layer: model.layers.20.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:31:11,687 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_20_mlp_gate_proj.safetensors2025-04-27 22:31:11,687 - INFO - exists: True2025-04-27 22:31:11,700 - INFO - factorize_layer_kron_svd2025-04-27 22:31:12,954 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:31:14,127 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:31:15,322 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:31:17,163 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:31:20,326 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_20_mlp_gate_proj.safetensors TrueLayer: model.layers.20.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [5.6294589e-03 2.1218650e-05 1.5237835e-05 1.4524096e-05 1.3646030e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:32:04,296 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:32:04,297 - INFO - Replacing 'model.layers.20.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  64%|█████████████████████████████████████████████████████████████████████████████████████████████████▎                                                     | 145/225 [27:05<13:22, 10.03s/it]2025-04-27 22:32:04,297 - INFO - Skipping layer model.layers.20.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:32:04,297 - INFO - Skipping layer model.layers.20.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:32:04,297 - INFO - Skipping layer model.layers.21.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:32:04,297 - INFO - Skipping layer model.layers.21.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:32:04,297 - INFO - Skipping layer model.layers.21.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:32:04,297 - INFO - Skipping layer model.layers.21.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:32:04,297 - INFO - Skipping layer model.layers.21.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:32:04,297 - INFO - Layer: model.layers.21.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:32:04,298 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_up_proj.safetensors2025-04-27 22:32:04,298 - INFO - exists: True2025-04-27 22:32:04,319 - INFO - factorize_layer_kron_svd2025-04-27 22:32:05,555 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:32:06,744 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:32:07,962 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:32:09,845 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:32:13,065 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_up_proj.safetensors TrueLayer: model.layers.21.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.8695735e-03 5.5963137e-05 1.4356574e-05 1.3274203e-05 1.2030486e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:32:58,739 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:32:58,740 - INFO - Replacing 'model.layers.21.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  68%|██████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                | 153/225 [27:59<10:22,  8.64s/it]2025-04-27 22:32:58,740 - INFO - Layer: model.layers.21.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:32:58,741 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_down_proj.safetensors2025-04-27 22:32:58,741 - INFO - exists: True2025-04-27 22:32:58,767 - INFO - factorize_layer_kron_svd2025-04-27 22:33:00,650 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:33:02,978 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:33:06,215 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:33:07,284 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:33:08,451 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:33:09,707 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_down_proj.safetensors TrueLayer: model.layers.21.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=11008, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x11008)...  SVD complete. Singular values (top 5): [4.5640193e-02 9.2487091e-05 5.4519722e-05 4.7700971e-05 3.9835424e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,11008), (4096,1496)factorized_sequential Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))2025-04-27 22:34:01,400 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))')2025-04-27 22:34:01,400 - INFO - Replacing 'model.layers.21.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)Compressing Layers:  68%|███████████████████████████████████████████████████████████████████████████████████████████████████████▎                                               | 154/225 [29:02<14:48, 12.51s/it]2025-04-27 22:34:01,401 - INFO - Skipping layer model.layers.22.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:01,401 - INFO - Skipping layer model.layers.22.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:01,401 - INFO - Skipping layer model.layers.22.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:01,401 - INFO - Skipping layer model.layers.22.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:01,401 - INFO - Skipping layer model.layers.22.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:34:01,401 - INFO - Layer: model.layers.22.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:34:01,402 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_up_proj.safetensors2025-04-27 22:34:01,403 - INFO - exists: True2025-04-27 22:34:01,427 - INFO - factorize_layer_kron_svd2025-04-27 22:34:02,716 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:34:03,934 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:34:05,177 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:34:07,060 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:34:10,250 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_up_proj.safetensors TrueLayer: model.layers.22.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.2615041e-03 2.1604352e-05 1.3284305e-05 1.1028132e-05 1.0561911e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:34:56,181 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:34:56,181 - INFO - Replacing 'model.layers.22.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                           | 160/225 [29:57<12:09, 11.22s/it]2025-04-27 22:34:56,182 - INFO - Layer: model.layers.22.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:34:56,182 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_down_proj.safetensors2025-04-27 22:34:56,182 - INFO - exists: True2025-04-27 22:34:56,203 - INFO - factorize_layer_kron_svd2025-04-27 22:34:58,117 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:35:00,457 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:35:03,708 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:35:04,797 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:35:05,976 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:35:07,236 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_down_proj.safetensors TrueLayer: model.layers.22.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=11008, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x11008)...  SVD complete. Singular values (top 5): [2.3855805e-02 8.0862206e-05 5.0205126e-05 4.2752916e-05 3.9248724e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,11008), (4096,1496)factorized_sequential Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))2025-04-27 22:35:59,959 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))')2025-04-27 22:35:59,959 - INFO - Replacing 'model.layers.22.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)Compressing Layers:  72%|████████████████████████████████████████████████████████████████████████████████████████████████████████████                                           | 161/225 [31:00<16:37, 15.59s/it]2025-04-27 22:35:59,960 - INFO - Skipping layer model.layers.23.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:35:59,960 - INFO - Skipping layer model.layers.23.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:35:59,960 - INFO - Skipping layer model.layers.23.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:35:59,960 - INFO - Skipping layer model.layers.23.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:35:59,960 - INFO - Layer: model.layers.23.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:35:59,961 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_gate_proj.safetensors2025-04-27 22:35:59,961 - INFO - exists: True2025-04-27 22:35:59,982 - INFO - factorize_layer_kron_svd2025-04-27 22:36:01,220 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:36:02,362 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:36:03,603 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:36:05,524 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:36:08,739 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_gate_proj.safetensors TrueLayer: model.layers.23.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [1.6312244e-03 1.5538668e-05 4.1256385e-06 2.1622898e-06 2.0991954e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:36:53,534 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:36:53,534 - INFO - Replacing 'model.layers.23.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  74%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                       | 166/225 [31:54<13:32, 13.77s/it]2025-04-27 22:36:53,534 - INFO - Skipping layer model.layers.23.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:36:53,534 - INFO - Layer: model.layers.23.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:36:53,535 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_down_proj.safetensors2025-04-27 22:36:53,535 - INFO - exists: True2025-04-27 22:36:53,559 - INFO - factorize_layer_kron_svd2025-04-27 22:36:55,484 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:36:57,931 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:37:01,169 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:37:02,269 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:37:03,466 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:37:04,722 - INFO -   Factor is positive definite (alpha=1.00e-03)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_down_proj.safetensors TrueLayer: model.layers.23.mlp.down_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=11008, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x11008)...  SVD complete. Singular values (top 5): [6.6739982e-03 2.3223182e-04 4.9300255e-05 3.7862912e-05 3.5104458e-05]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,11008), (4096,1496)factorized_sequential Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))2025-04-27 22:37:56,667 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=11008, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=4096, bias=False))')2025-04-27 22:37:56,668 - INFO - Replacing 'model.layers.23.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)Compressing Layers:  75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                      | 168/225 [32:57<16:02, 16.89s/it]2025-04-27 22:37:56,668 - INFO - Skipping layer model.layers.24.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:37:56,668 - INFO - Skipping layer model.layers.24.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:37:56,668 - INFO - Skipping layer model.layers.24.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:37:56,668 - INFO - Skipping layer model.layers.24.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:37:56,668 - INFO - Skipping layer model.layers.24.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:37:56,668 - INFO - Skipping layer model.layers.24.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:37:56,668 - INFO - Skipping layer model.layers.24.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:37:56,668 - INFO - Skipping layer model.layers.25.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:37:56,668 - INFO - Skipping layer model.layers.25.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:37:56,668 - INFO - Skipping layer model.layers.25.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:37:56,668 - INFO - Skipping layer model.layers.25.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:37:56,668 - INFO - Skipping layer model.layers.25.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:37:56,668 - INFO - Layer: model.layers.25.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:37:56,670 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_25_mlp_up_proj.safetensors2025-04-27 22:37:56,670 - INFO - exists: True2025-04-27 22:37:56,695 - INFO - factorize_layer_kron_svd2025-04-27 22:37:57,949 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:37:59,141 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:38:00,371 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:38:02,283 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:38:05,529 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_25_mlp_up_proj.safetensors TrueLayer: model.layers.25.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [5.2369884e-03 1.2808485e-05 1.0910483e-05 1.0035876e-05 9.8339870e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:38:51,819 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:38:51,820 - INFO - Replacing 'model.layers.25.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                             | 181/225 [33:52<06:38,  9.06s/it]2025-04-27 22:38:51,820 - INFO - Skipping layer model.layers.25.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:38:51,820 - INFO - Skipping layer model.layers.26.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:38:51,820 - INFO - Skipping layer model.layers.26.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:38:51,820 - INFO - Skipping layer model.layers.26.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:38:51,820 - INFO - Skipping layer model.layers.26.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:38:51,820 - INFO - Layer: model.layers.26.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:38:51,821 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_mlp_gate_proj.safetensors2025-04-27 22:38:51,821 - INFO - exists: True2025-04-27 22:38:51,849 - INFO - factorize_layer_kron_svd2025-04-27 22:38:53,145 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:38:54,353 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:38:55,618 - INFO -   Factor is positive definite (alpha=1.00e-03)2025-04-27 22:38:57,513 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:39:00,716 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_mlp_gate_proj.safetensors TrueLayer: model.layers.26.mlp.gate_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Factor is positive definite (alpha=1.00e-03)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [3.5354062e-03 1.2539586e-05 5.3919930e-06 4.6146652e-06 4.4492190e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:39:46,348 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:39:46,348 - INFO - Replacing 'model.layers.26.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)Compressing Layers:  83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                         | 187/225 [34:47<05:44,  9.06s/it]2025-04-27 22:39:46,349 - INFO - Layer: model.layers.26.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)2025-04-27 22:39:46,350 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_mlp_up_proj.safetensors2025-04-27 22:39:46,350 - INFO - exists: True2025-04-27 22:39:46,381 - INFO - factorize_layer_kron_svd2025-04-27 22:39:47,622 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:39:48,872 - INFO -   Factor is positive definite (alpha=1.00e-04)2025-04-27 22:39:50,774 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:39:54,028 - INFO -   Factor is positive definite (alpha=1.00e-04)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_mlp_up_proj.safetensors TrueLayer: model.layers.26.mlp.up_proj | Ratio: 0.500 -> Target Rank: 1496 (Align: 8)layer module Linear(in_features=4096, out_features=11008, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Regularizing factor (try 1, alpha=1.00e-05)  Factor is positive definite (alpha=1.00e-04)  Cholesky decomposition successful.  Performing SVD on transformed matrix (11008x4096)...  SVD complete. Singular values (top 5): [5.6883384e-04 1.2054178e-04 1.2884599e-05 1.1027682e-05 9.3955405e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (1496,4096), (11008,1496)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))2025-04-27 22:40:39,853 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=1496, bias=False)  (1): Linear(in_features=1496, out_features=11008, bias=False))')2025-04-27 22:40:39,854 - INFO - Replacing 'model.layers.26.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)Compressing Layers:  84%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                        | 188/225 [35:40<07:21, 11.94s/it]2025-04-27 22:40:39,854 - INFO - Skipping layer model.layers.26.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:40:39,854 - INFO - Skipping layer model.layers.27.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:40:39,854 - INFO - Skipping layer model.layers.27.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:40:39,854 - INFO - Skipping layer model.layers.27.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:40:39,854 - INFO - Skipping layer model.layers.27.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:40:39,854 - INFO - Skipping layer model.layers.27.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:40:39,854 - INFO - Skipping layer model.layers.27.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:40:39,854 - INFO - Skipping layer model.layers.27.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:40:39,854 - INFO - Skipping layer model.layers.28.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:40:39,854 - INFO - Layer: model.layers.28.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)2025-04-27 22:40:39,855 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_28_self_attn_k_proj.safetensors2025-04-27 22:40:39,855 - INFO - exists: True2025-04-27 22:40:39,869 - INFO - factorize_layer_kron_svd2025-04-27 22:40:40,984 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:40:42,095 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:40:43,186 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:40:44,298 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:40:45,363 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:40:46,577 - INFO -   Factor is positive definite (alpha=1.00e+00)2025-04-27 22:40:47,651 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)2025-04-27 22:40:48,720 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)2025-04-27 22:40:49,797 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)2025-04-27 22:40:50,891 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)2025-04-27 22:40:51,959 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)2025-04-27 22:40:53,186 - INFO -   Factor is positive definite (alpha=1.00e+00)factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_28_self_attn_k_proj.safetensors TrueLayer: model.layers.28.self_attn.k_proj | Ratio: 0.300 -> Target Rank: 616 (Align: 8)layer module Linear(in_features=4096, out_features=4096, bias=False)factorize_layer_kron_svdRegularizing factors for layer (initial alpha=1.00e-05)...  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Regularizing factor (try 1, alpha=1.00e-05)  Regularizing factor (try 2, alpha=1.00e-04)  Regularizing factor (try 3, alpha=1.00e-03)  Regularizing factor (try 4, alpha=1.00e-02)  Regularizing factor (try 5, alpha=1.00e-01)  Factor is positive definite (alpha=1.00e+00)  Cholesky decomposition successful.  Performing SVD on transformed matrix (4096x4096)...  SVD complete. Singular values (top 5): [1.3403688e-05 6.0787293e-06 5.6532840e-06 5.4129196e-06 5.2258588e-06]  Cholesky factor inverses computed.  Factorization complete for layer. New shapes: (616,4096), (4096,616)factorized_sequential Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))2025-04-27 22:41:19,160 - INFO - factorized_sequential 'Sequential(  (0): Linear(in_features=4096, out_features=616, bias=False)  (1): Linear(in_features=616, out_features=4096, bias=False))')2025-04-27 22:41:19,160 - INFO - Replacing 'model.layers.28.self_attn.k_proj' (attribute 'k_proj' within parent LlamaAttention)Compressing Layers:  88%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                  | 198/225 [36:20<03:38,  8.09s/it]2025-04-27 22:41:19,160 - INFO - Skipping layer model.layers.28.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,160 - INFO - Skipping layer model.layers.28.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,160 - INFO - Skipping layer model.layers.28.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,160 - INFO - Skipping layer model.layers.28.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,160 - INFO - Skipping layer model.layers.28.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,160 - INFO - Skipping layer model.layers.29.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,160 - INFO - Skipping layer model.layers.29.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,160 - INFO - Skipping layer model.layers.29.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.29.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.29.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.29.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.29.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.30.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.30.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.30.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.30.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.30.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.30.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.30.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.31.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.31.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.31.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.31.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.31.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.31.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer model.layers.31.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.2025-04-27 22:41:19,161 - INFO - Skipping layer lm_head: Sensitivity ratio >= 1.0 (1.00). No compression.Compressing Layers: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [36:20<00:00,  9.69s/it]2025-04-27 22:41:19,161 - INFO - Compression finished. Processed: 46, Skipped (Ratio>=1 or No Sensitivity/Factors): 179, Failed: 02025-04-27 22:41:19,163 - INFO - Total parameters after compression: 59943096322025-04-27 22:41:19,163 - INFO - C rate : 0.88957256031623212025-04-27 22:41:19,163 - INFO - Saving compressed model to ./llama102025-04-27 22:41:19,163 - INFO - Compressed model and tokenizer saved.2025-04-27 22:41:19,172 - INFO - Evaluating on wikitext2Evaluating:   0%|                                                                                                                                                                          | 0/21 [00:00<?, ?it/s]Evaluating:   5%|███████▋                                                                                                                                                          | 1/21 [00:03<01:15,  3.77s/it]Evaluating:  10%|███████████████▍                                                                                                                                                  | 2/21 [00:04<00:42,  2.22s/it]Evaluating:  14%|███████████████████████▏                                                                                                                                          | 3/21 [00:06<00:30,  1.72s/it]Evaluating:  19%|██████████████████████████████▊                                                                                                                                   | 4/21 [00:07<00:25,  1.49s/it]Evaluating:  24%|██████████████████████████████████████▌                                                                                                                           | 5/21 [00:08<00:21,  1.36s/it]Evaluating:  29%|██████████████████████████████████████████████▎                                                                                                                   | 6/21 [00:09<00:19,  1.28s/it]Evaluating:  33%|██████████████████████████████████████████████████████                                                                                                            | 7/21 [00:10<00:17,  1.23s/it]Evaluating:  38%|█████████████████████████████████████████████████████████████▋                                                                                                    | 8/21 [00:11<00:15,  1.20s/it]Evaluating:  43%|█████████████████████████████████████████████████████████████████████▍                                                                                            | 9/21 [00:12<00:14,  1.18s/it]Evaluating:  48%|████████████████████████████████████████████████████████████████████████████▋                                                                                    | 10/21 [00:13<00:12,  1.17s/it]Evaluating:  52%|████████████████████████████████████████████████████████████████████████████████████▎                                                                            | 11/21 [00:15<00:11,  1.16s/it]Evaluating:  57%|████████████████████████████████████████████████████████████████████████████████████████████                                                                     | 12/21 [00:16<00:10,  1.15s/it]Evaluating:  62%|███████████████████████████████████████████████████████████████████████████████████████████████████▋                                                             | 13/21 [00:17<00:09,  1.14s/it]Evaluating:  67%|███████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                     | 14/21 [00:18<00:07,  1.14s/it]Evaluating:  71%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 15/21 [00:19<00:06,  1.14s/it]Evaluating:  76%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                      | 16/21 [00:20<00:05,  1.14s/it]Evaluating:  81%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                              | 17/21 [00:21<00:04,  1.14s/it]Evaluating:  86%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 18/21 [00:23<00:03,  1.13s/it]Evaluating:  90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋               | 19/21 [00:24<00:02,  1.13s/it]Evaluating:  95%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎       | 20/21 [00:25<00:01,  1.13s/it]Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:26<00:00,  1.06s/it]Evaluating: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 21/21 [00:26<00:00,  1.25s/it]2025-04-27 22:41:52,570 - INFO - wikitext2 perplexity: 9.18752025-04-27 22:41:52,571 - INFO - Evaluating on ptbnlls.shape torch.Size([16376])Mean NLL: 2.21875Evaluating:   0%|                                                                                                                                                                           | 0/7 [00:00<?, ?it/s]Evaluating:  14%|███████████████████████▎                                                                                                                                           | 1/7 [00:01<00:06,  1.13s/it]Evaluating:  29%|██████████████████████████████████████████████▌                                                                                                                    | 2/7 [00:02<00:05,  1.13s/it]Evaluating:  43%|█████████████████████████████████████████████████████████████████████▊                                                                                             | 3/7 [00:03<00:04,  1.13s/it]Evaluating:  57%|█████████████████████████████████████████████████████████████████████████████████████████████▏                                                                     | 4/7 [00:04<00:03,  1.13s/it]Evaluating:  71%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                              | 5/7 [00:05<00:02,  1.13s/it]Evaluating:  86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                       | 6/7 [00:06<00:01,  1.13s/it]Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:07<00:00,  1.09s/it]Evaluating: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:07<00:00,  1.11s/it]2025-04-27 22:42:02,754 - INFO - ptb perplexity: 37.50002025-04-27 22:42:02,754 - INFO - Evaluation results:2025-04-27 22:42:02,754 - INFO -   wikitext2: 9.18752025-04-27 22:42:02,754 - INFO -   ptb: 37.5000nlls.shape torch.Size([16376])Mean NLL: 3.625
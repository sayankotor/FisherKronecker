{'lm_head': 1, 'model.layers.31.mlp.gate_proj': 0.72, 'model.layers.31.mlp.up_proj': 0.72, 'model.layers.31.mlp.down_proj': 1, 'model.layers.31.self_attn.q_proj': 1, 'model.layers.31.self_attn.k_proj': 1, 'model.layers.31.self_attn.v_proj': 1, 'model.layers.31.self_attn.o_proj': 1, 'model.layers.30.mlp.gate_proj': 0.72, 'model.layers.30.mlp.up_proj': 1, 'model.layers.30.mlp.down_proj': 1, 'model.layers.30.self_attn.q_proj': 1, 'model.layers.30.self_attn.k_proj': 1, 'model.layers.30.self_attn.v_proj': 1, 'model.layers.30.self_attn.o_proj': 1, 'model.layers.29.mlp.gate_proj': 1, 'model.layers.29.mlp.up_proj': 0.72, 'model.layers.29.mlp.down_proj': 0.72, 'model.layers.29.self_attn.q_proj': 1, 'model.layers.29.self_attn.k_proj': 1, 'model.layers.29.self_attn.v_proj': 1, 'model.layers.29.self_attn.o_proj': 1, 'model.layers.28.mlp.gate_proj': 0.72, 'model.layers.28.mlp.up_proj': 1, 'model.layers.28.mlp.down_proj': 1, 'model.layers.28.self_attn.q_proj': 1, 'model.layers.28.self_attn.k_proj': 1, 'model.layers.28.self_attn.v_proj': 1, 'model.layers.28.self_attn.o_proj': 1, 'model.layers.27.mlp.gate_proj': 0.72, 'model.layers.27.mlp.up_proj': 1, 'model.layers.27.mlp.down_proj': 0.72, 'model.layers.27.self_attn.q_proj': 1, 'model.layers.27.self_attn.k_proj': 1, 'model.layers.27.self_attn.v_proj': 1, 'model.layers.27.self_attn.o_proj': 1, 'model.layers.26.mlp.gate_proj': 1, 'model.layers.26.mlp.up_proj': 1, 'model.layers.26.mlp.down_proj': 0.72, 'model.layers.26.self_attn.q_proj': 1, 'model.layers.26.self_attn.k_proj': 1, 'model.layers.26.self_attn.v_proj': 1, 'model.layers.26.self_attn.o_proj': 1, 'model.layers.25.mlp.gate_proj': 1, 'model.layers.25.mlp.up_proj': 0.72, 'model.layers.25.mlp.down_proj': 1, 'model.layers.25.self_attn.q_proj': 1, 'model.layers.25.self_attn.k_proj': 1, 'model.layers.25.self_attn.v_proj': 1, 'model.layers.25.self_attn.o_proj': 1, 'model.layers.24.mlp.gate_proj': 1, 'model.layers.24.mlp.up_proj': 1, 'model.layers.24.mlp.down_proj': 0.72, 'model.layers.24.self_attn.q_proj': 1, 'model.layers.24.self_attn.k_proj': 1, 'model.layers.24.self_attn.v_proj': 1, 'model.layers.24.self_attn.o_proj': 1, 'model.layers.23.mlp.gate_proj': 0.72, 'model.layers.23.mlp.up_proj': 1, 'model.layers.23.mlp.down_proj': 0.72, 'model.layers.23.self_attn.q_proj': 1, 'model.layers.23.self_attn.k_proj': 1, 'model.layers.23.self_attn.v_proj': 1, 'model.layers.23.self_attn.o_proj': 1, 'model.layers.22.mlp.gate_proj': 1, 'model.layers.22.mlp.up_proj': 1, 'model.layers.22.mlp.down_proj': 0.72, 'model.layers.22.self_attn.q_proj': 1, 'model.layers.22.self_attn.k_proj': 1, 'model.layers.22.self_attn.v_proj': 1, 'model.layers.22.self_attn.o_proj': 1, 'model.layers.21.mlp.gate_proj': 1, 'model.layers.21.mlp.up_proj': 0.72, 'model.layers.21.mlp.down_proj': 1, 'model.layers.21.self_attn.q_proj': 1, 'model.layers.21.self_attn.k_proj': 1, 'model.layers.21.self_attn.v_proj': 1, 'model.layers.21.self_attn.o_proj': 1, 'model.layers.20.mlp.gate_proj': 0.72, 'model.layers.20.mlp.up_proj': 1, 'model.layers.20.mlp.down_proj': 1, 'model.layers.20.self_attn.q_proj': 1, 'model.layers.20.self_attn.k_proj': 1, 'model.layers.20.self_attn.v_proj': 1, 'model.layers.20.self_attn.o_proj': 1, 'model.layers.19.mlp.gate_proj': 1, 'model.layers.19.mlp.up_proj': 1, 'model.layers.19.mlp.down_proj': 0.72, 'model.layers.19.self_attn.q_proj': 1, 'model.layers.19.self_attn.k_proj': 1, 'model.layers.19.self_attn.v_proj': 1, 'model.layers.19.self_attn.o_proj': 1, 'model.layers.18.mlp.gate_proj': 1, 'model.layers.18.mlp.up_proj': 0.72, 'model.layers.18.mlp.down_proj': 1, 'model.layers.18.self_attn.q_proj': 1, 'model.layers.18.self_attn.k_proj': 1, 'model.layers.18.self_attn.v_proj': 1, 'model.layers.18.self_attn.o_proj': 1, 'model.layers.17.mlp.gate_proj': 0.72, 'model.layers.17.mlp.up_proj': 0.72, 'model.layers.17.mlp.down_proj': 1, 'model.layers.17.self_attn.q_proj': 1, 'model.layers.17.self_attn.k_proj': 1, 'model.layers.17.self_attn.v_proj': 1, 'model.layers.17.self_attn.o_proj': 1, 'model.layers.16.mlp.gate_proj': 0.72, 'model.layers.16.mlp.up_proj': 1, 'model.layers.16.mlp.down_proj': 0.72, 'model.layers.16.self_attn.q_proj': 1, 'model.layers.16.self_attn.k_proj': 1, 'model.layers.16.self_attn.v_proj': 1, 'model.layers.16.self_attn.o_proj': 1, 'model.layers.15.mlp.gate_proj': 1, 'model.layers.15.mlp.up_proj': 0.72, 'model.layers.15.mlp.down_proj': 1, 'model.layers.15.self_attn.q_proj': 1, 'model.layers.15.self_attn.k_proj': 1, 'model.layers.15.self_attn.v_proj': 1, 'model.layers.15.self_attn.o_proj': 1, 'model.layers.14.mlp.gate_proj': 1, 'model.layers.14.mlp.up_proj': 0.72, 'model.layers.14.mlp.down_proj': 1, 'model.layers.14.self_attn.q_proj': 1, 'model.layers.14.self_attn.k_proj': 1, 'model.layers.14.self_attn.v_proj': 1, 'model.layers.14.self_attn.o_proj': 1, 'model.layers.13.mlp.gate_proj': 0.72, 'model.layers.13.mlp.up_proj': 0.72, 'model.layers.13.mlp.down_proj': 1, 'model.layers.13.self_attn.q_proj': 1, 'model.layers.13.self_attn.k_proj': 1, 'model.layers.13.self_attn.v_proj': 1, 'model.layers.13.self_attn.o_proj': 1, 'model.layers.12.mlp.gate_proj': 0.72, 'model.layers.12.mlp.up_proj': 0.72, 'model.layers.12.mlp.down_proj': 1, 'model.layers.12.self_attn.q_proj': 1, 'model.layers.12.self_attn.k_proj': 1, 'model.layers.12.self_attn.v_proj': 1, 'model.layers.12.self_attn.o_proj': 1, 'model.layers.11.mlp.gate_proj': 1, 'model.layers.11.mlp.up_proj': 0.72, 'model.layers.11.mlp.down_proj': 1, 'model.layers.11.self_attn.q_proj': 1, 'model.layers.11.self_attn.k_proj': 1, 'model.layers.11.self_attn.v_proj': 1, 'model.layers.11.self_attn.o_proj': 1, 'model.layers.10.mlp.gate_proj': 1, 'model.layers.10.mlp.up_proj': 0.72, 'model.layers.10.mlp.down_proj': 0.72, 'model.layers.10.self_attn.q_proj': 1, 'model.layers.10.self_attn.k_proj': 1, 'model.layers.10.self_attn.v_proj': 1, 'model.layers.10.self_attn.o_proj': 1, 'model.layers.9.mlp.gate_proj': 1, 'model.layers.9.mlp.up_proj': 1, 'model.layers.9.mlp.down_proj': 0.72, 'model.layers.9.self_attn.q_proj': 1, 'model.layers.9.self_attn.k_proj': 1, 'model.layers.9.self_attn.v_proj': 1, 'model.layers.9.self_attn.o_proj': 1, 'model.layers.8.mlp.gate_proj': 1, 'model.layers.8.mlp.up_proj': 0.72, 'model.layers.8.mlp.down_proj': 1, 'model.layers.8.self_attn.q_proj': 1, 'model.layers.8.self_attn.k_proj': 1, 'model.layers.8.self_attn.v_proj': 1, 'model.layers.8.self_attn.o_proj': 1, 'model.layers.7.mlp.gate_proj': 0.72, 'model.layers.7.mlp.up_proj': 0.72, 'model.layers.7.mlp.down_proj': 1, 'model.layers.7.self_attn.q_proj': 1, 'model.layers.7.self_attn.k_proj': 1, 'model.layers.7.self_attn.v_proj': 1, 'model.layers.7.self_attn.o_proj': 1, 'model.layers.6.mlp.gate_proj': 0.72, 'model.layers.6.mlp.up_proj': 1, 'model.layers.6.mlp.down_proj': 0.72, 'model.layers.6.self_attn.q_proj': 1, 'model.layers.6.self_attn.k_proj': 1, 'model.layers.6.self_attn.v_proj': 1, 'model.layers.6.self_attn.o_proj': 1, 'model.layers.5.mlp.gate_proj': 1, 'model.layers.5.mlp.up_proj': 1, 'model.layers.5.mlp.down_proj': 0.72, 'model.layers.5.self_attn.q_proj': 1, 'model.layers.5.self_attn.k_proj': 1, 'model.layers.5.self_attn.v_proj': 1, 'model.layers.5.self_attn.o_proj': 1, 'model.layers.4.mlp.gate_proj': 1, 'model.layers.4.mlp.up_proj': 0.72, 'model.layers.4.mlp.down_proj': 1, 'model.layers.4.self_attn.q_proj': 1, 'model.layers.4.self_attn.k_proj': 1, 'model.layers.4.self_attn.v_proj': 1, 'model.layers.4.self_attn.o_proj': 1, 'model.layers.3.mlp.gate_proj': 1, 'model.layers.3.mlp.up_proj': 1, 'model.layers.3.mlp.down_proj': 0.72, 'model.layers.3.self_attn.q_proj': 1, 'model.layers.3.self_attn.k_proj': 1, 'model.layers.3.self_attn.v_proj': 1, 'model.layers.3.self_attn.o_proj': 1, 'model.layers.2.mlp.gate_proj': 0.72, 'model.layers.2.mlp.up_proj': 1, 'model.layers.2.mlp.down_proj': 0.72, 'model.layers.2.self_attn.q_proj': 1, 'model.layers.2.self_attn.k_proj': 1, 'model.layers.2.self_attn.v_proj': 1, 'model.layers.2.self_attn.o_proj': 1, 'model.layers.1.mlp.gate_proj': 0.72, 'model.layers.1.mlp.up_proj': 0.72, 'model.layers.1.mlp.down_proj': 1, 'model.layers.1.self_attn.q_proj': 1, 'model.layers.1.self_attn.k_proj': 1, 'model.layers.1.self_attn.v_proj': 1, 'model.layers.1.self_attn.o_proj': 1, 'model.layers.0.mlp.gate_proj': 0.72, 'model.layers.0.mlp.up_proj': 0.72, 'model.layers.0.mlp.down_proj': 0.72, 'model.layers.0.self_attn.q_proj': 1, 'model.layers.0.self_attn.k_proj': 1, 'model.layers.0.self_attn.v_proj': 1, 'model.layers.0.self_attn.o_proj': 1}2025-04-26 17:04:18,993 - INFO - Loading model: unsloth/llama-2-7b-chat

[2025-04-26 17:04:24,077] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /home/jovyan/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
Loading checkpoint shards:   0%|                                                                                                                                                                                            | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|████████████████████████████████████████████████████████████                                                                                                                        | 1/3 [00:17<00:34, 17.11s/it]Loading checkpoint shards:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                            | 2/3 [00:41<00:21, 21.54s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:57<00:00, 18.93s/it]Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:57<00:00, 19.19s/it]
2025-04-26 17:05:24,544 - INFO - Model loaded with dtype torch.bfloat16
2025-04-26 17:06:08,949 - INFO - Model moved to cuda:0
2025-04-26 17:06:08,951 - INFO - Total parameters before compression: 6738415616
2025-04-26 17:06:08,952 - INFO - Found 225 linear layers to potentially compress.
Compressing Layers:   0%|                                                                                                                                                                                                 | 0/225 [00:00<?, ?it/s]2025-04-26 17:06:08,953 - INFO - Skipping layer model.layers.0.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:06:08,953 - INFO - Skipping layer model.layers.0.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:06:08,953 - INFO - Skipping layer model.layers.0.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:06:08,953 - INFO - Skipping layer model.layers.0.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:06:08,953 - INFO - Layer: model.layers.0.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:06:08,957 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_0_mlp_gate_proj.safetensors
2025-04-26 17:06:08,957 - INFO - exists: True
2025-04-26 17:06:08,959 - INFO - factorize_layer_kron_svd
2025-04-26 17:06:10,798 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:06:12,511 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:06:14,256 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 17:06:16,684 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:06:22,007 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_0_mlp_gate_proj.safetensors True
Layer: model.layers.0.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [8.1718399e-04 2.9179015e-05 1.2629871e-05 1.1120813e-05 5.3344133e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 17:07:45,477 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 17:07:45,477 - INFO - Replacing 'model.layers.0.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:   2%|████                                                                                                                                                                                   | 5/225 [01:36<1:10:47, 19.30s/it]2025-04-26 17:07:45,478 - INFO - Layer: model.layers.0.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:07:45,481 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_0_mlp_up_proj.safetensors
2025-04-26 17:07:45,481 - INFO - exists: True
2025-04-26 17:07:45,507 - INFO - factorize_layer_kron_svd
2025-04-26 17:07:47,301 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:07:48,949 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:07:50,744 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 17:07:53,178 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:07:57,974 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_0_mlp_up_proj.safetensors True
Layer: model.layers.0.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [4.4869381e-04 9.4277355e-05 2.2874417e-05 1.9138701e-05 1.4721376e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 17:09:29,868 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 17:09:29,868 - INFO - Replacing 'model.layers.0.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:   3%|████▉                                                                                                                                                                                  | 6/225 [03:20<2:19:28, 38.21s/it]2025-04-26 17:09:29,869 - INFO - Layer: model.layers.0.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:09:29,872 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_0_mlp_down_proj.safetensors
2025-04-26 17:09:29,872 - INFO - exists: True
2025-04-26 17:09:29,935 - INFO - factorize_layer_kron_svd
2025-04-26 17:09:32,480 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:09:35,043 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:09:37,601 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 17:09:40,097 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 17:09:42,624 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 17:09:47,623 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-26 17:09:49,047 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:09:50,507 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:09:51,945 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 17:09:53,409 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 17:09:54,854 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 17:09:56,558 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_0_mlp_down_proj.safetensors True
Layer: model.layers.0.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [8.8298384e-06 6.1057090e-06 5.6111639e-06 5.3944013e-06 5.1141064e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,11008), (4096,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)
2025-04-26 17:11:46,544 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)')
2025-04-26 17:11:46,545 - INFO - Replacing 'model.layers.0.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:   3%|█████▋                                                                                                                                                                                 | 7/225 [05:37<3:45:02, 61.94s/it]2025-04-26 17:11:46,546 - INFO - Skipping layer model.layers.1.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:11:46,546 - INFO - Skipping layer model.layers.1.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:11:46,546 - INFO - Skipping layer model.layers.1.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:11:46,546 - INFO - Skipping layer model.layers.1.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:11:46,546 - INFO - Layer: model.layers.1.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:11:46,550 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_1_mlp_gate_proj.safetensors
2025-04-26 17:11:46,550 - INFO - exists: True
2025-04-26 17:11:46,604 - INFO - factorize_layer_kron_svd
2025-04-26 17:11:48,500 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:11:49,977 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:11:51,431 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 17:11:52,910 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 17:11:54,355 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 17:11:56,025 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-26 17:11:58,335 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:12:00,983 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:12:03,440 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 17:12:05,924 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 17:12:08,517 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 17:12:13,317 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_1_mlp_gate_proj.safetensors True
Layer: model.layers.1.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.4005915e-05 9.7339671e-06 8.9150799e-06 8.8396546e-06 8.2917913e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 17:13:40,357 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 17:13:40,358 - INFO - Replacing 'model.layers.1.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:   5%|█████████▋                                                                                                                                                                            | 12/225 [07:31<2:11:54, 37.16s/it]2025-04-26 17:13:40,358 - INFO - Layer: model.layers.1.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:13:40,361 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_1_mlp_up_proj.safetensors
2025-04-26 17:13:40,362 - INFO - exists: True
2025-04-26 17:13:40,387 - INFO - factorize_layer_kron_svd
2025-04-26 17:13:42,185 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:13:43,952 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:13:45,777 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 17:13:48,180 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:13:53,672 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_1_mlp_up_proj.safetensors True
Layer: model.layers.1.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [8.3380165e-03 1.2735171e-05 4.8180482e-06 1.3797395e-06 1.0792503e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 17:15:21,766 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 17:15:21,767 - INFO - Replacing 'model.layers.1.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:   6%|██████████▌                                                                                                                                                                           | 13/225 [09:12<2:46:02, 46.99s/it]2025-04-26 17:15:21,767 - INFO - Skipping layer model.layers.1.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:15:21,767 - INFO - Skipping layer model.layers.2.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:15:21,767 - INFO - Skipping layer model.layers.2.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:15:21,767 - INFO - Skipping layer model.layers.2.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:15:21,768 - INFO - Skipping layer model.layers.2.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:15:21,768 - INFO - Layer: model.layers.2.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:15:21,770 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_gate_proj.safetensors
2025-04-26 17:15:21,770 - INFO - exists: True
2025-04-26 17:15:21,810 - INFO - factorize_layer_kron_svd
2025-04-26 17:15:23,667 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:15:25,163 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:15:26,620 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 17:15:28,113 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 17:15:29,558 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 17:15:31,344 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-26 17:15:33,729 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:15:36,328 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:15:38,878 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 17:15:41,363 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 17:15:43,857 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 17:15:49,151 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_gate_proj.safetensors True
Layer: model.layers.2.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.0536157e-05 7.8889116e-06 7.3416263e-06 6.9958951e-06 6.9346693e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 17:17:20,980 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 17:17:20,981 - INFO - Replacing 'model.layers.2.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:   8%|███████████████▎                                                                                                                                                                      | 19/225 [11:12<1:48:29, 31.60s/it]2025-04-26 17:17:20,981 - INFO - Skipping layer model.layers.2.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:17:20,981 - INFO - Layer: model.layers.2.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:17:20,985 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_down_proj.safetensors
2025-04-26 17:17:20,985 - INFO - exists: True
2025-04-26 17:17:21,033 - INFO - factorize_layer_kron_svd
2025-04-26 17:17:24,107 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:17:29,551 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-26 17:17:31,069 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:17:32,722 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:17:34,452 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_2_mlp_down_proj.safetensors True
Layer: model.layers.2.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [9.0164710e-03 9.7567434e-05 5.4176409e-05 2.1932610e-05 1.9526537e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,11008), (4096,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)
2025-04-26 17:20:08,765 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)')
2025-04-26 17:20:08,765 - INFO - Replacing 'model.layers.2.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:   9%|████████████████▉                                                                                                                                                                     | 21/225 [13:59<2:25:15, 42.73s/it]2025-04-26 17:20:08,766 - INFO - Skipping layer model.layers.3.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:20:08,766 - INFO - Skipping layer model.layers.3.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:20:08,766 - INFO - Skipping layer model.layers.3.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:20:08,766 - INFO - Skipping layer model.layers.3.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:20:08,766 - INFO - Skipping layer model.layers.3.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:20:08,766 - INFO - Skipping layer model.layers.3.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:20:08,767 - INFO - Layer: model.layers.3.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:20:08,769 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_mlp_down_proj.safetensors
2025-04-26 17:20:08,769 - INFO - exists: True
2025-04-26 17:20:08,816 - INFO - factorize_layer_kron_svd
2025-04-26 17:20:11,909 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:20:16,415 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:20:21,447 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 17:20:22,988 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:20:24,729 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_3_mlp_down_proj.safetensors True
Layer: model.layers.3.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [2.5388924e-03 3.4759837e-04 1.6566851e-04 1.1550266e-04 6.5852117e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,11008), (4096,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)
2025-04-26 17:23:12,848 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)')
2025-04-26 17:23:12,849 - INFO - Replacing 'model.layers.3.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  12%|██████████████████████▋                                                                                                                                                               | 28/225 [17:03<1:52:28, 34.26s/it]2025-04-26 17:23:12,849 - INFO - Skipping layer model.layers.4.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:23:12,849 - INFO - Skipping layer model.layers.4.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:23:12,849 - INFO - Skipping layer model.layers.4.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:23:12,849 - INFO - Skipping layer model.layers.4.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:23:12,849 - INFO - Skipping layer model.layers.4.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:23:12,849 - INFO - Layer: model.layers.4.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:23:12,853 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_mlp_up_proj.safetensors
2025-04-26 17:23:12,853 - INFO - exists: True
2025-04-26 17:23:12,897 - INFO - factorize_layer_kron_svd
2025-04-26 17:23:15,411 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:23:18,468 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:23:21,501 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 17:23:24,131 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 17:23:25,620 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 17:23:27,828 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-26 17:23:30,065 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:23:32,651 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:23:35,193 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 17:23:37,827 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 17:23:40,534 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 17:23:47,312 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_4_mlp_up_proj.safetensors True
Layer: model.layers.4.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [4.2674606e-06 3.9633169e-06 3.8188300e-06 3.7764739e-06 3.7316508e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 17:25:49,251 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 17:25:49,251 - INFO - Replacing 'model.layers.4.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  15%|███████████████████████████▌                                                                                                                                                          | 34/225 [19:40<1:38:57, 31.09s/it]2025-04-26 17:25:49,252 - INFO - Skipping layer model.layers.4.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:25:49,252 - INFO - Skipping layer model.layers.5.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:25:49,252 - INFO - Skipping layer model.layers.5.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:25:49,252 - INFO - Skipping layer model.layers.5.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:25:49,252 - INFO - Skipping layer model.layers.5.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:25:49,252 - INFO - Skipping layer model.layers.5.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:25:49,252 - INFO - Skipping layer model.layers.5.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:25:49,252 - INFO - Layer: model.layers.5.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:25:49,256 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_5_mlp_down_proj.safetensors
2025-04-26 17:25:49,256 - INFO - exists: True
2025-04-26 17:25:49,362 - INFO - factorize_layer_kron_svd
2025-04-26 17:25:53,590 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:26:03,681 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:26:09,373 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 17:26:12,589 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:26:16,177 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:26:20,079 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_5_mlp_down_proj.safetensors True
Layer: model.layers.5.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [4.2569707e-03 5.2021828e-04 9.3271483e-05 6.7189409e-05 5.4636472e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,11008), (4096,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)
2025-04-26 17:28:36,163 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)')
2025-04-26 17:28:36,163 - INFO - Replacing 'model.layers.5.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  19%|█████████████████████████████████▉                                                                                                                                                    | 42/225 [22:27<1:21:35, 26.75s/it]2025-04-26 17:28:36,164 - INFO - Skipping layer model.layers.6.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:28:36,164 - INFO - Skipping layer model.layers.6.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:28:36,164 - INFO - Skipping layer model.layers.6.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:28:36,164 - INFO - Skipping layer model.layers.6.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:28:36,164 - INFO - Layer: model.layers.6.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:28:36,167 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_mlp_gate_proj.safetensors
2025-04-26 17:28:36,167 - INFO - exists: True
2025-04-26 17:28:36,222 - INFO - factorize_layer_kron_svd
2025-04-26 17:28:38,143 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:28:40,764 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:28:43,777 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 17:28:46,713 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 17:28:48,490 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 17:28:50,558 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-26 17:28:53,073 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:28:55,679 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:28:58,444 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 17:29:01,282 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 17:29:04,279 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 17:29:09,266 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_mlp_gate_proj.safetensors True
Layer: model.layers.6.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.3830311e-05 7.5414341e-06 7.1951326e-06 7.1753102e-06 6.8685222e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 17:31:15,886 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 17:31:15,887 - INFO - Replacing 'model.layers.6.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  21%|██████████████████████████████████████                                                                                                                                                | 47/225 [25:06<1:23:35, 28.18s/it]2025-04-26 17:31:15,887 - INFO - Skipping layer model.layers.6.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:31:15,888 - INFO - Layer: model.layers.6.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:31:15,892 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_mlp_down_proj.safetensors
2025-04-26 17:31:15,892 - INFO - exists: True
2025-04-26 17:31:15,969 - INFO - factorize_layer_kron_svd
2025-04-26 17:31:23,175 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:31:33,488 - INFO -   Factor is positive definite (alpha=1.00e-04)
2025-04-26 17:31:35,014 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:31:36,893 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:31:39,670 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_6_mlp_down_proj.safetensors True
Layer: model.layers.6.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [1.1730455e-02 7.5274904e-05 5.8971975e-05 3.9316310e-05 2.7261001e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,11008), (4096,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)
2025-04-26 17:33:33,104 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)')
2025-04-26 17:33:33,105 - INFO - Replacing 'model.layers.6.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  22%|███████████████████████████████████████▋                                                                                                                                              | 49/225 [27:24<1:38:44, 33.66s/it]2025-04-26 17:33:33,105 - INFO - Skipping layer model.layers.7.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:33:33,105 - INFO - Skipping layer model.layers.7.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:33:33,105 - INFO - Skipping layer model.layers.7.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:33:33,105 - INFO - Skipping layer model.layers.7.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:33:33,105 - INFO - Layer: model.layers.7.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:33:33,110 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_gate_proj.safetensors
2025-04-26 17:33:33,110 - INFO - exists: True
2025-04-26 17:33:33,275 - INFO - factorize_layer_kron_svd
2025-04-26 17:33:37,108 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:33:40,688 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:33:43,669 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 17:33:46,761 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:33:57,502 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_gate_proj.safetensors True
Layer: model.layers.7.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [6.1598225e-03 4.3083266e-05 2.9594266e-05 2.0862246e-05 1.8714514e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 17:35:36,087 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 17:35:36,087 - INFO - Replacing 'model.layers.7.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  24%|███████████████████████████████████████████▋                                                                                                                                          | 54/225 [29:27<1:27:30, 30.70s/it]2025-04-26 17:35:36,088 - INFO - Layer: model.layers.7.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:35:36,091 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_up_proj.safetensors
2025-04-26 17:35:36,091 - INFO - exists: True
2025-04-26 17:35:36,208 - INFO - factorize_layer_kron_svd
2025-04-26 17:35:40,175 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:35:43,595 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:35:47,165 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 17:35:52,004 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:36:02,867 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_7_mlp_up_proj.safetensors True
Layer: model.layers.7.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [2.8216012e-03 3.8601193e-05 2.6869149e-05 2.3445984e-05 1.8939305e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 17:37:55,480 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 17:37:55,480 - INFO - Replacing 'model.layers.7.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  24%|████████████████████████████████████████████▍                                                                                                                                         | 55/225 [31:46<1:53:15, 39.98s/it]2025-04-26 17:37:55,481 - INFO - Skipping layer model.layers.7.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:37:55,481 - INFO - Skipping layer model.layers.8.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:37:55,481 - INFO - Skipping layer model.layers.8.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:37:55,481 - INFO - Skipping layer model.layers.8.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:37:55,481 - INFO - Skipping layer model.layers.8.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:37:55,481 - INFO - Skipping layer model.layers.8.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:37:55,481 - INFO - Layer: model.layers.8.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:37:55,483 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_mlp_up_proj.safetensors
2025-04-26 17:37:55,483 - INFO - exists: True
2025-04-26 17:37:55,604 - INFO - factorize_layer_kron_svd
2025-04-26 17:37:58,069 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:37:59,745 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:38:01,316 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 17:38:02,993 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 17:38:04,582 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 17:38:07,821 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-26 17:38:10,091 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:38:14,779 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:38:20,488 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 17:38:26,085 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 17:38:31,488 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 17:38:41,085 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_8_mlp_up_proj.safetensors True
Layer: model.layers.8.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [5.3510244e-06 4.6259770e-06 4.5105817e-06 4.3957298e-06 4.3850546e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 17:40:39,432 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 17:40:39,432 - INFO - Replacing 'model.layers.8.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  28%|██████████████████████████████████████████████████▏                                                                                                                                   | 62/225 [34:30<1:27:53, 32.36s/it]2025-04-26 17:40:39,433 - INFO - Skipping layer model.layers.8.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:40:39,433 - INFO - Skipping layer model.layers.9.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:40:39,433 - INFO - Skipping layer model.layers.9.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:40:39,433 - INFO - Skipping layer model.layers.9.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:40:39,433 - INFO - Skipping layer model.layers.9.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:40:39,433 - INFO - Skipping layer model.layers.9.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:40:39,433 - INFO - Skipping layer model.layers.9.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:40:39,433 - INFO - Layer: model.layers.9.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:40:39,434 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_9_mlp_down_proj.safetensors
2025-04-26 17:40:39,434 - INFO - exists: True
2025-04-26 17:40:39,511 - INFO - factorize_layer_kron_svd
2025-04-26 17:40:42,513 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:40:46,201 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:40:50,667 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 17:40:52,227 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:40:53,842 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:40:55,540 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_9_mlp_down_proj.safetensors True
Layer: model.layers.9.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [2.8424084e-02 1.8538581e-04 1.7036285e-04 1.0448481e-04 9.5918804e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,11008), (4096,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)
2025-04-26 17:43:05,473 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)')
2025-04-26 17:43:05,474 - INFO - Replacing 'model.layers.9.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  31%|████████████████████████████████████████████████████████▌                                                                                                                             | 70/225 [36:56<1:07:57, 26.30s/it]2025-04-26 17:43:05,475 - INFO - Skipping layer model.layers.10.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:43:05,475 - INFO - Skipping layer model.layers.10.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:43:05,475 - INFO - Skipping layer model.layers.10.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:43:05,475 - INFO - Skipping layer model.layers.10.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:43:05,475 - INFO - Skipping layer model.layers.10.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:43:05,475 - INFO - Layer: model.layers.10.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:43:05,480 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_10_mlp_up_proj.safetensors
2025-04-26 17:43:05,480 - INFO - exists: True
2025-04-26 17:43:05,604 - INFO - factorize_layer_kron_svd
2025-04-26 17:43:09,500 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:43:12,253 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:43:14,271 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 17:43:16,860 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:43:21,338 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_10_mlp_up_proj.safetensors True
Layer: model.layers.10.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [2.4830976e-03 1.3603101e-04 2.3816692e-05 2.0194741e-05 1.6129978e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 17:44:54,293 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 17:44:54,293 - INFO - Replacing 'model.layers.10.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  34%|██████████████████████████████████████████████████████████████▏                                                                                                                         | 76/225 [38:45<58:56, 23.73s/it]2025-04-26 17:44:54,294 - INFO - Layer: model.layers.10.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:44:54,297 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_10_mlp_down_proj.safetensors
2025-04-26 17:44:54,298 - INFO - exists: True
2025-04-26 17:44:54,473 - INFO - factorize_layer_kron_svd
2025-04-26 17:45:02,800 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:45:10,094 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:45:19,968 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 17:45:22,987 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:45:25,780 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:45:27,777 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_10_mlp_down_proj.safetensors True
Layer: model.layers.10.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [0.05109505 0.00017506 0.00013073 0.00011839 0.00010639]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,11008), (4096,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)
2025-04-26 17:47:16,065 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)')
2025-04-26 17:47:16,065 - INFO - Replacing 'model.layers.10.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  34%|██████████████████████████████████████████████████████████████▎                                                                                                                       | 77/225 [41:07<1:18:51, 31.97s/it]2025-04-26 17:47:16,065 - INFO - Skipping layer model.layers.11.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:47:16,066 - INFO - Skipping layer model.layers.11.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:47:16,066 - INFO - Skipping layer model.layers.11.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:47:16,066 - INFO - Skipping layer model.layers.11.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:47:16,066 - INFO - Skipping layer model.layers.11.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:47:16,066 - INFO - Layer: model.layers.11.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:47:16,068 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_mlp_up_proj.safetensors
2025-04-26 17:47:16,068 - INFO - exists: True
2025-04-26 17:47:16,099 - INFO - factorize_layer_kron_svd
2025-04-26 17:47:19,943 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:47:22,566 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:47:25,033 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 17:47:27,341 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:47:38,111 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:47:50,204 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_11_mlp_up_proj.safetensors True
Layer: model.layers.11.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [3.9379876e-03 8.7134307e-05 2.5501480e-05 2.0400832e-05 1.9031770e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 17:49:32,387 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 17:49:32,388 - INFO - Replacing 'model.layers.11.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  37%|███████████████████████████████████████████████████████████████████▏                                                                                                                  | 83/225 [43:23<1:07:27, 28.51s/it]2025-04-26 17:49:32,388 - INFO - Skipping layer model.layers.11.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:49:32,388 - INFO - Skipping layer model.layers.12.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:49:32,388 - INFO - Skipping layer model.layers.12.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:49:32,388 - INFO - Skipping layer model.layers.12.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:49:32,388 - INFO - Skipping layer model.layers.12.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:49:32,388 - INFO - Layer: model.layers.12.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:49:32,390 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_gate_proj.safetensors
2025-04-26 17:49:32,390 - INFO - exists: True
2025-04-26 17:49:32,468 - INFO - factorize_layer_kron_svd
2025-04-26 17:49:34,785 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:49:36,808 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:49:38,483 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 17:49:40,773 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 17:49:43,166 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 17:49:46,663 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-26 17:49:51,107 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:49:56,380 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:50:01,609 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 17:50:06,988 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 17:50:12,698 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 17:50:23,183 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_gate_proj.safetensors True
Layer: model.layers.12.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.2988171e-05 7.1681275e-06 6.5595987e-06 6.5212694e-06 6.4388596e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 17:52:10,236 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 17:52:10,236 - INFO - Replacing 'model.layers.12.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  40%|███████████████████████████████████████████████████████████████████████▉                                                                                                              | 89/225 [46:01<1:02:52, 27.74s/it]2025-04-26 17:52:10,237 - INFO - Layer: model.layers.12.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:52:10,239 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_up_proj.safetensors
2025-04-26 17:52:10,239 - INFO - exists: True
2025-04-26 17:52:10,313 - INFO - factorize_layer_kron_svd
2025-04-26 17:52:14,073 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:52:16,289 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:52:17,722 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 17:52:19,692 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 17:52:22,695 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 17:52:26,495 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-26 17:52:30,887 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:52:35,965 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:52:40,897 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 17:52:45,877 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 17:52:49,277 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 17:53:00,666 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_12_mlp_up_proj.safetensors True
Layer: model.layers.12.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [5.9819213e-06 4.8709753e-06 4.7432140e-06 4.6781370e-06 4.6636528e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 17:54:46,067 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 17:54:46,067 - INFO - Replacing 'model.layers.12.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  40%|████████████████████████████████████████████████████████████████████████▊                                                                                                             | 90/225 [48:37<1:24:29, 37.55s/it]2025-04-26 17:54:46,068 - INFO - Skipping layer model.layers.12.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:54:46,068 - INFO - Skipping layer model.layers.13.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:54:46,068 - INFO - Skipping layer model.layers.13.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:54:46,068 - INFO - Skipping layer model.layers.13.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:54:46,068 - INFO - Skipping layer model.layers.13.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:54:46,069 - INFO - Layer: model.layers.13.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:54:46,074 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_gate_proj.safetensors
2025-04-26 17:54:46,074 - INFO - exists: True
2025-04-26 17:54:46,100 - INFO - factorize_layer_kron_svd
2025-04-26 17:54:50,204 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:54:53,908 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:54:57,671 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 17:55:02,382 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:55:09,392 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_gate_proj.safetensors True
Layer: model.layers.13.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [6.0720551e-03 4.1635503e-05 2.5478646e-05 2.1152418e-05 1.8824872e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 17:56:46,753 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 17:56:46,754 - INFO - Replacing 'model.layers.13.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  43%|█████████████████████████████████████████████████████████████████████████████▋                                                                                                        | 96/225 [50:37<1:05:52, 30.64s/it]2025-04-26 17:56:46,755 - INFO - Layer: model.layers.13.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:56:46,756 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_up_proj.safetensors
2025-04-26 17:56:46,757 - INFO - exists: True
2025-04-26 17:56:46,835 - INFO - factorize_layer_kron_svd
2025-04-26 17:56:48,592 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:56:51,106 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:56:52,793 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 17:56:55,309 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:57:01,871 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:57:06,691 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_13_mlp_up_proj.safetensors True
Layer: model.layers.13.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [6.4682798e-03 6.1749437e-05 2.4349400e-05 2.1919277e-05 2.0116417e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 17:58:53,746 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 17:58:53,746 - INFO - Replacing 'model.layers.13.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  43%|██████████████████████████████████████████████████████████████████████████████▍                                                                                                       | 97/225 [52:44<1:23:05, 38.95s/it]2025-04-26 17:58:53,747 - INFO - Skipping layer model.layers.13.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:58:53,747 - INFO - Skipping layer model.layers.14.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:58:53,747 - INFO - Skipping layer model.layers.14.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:58:53,747 - INFO - Skipping layer model.layers.14.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:58:53,747 - INFO - Skipping layer model.layers.14.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:58:53,747 - INFO - Skipping layer model.layers.14.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 17:58:53,747 - INFO - Layer: model.layers.14.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 17:58:53,749 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_up_proj.safetensors
2025-04-26 17:58:53,749 - INFO - exists: True
2025-04-26 17:58:53,827 - INFO - factorize_layer_kron_svd
2025-04-26 17:58:56,004 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:58:58,005 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:59:00,301 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 17:59:02,619 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 17:59:06,822 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 17:59:12,135 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_14_mlp_up_proj.safetensors True
Layer: model.layers.14.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [9.7202640e-03 2.5967920e-05 2.1275189e-05 1.6423986e-05 1.5875536e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 18:01:18,669 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 18:01:18,671 - INFO - Replacing 'model.layers.14.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  46%|███████████████████████████████████████████████████████████████████████████████████▋                                                                                                 | 104/225 [55:09<1:01:30, 30.50s/it]2025-04-26 18:01:18,671 - INFO - Skipping layer model.layers.14.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:01:18,671 - INFO - Skipping layer model.layers.15.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:01:18,671 - INFO - Skipping layer model.layers.15.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:01:18,671 - INFO - Skipping layer model.layers.15.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:01:18,671 - INFO - Skipping layer model.layers.15.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:01:18,671 - INFO - Skipping layer model.layers.15.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:01:18,672 - INFO - Layer: model.layers.15.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:01:18,677 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_mlp_up_proj.safetensors
2025-04-26 18:01:18,677 - INFO - exists: True
2025-04-26 18:01:18,864 - INFO - factorize_layer_kron_svd
2025-04-26 18:01:24,909 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:01:26,515 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:01:29,422 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:01:31,712 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:01:35,415 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:01:39,735 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_15_mlp_up_proj.safetensors True
Layer: model.layers.15.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [5.9048464e-03 3.4873661e-05 1.9737899e-05 1.7797654e-05 1.4542696e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 18:03:11,076 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 18:03:11,076 - INFO - Replacing 'model.layers.15.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  49%|██████████████████████████████████████████████████████████████████████████████████████████▎                                                                                            | 111/225 [57:02<47:01, 24.75s/it]2025-04-26 18:03:11,077 - INFO - Skipping layer model.layers.15.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:03:11,077 - INFO - Skipping layer model.layers.16.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:03:11,077 - INFO - Skipping layer model.layers.16.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:03:11,077 - INFO - Skipping layer model.layers.16.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:03:11,077 - INFO - Skipping layer model.layers.16.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:03:11,077 - INFO - Layer: model.layers.16.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:03:11,081 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_mlp_gate_proj.safetensors
2025-04-26 18:03:11,081 - INFO - exists: True
2025-04-26 18:03:11,103 - INFO - factorize_layer_kron_svd
2025-04-26 18:03:15,163 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:03:17,355 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:03:20,473 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:03:25,508 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:03:31,405 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_mlp_gate_proj.safetensors True
Layer: model.layers.16.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [3.0645484e-03 2.1843742e-05 8.2429324e-06 3.9983265e-06 3.0144206e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 18:04:51,569 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 18:04:51,570 - INFO - Replacing 'model.layers.16.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  52%|███████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                       | 117/225 [58:42<39:49, 22.13s/it]2025-04-26 18:04:51,570 - INFO - Skipping layer model.layers.16.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:04:51,570 - INFO - Layer: model.layers.16.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:04:51,573 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_mlp_down_proj.safetensors
2025-04-26 18:04:51,573 - INFO - exists: True
2025-04-26 18:04:51,639 - INFO - factorize_layer_kron_svd
2025-04-26 18:04:54,987 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:05:02,585 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:05:12,490 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:05:15,683 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:05:19,371 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:05:23,213 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_16_mlp_down_proj.safetensors True
Layer: model.layers.16.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [5.7327412e-02 1.1083911e-04 5.4754466e-05 5.0081719e-05 4.2930162e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,11008), (4096,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)
2025-04-26 18:07:36,032 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)')
2025-04-26 18:07:36,033 - INFO - Replacing 'model.layers.16.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  53%|███████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                     | 119/225 [1:01:27<53:25, 30.24s/it]2025-04-26 18:07:36,033 - INFO - Skipping layer model.layers.17.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:07:36,033 - INFO - Skipping layer model.layers.17.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:07:36,033 - INFO - Skipping layer model.layers.17.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:07:36,033 - INFO - Skipping layer model.layers.17.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:07:36,033 - INFO - Layer: model.layers.17.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:07:36,037 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_mlp_gate_proj.safetensors
2025-04-26 18:07:36,037 - INFO - exists: True
2025-04-26 18:07:36,104 - INFO - factorize_layer_kron_svd
2025-04-26 18:07:38,120 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:07:39,740 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:07:42,231 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:07:45,933 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:07:51,463 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_mlp_gate_proj.safetensors True
Layer: model.layers.17.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.2734500e-02 1.9533880e-05 1.8360362e-05 1.5285072e-05 1.4013754e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 18:09:42,119 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 18:09:42,119 - INFO - Replacing 'model.layers.17.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  55%|███████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                 | 124/225 [1:03:33<48:09, 28.61s/it]2025-04-26 18:09:42,120 - INFO - Layer: model.layers.17.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:09:42,121 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_mlp_up_proj.safetensors
2025-04-26 18:09:42,121 - INFO - exists: True
2025-04-26 18:09:42,184 - INFO - factorize_layer_kron_svd
2025-04-26 18:09:44,084 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:09:46,891 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:09:48,823 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:09:51,373 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:09:56,271 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_17_mlp_up_proj.safetensors True
Layer: model.layers.17.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [5.35030011e-03 2.14593056e-05 1.69268369e-05 1.56144724e-05
 1.40765405e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 18:12:13,075 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 18:12:13,075 - INFO - Replacing 'model.layers.17.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  56%|███████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                               | 125/225 [1:06:04<1:05:01, 39.01s/it]2025-04-26 18:12:13,076 - INFO - Skipping layer model.layers.17.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:12:13,076 - INFO - Skipping layer model.layers.18.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:12:13,076 - INFO - Skipping layer model.layers.18.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:12:13,076 - INFO - Skipping layer model.layers.18.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:12:13,076 - INFO - Skipping layer model.layers.18.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:12:13,076 - INFO - Skipping layer model.layers.18.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:12:13,076 - INFO - Layer: model.layers.18.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:12:13,081 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_mlp_up_proj.safetensors
2025-04-26 18:12:13,081 - INFO - exists: True
2025-04-26 18:12:13,191 - INFO - factorize_layer_kron_svd
2025-04-26 18:12:16,901 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:12:20,567 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:12:23,345 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:12:25,756 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:12:31,464 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_18_mlp_up_proj.safetensors True
Layer: model.layers.18.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [5.7937209e-03 2.0253658e-05 1.6411330e-05 1.4584949e-05 1.2499403e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 18:13:44,145 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 18:13:44,146 - INFO - Replacing 'model.layers.18.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  59%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                          | 132/225 [1:07:35<41:56, 27.06s/it]2025-04-26 18:13:44,146 - INFO - Skipping layer model.layers.18.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:13:44,146 - INFO - Skipping layer model.layers.19.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:13:44,146 - INFO - Skipping layer model.layers.19.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:13:44,146 - INFO - Skipping layer model.layers.19.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:13:44,146 - INFO - Skipping layer model.layers.19.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:13:44,147 - INFO - Skipping layer model.layers.19.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:13:44,147 - INFO - Skipping layer model.layers.19.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:13:44,147 - INFO - Layer: model.layers.19.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:13:44,157 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_mlp_down_proj.safetensors
2025-04-26 18:13:44,158 - INFO - exists: True
2025-04-26 18:13:44,224 - INFO - factorize_layer_kron_svd
2025-04-26 18:13:46,968 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:13:50,732 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:13:55,246 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:13:56,721 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:13:58,337 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:14:00,155 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_19_mlp_down_proj.safetensors True
Layer: model.layers.19.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [3.5986744e-02 9.2276161e-05 4.2714488e-05 3.8102378e-05 3.5181394e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,11008), (4096,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)
2025-04-26 18:15:42,465 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)')
2025-04-26 18:15:42,465 - INFO - Replacing 'model.layers.19.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                    | 140/225 [1:09:33<30:53, 21.80s/it]2025-04-26 18:15:42,466 - INFO - Skipping layer model.layers.20.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:15:42,466 - INFO - Skipping layer model.layers.20.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:15:42,466 - INFO - Skipping layer model.layers.20.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:15:42,466 - INFO - Skipping layer model.layers.20.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:15:42,466 - INFO - Layer: model.layers.20.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:15:42,468 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_20_mlp_gate_proj.safetensors
2025-04-26 18:15:42,468 - INFO - exists: True
2025-04-26 18:15:42,528 - INFO - factorize_layer_kron_svd
2025-04-26 18:15:44,487 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:15:46,140 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:15:48,052 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:15:50,524 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:15:55,376 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_20_mlp_gate_proj.safetensors True
Layer: model.layers.20.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [5.6294589e-03 2.1218650e-05 1.5237835e-05 1.4524096e-05 1.3646030e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 18:17:19,403 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 18:17:19,404 - INFO - Replacing 'model.layers.20.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  64%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                | 145/225 [1:11:10<28:10, 21.13s/it]2025-04-26 18:17:19,405 - INFO - Skipping layer model.layers.20.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:17:19,405 - INFO - Skipping layer model.layers.20.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:17:19,405 - INFO - Skipping layer model.layers.21.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:17:19,405 - INFO - Skipping layer model.layers.21.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:17:19,405 - INFO - Skipping layer model.layers.21.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:17:19,405 - INFO - Skipping layer model.layers.21.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:17:19,405 - INFO - Skipping layer model.layers.21.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:17:19,405 - INFO - Layer: model.layers.21.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:17:19,409 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_up_proj.safetensors
2025-04-26 18:17:19,409 - INFO - exists: True
2025-04-26 18:17:19,437 - INFO - factorize_layer_kron_svd
2025-04-26 18:17:21,307 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:17:23,028 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:17:24,725 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:17:27,383 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:17:32,971 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_21_mlp_up_proj.safetensors True
Layer: model.layers.21.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.8695735e-03 5.5963137e-05 1.4356574e-05 1.3274203e-05 1.2030486e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 18:19:00,076 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 18:19:00,077 - INFO - Replacing 'model.layers.21.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  68%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                          | 153/225 [1:12:51<21:23, 17.82s/it]2025-04-26 18:19:00,077 - INFO - Skipping layer model.layers.21.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:19:00,077 - INFO - Skipping layer model.layers.22.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:19:00,077 - INFO - Skipping layer model.layers.22.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:19:00,077 - INFO - Skipping layer model.layers.22.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:19:00,077 - INFO - Skipping layer model.layers.22.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:19:00,077 - INFO - Skipping layer model.layers.22.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:19:00,077 - INFO - Skipping layer model.layers.22.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:19:00,077 - INFO - Layer: model.layers.22.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:19:00,081 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_down_proj.safetensors
2025-04-26 18:19:00,081 - INFO - exists: True
2025-04-26 18:19:00,143 - INFO - factorize_layer_kron_svd
2025-04-26 18:19:02,788 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:19:06,034 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:19:10,761 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:19:12,259 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:19:13,834 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:19:15,620 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_22_mlp_down_proj.safetensors True
Layer: model.layers.22.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [2.3855805e-02 8.0862206e-05 5.0205126e-05 4.2752916e-05 3.9248724e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,11008), (4096,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)
2025-04-26 18:20:49,953 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)')
2025-04-26 18:20:49,954 - INFO - Replacing 'model.layers.22.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  72%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                   | 161/225 [1:14:41<17:27, 16.36s/it]2025-04-26 18:20:49,955 - INFO - Skipping layer model.layers.23.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:20:49,955 - INFO - Skipping layer model.layers.23.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:20:49,955 - INFO - Skipping layer model.layers.23.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:20:49,955 - INFO - Skipping layer model.layers.23.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:20:49,955 - INFO - Layer: model.layers.23.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:20:49,958 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_gate_proj.safetensors
2025-04-26 18:20:49,958 - INFO - exists: True
2025-04-26 18:20:50,000 - INFO - factorize_layer_kron_svd
2025-04-26 18:20:51,820 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:20:53,536 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:20:55,166 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:20:57,720 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:21:02,561 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_gate_proj.safetensors True
Layer: model.layers.23.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.6312244e-03 1.5538668e-05 4.1256385e-06 2.1622898e-06 2.0991954e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 18:22:26,364 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 18:22:26,364 - INFO - Replacing 'model.layers.23.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  74%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                               | 166/225 [1:16:17<16:47, 17.07s/it]2025-04-26 18:22:26,365 - INFO - Skipping layer model.layers.23.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:22:26,365 - INFO - Layer: model.layers.23.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:22:26,366 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_down_proj.safetensors
2025-04-26 18:22:26,367 - INFO - exists: True
2025-04-26 18:22:26,455 - INFO - factorize_layer_kron_svd
2025-04-26 18:22:29,075 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:22:32,708 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:22:37,591 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:22:39,116 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:22:40,758 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:22:42,469 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_23_mlp_down_proj.safetensors True
Layer: model.layers.23.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [6.6739982e-03 2.3223182e-04 4.9300255e-05 3.7862912e-05 3.5104458e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,11008), (4096,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)
2025-04-26 18:24:30,942 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)')
2025-04-26 18:24:30,942 - INFO - Replacing 'model.layers.23.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                             | 168/225 [1:18:21<21:25, 22.55s/it]2025-04-26 18:24:30,943 - INFO - Skipping layer model.layers.24.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:24:30,943 - INFO - Skipping layer model.layers.24.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:24:30,943 - INFO - Skipping layer model.layers.24.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:24:30,943 - INFO - Skipping layer model.layers.24.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:24:30,943 - INFO - Skipping layer model.layers.24.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:24:30,943 - INFO - Skipping layer model.layers.24.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:24:30,943 - INFO - Layer: model.layers.24.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:24:30,946 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_24_mlp_down_proj.safetensors
2025-04-26 18:24:30,946 - INFO - exists: True
2025-04-26 18:24:30,987 - INFO - factorize_layer_kron_svd
2025-04-26 18:24:33,632 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:24:36,183 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:24:38,787 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 18:24:41,480 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 18:24:44,041 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 18:24:48,398 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-26 18:24:49,856 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:24:51,352 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:24:52,830 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 18:24:54,295 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 18:24:55,757 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 18:24:57,560 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_24_mlp_down_proj.safetensors True
Layer: model.layers.24.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [5.5479163e-06 4.7517065e-06 4.1828225e-06 4.1455337e-06 4.0539248e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,11008), (4096,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)
2025-04-26 18:27:26,263 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)')
2025-04-26 18:27:26,264 - INFO - Replacing 'model.layers.24.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  78%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                        | 175/225 [1:21:17<19:34, 23.49s/it]2025-04-26 18:27:26,264 - INFO - Skipping layer model.layers.25.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:27:26,264 - INFO - Skipping layer model.layers.25.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:27:26,264 - INFO - Skipping layer model.layers.25.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:27:26,264 - INFO - Skipping layer model.layers.25.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:27:26,265 - INFO - Skipping layer model.layers.25.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:27:26,265 - INFO - Layer: model.layers.25.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:27:26,268 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_25_mlp_up_proj.safetensors
2025-04-26 18:27:26,268 - INFO - exists: True
2025-04-26 18:27:26,329 - INFO - factorize_layer_kron_svd
2025-04-26 18:27:28,219 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:27:29,846 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:27:31,539 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:27:34,081 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:27:38,685 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_25_mlp_up_proj.safetensors True
Layer: model.layers.25.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [5.2369884e-03 1.2808485e-05 1.0910483e-05 1.0035876e-05 9.8339870e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 18:28:58,055 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 18:28:58,055 - INFO - Replacing 'model.layers.25.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                   | 181/225 [1:22:49<15:19, 20.90s/it]2025-04-26 18:28:58,056 - INFO - Skipping layer model.layers.25.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:28:58,056 - INFO - Skipping layer model.layers.26.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:28:58,056 - INFO - Skipping layer model.layers.26.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:28:58,056 - INFO - Skipping layer model.layers.26.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:28:58,056 - INFO - Skipping layer model.layers.26.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:28:58,056 - INFO - Skipping layer model.layers.26.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:28:58,056 - INFO - Skipping layer model.layers.26.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:28:58,056 - INFO - Layer: model.layers.26.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:28:58,058 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_mlp_down_proj.safetensors
2025-04-26 18:28:58,058 - INFO - exists: True
2025-04-26 18:28:58,123 - INFO - factorize_layer_kron_svd
2025-04-26 18:29:04,177 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:29:13,105 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:29:27,211 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:29:30,483 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:29:34,401 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:29:37,571 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_26_mlp_down_proj.safetensors True
Layer: model.layers.26.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [4.1444181e-03 7.6422427e-04 4.0036848e-05 3.7765723e-05 3.1555592e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,11008), (4096,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)
2025-04-26 18:32:04,076 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)')
2025-04-26 18:32:04,077 - INFO - Replacing 'model.layers.26.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                             | 189/225 [1:25:55<13:04, 21.79s/it]2025-04-26 18:32:04,077 - INFO - Skipping layer model.layers.27.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:32:04,077 - INFO - Skipping layer model.layers.27.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:32:04,077 - INFO - Skipping layer model.layers.27.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:32:04,077 - INFO - Skipping layer model.layers.27.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:32:04,077 - INFO - Layer: model.layers.27.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:32:04,082 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_27_mlp_gate_proj.safetensors
2025-04-26 18:32:04,083 - INFO - exists: True
2025-04-26 18:32:04,201 - INFO - factorize_layer_kron_svd
2025-04-26 18:32:08,477 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:32:11,800 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:32:15,502 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:32:19,616 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:32:29,011 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_27_mlp_gate_proj.safetensors True
Layer: model.layers.27.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [8.5178309e-04 1.7080700e-04 1.5016245e-05 1.2949634e-05 1.1322722e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 18:34:10,198 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 18:34:10,199 - INFO - Replacing 'model.layers.27.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  86%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                         | 194/225 [1:28:01<11:42, 22.65s/it]2025-04-26 18:34:10,199 - INFO - Skipping layer model.layers.27.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:34:10,199 - INFO - Layer: model.layers.27.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:34:10,203 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_27_mlp_down_proj.safetensors
2025-04-26 18:34:10,203 - INFO - exists: True
2025-04-26 18:34:10,299 - INFO - factorize_layer_kron_svd
2025-04-26 18:34:15,985 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:34:22,797 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:34:32,487 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:34:35,543 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:34:37,649 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:34:40,491 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_27_mlp_down_proj.safetensors True
Layer: model.layers.27.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [4.3249605e-03 9.9447533e-04 4.9234852e-05 3.5841313e-05 3.4881272e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,11008), (4096,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)
2025-04-26 18:36:39,847 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)')
2025-04-26 18:36:39,848 - INFO - Replacing 'model.layers.27.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                       | 196/225 [1:30:30<14:06, 29.20s/it]2025-04-26 18:36:39,848 - INFO - Skipping layer model.layers.28.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:36:39,848 - INFO - Skipping layer model.layers.28.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:36:39,848 - INFO - Skipping layer model.layers.28.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:36:39,848 - INFO - Skipping layer model.layers.28.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:36:39,848 - INFO - Layer: model.layers.28.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:36:39,852 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_28_mlp_gate_proj.safetensors
2025-04-26 18:36:39,852 - INFO - exists: True
2025-04-26 18:36:39,879 - INFO - factorize_layer_kron_svd
2025-04-26 18:36:42,191 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:36:45,679 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:36:49,412 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:36:55,083 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:37:05,979 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_28_mlp_gate_proj.safetensors True
Layer: model.layers.28.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [2.48785485e-02 2.44410057e-05 1.47788587e-05 1.22457295e-05
 1.07915093e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 18:38:47,220 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 18:38:47,220 - INFO - Replacing 'model.layers.28.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  89%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                   | 201/225 [1:32:38<11:13, 28.05s/it]2025-04-26 18:38:47,221 - INFO - Skipping layer model.layers.28.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:38:47,221 - INFO - Skipping layer model.layers.28.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:38:47,221 - INFO - Skipping layer model.layers.29.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:38:47,221 - INFO - Skipping layer model.layers.29.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:38:47,221 - INFO - Skipping layer model.layers.29.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:38:47,221 - INFO - Skipping layer model.layers.29.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:38:47,221 - INFO - Skipping layer model.layers.29.mlp.gate_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:38:47,221 - INFO - Layer: model.layers.29.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:38:47,223 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_29_mlp_up_proj.safetensors
2025-04-26 18:38:47,223 - INFO - exists: True
2025-04-26 18:38:47,262 - INFO - factorize_layer_kron_svd
2025-04-26 18:38:49,090 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:38:50,800 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:38:54,905 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:39:00,489 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:39:10,701 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_29_mlp_up_proj.safetensors True
Layer: model.layers.29.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [2.5008040e-02 2.4909425e-05 1.7541013e-05 1.1958569e-05 1.1578863e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 18:40:51,534 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 18:40:51,535 - INFO - Replacing 'model.layers.29.mlp.up_proj' (attribute 'up_proj' within parent LlamaMLP)
Compressing Layers:  93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏            | 209/225 [1:34:42<06:05, 22.86s/it]2025-04-26 18:40:51,535 - INFO - Layer: model.layers.29.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:40:51,536 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_29_mlp_down_proj.safetensors
2025-04-26 18:40:51,536 - INFO - exists: True
2025-04-26 18:40:51,576 - INFO - factorize_layer_kron_svd
2025-04-26 18:40:54,366 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:40:59,453 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:41:06,200 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:41:09,184 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:41:12,711 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:41:16,404 - INFO -   Factor is positive definite (alpha=1.00e-03)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_29_mlp_down_proj.safetensors True
Layer: model.layers.29.mlp.down_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=11008, out_features=4096, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (4096x11008)...
  SVD complete. Singular values (top 5): [8.4675699e-02 1.7794147e-04 6.2655701e-05 4.7701098e-05 4.6559559e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,11008), (4096,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)
2025-04-26 18:43:52,150 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=11008, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=4096, bias=False)
)')
2025-04-26 18:43:52,151 - INFO - Replacing 'model.layers.29.mlp.down_proj' (attribute 'down_proj' within parent LlamaMLP)
Compressing Layers:  93%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉            | 210/225 [1:37:43<08:25, 33.73s/it]2025-04-26 18:43:52,151 - INFO - Skipping layer model.layers.30.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:43:52,151 - INFO - Skipping layer model.layers.30.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:43:52,151 - INFO - Skipping layer model.layers.30.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:43:52,151 - INFO - Skipping layer model.layers.30.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:43:52,151 - INFO - Layer: model.layers.30.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:43:52,153 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_30_mlp_gate_proj.safetensors
2025-04-26 18:43:52,153 - INFO - exists: True
2025-04-26 18:43:52,209 - INFO - factorize_layer_kron_svd
2025-04-26 18:43:54,183 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:43:55,794 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:43:57,341 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 18:43:59,115 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 18:44:00,629 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 18:44:03,388 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-26 18:44:05,701 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:44:08,999 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:44:12,983 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 18:44:15,519 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 18:44:19,881 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 18:44:28,909 - INFO -   Factor is positive definite (alpha=1.00e+00)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_30_mlp_gate_proj.safetensors True
Layer: model.layers.30.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Regularizing factor (try 3, alpha=1.00e-03)
  Regularizing factor (try 4, alpha=1.00e-02)
  Regularizing factor (try 5, alpha=1.00e-01)
  Factor is positive definite (alpha=1.00e+00)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [1.9579800e-05 1.1570799e-05 1.0300217e-05 7.9101983e-06 6.5702579e-06]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 18:46:17,251 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 18:46:17,251 - INFO - Replacing 'model.layers.30.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉        | 215/225 [1:40:08<05:21, 32.18s/it]2025-04-26 18:46:17,252 - INFO - Skipping layer model.layers.30.mlp.up_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:46:17,252 - INFO - Skipping layer model.layers.30.mlp.down_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:46:17,252 - INFO - Skipping layer model.layers.31.self_attn.q_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:46:17,252 - INFO - Skipping layer model.layers.31.self_attn.k_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:46:17,252 - INFO - Skipping layer model.layers.31.self_attn.v_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:46:17,252 - INFO - Skipping layer model.layers.31.self_attn.o_proj: Sensitivity ratio >= 1.0 (1.00). No compression.
2025-04-26 18:46:17,252 - INFO - Layer: model.layers.31.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:46:17,254 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_31_mlp_gate_proj.safetensors
2025-04-26 18:46:17,254 - INFO - exists: True
2025-04-26 18:46:17,327 - INFO - factorize_layer_kron_svd
2025-04-26 18:46:19,807 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:46:21,564 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:46:23,337 - INFO -   Factor is positive definite (alpha=1.00e-03)
2025-04-26 18:46:26,742 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:46:34,196 - INFO -   Factor is positive definite (alpha=1.00e-04)
factor_filename /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_31_mlp_gate_proj.safetensors True
Layer: model.layers.31.mlp.gate_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
layer module Linear(in_features=4096, out_features=11008, bias=False)
factorize_layer_kron_svd
Regularizing factors for layer (initial alpha=1.00e-05)...
  Regularizing factor (try 1, alpha=1.00e-05)
  Regularizing factor (try 2, alpha=1.00e-04)
  Factor is positive definite (alpha=1.00e-03)
  Regularizing factor (try 1, alpha=1.00e-05)
  Factor is positive definite (alpha=1.00e-04)
  Cholesky decomposition successful.
  Performing SVD on transformed matrix (11008x4096)...
  SVD complete. Singular values (top 5): [5.3034523e-03 6.2283059e-04 3.3307722e-05 2.5902780e-05 2.1905900e-05]
  Cholesky factor inverses computed.
  Factorization complete for layer. New shapes: (2152,4096), (11008,2152)
factorized_sequential Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)
2025-04-26 18:48:49,037 - INFO - factorized_sequential 'Sequential(
  (0): Linear(in_features=4096, out_features=2152, bias=False)
  (1): Linear(in_features=2152, out_features=11008, bias=False)
)')
2025-04-26 18:48:49,037 - INFO - Replacing 'model.layers.31.mlp.gate_proj' (attribute 'gate_proj' within parent LlamaMLP)
Compressing Layers:  99%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌  | 222/225 [1:42:40<01:24, 28.01s/it]2025-04-26 18:48:49,038 - INFO - Layer: model.layers.31.mlp.up_proj | Ratio: 0.720 -> Target Rank: 2152 (Align: 8)
2025-04-26 18:48:49,042 - INFO - factor_filename: /home/jovyan/shares/SR004.nfs2/chekalina/FisherKronecker/grads_output/llama-2-7b-chat/fisher_factors_output_1404/model_layers_31_mlp_up_proj.safetensors
2025-04-26 18:48:49,042 - INFO - exists: True
2025-04-26 18:48:49,109 - INFO - factorize_layer_kron_svd
2025-04-26 18:48:51,008 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:48:53,462 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:48:55,092 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 18:48:56,643 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 18:48:58,184 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 18:49:00,052 - INFO -   Factor is positive definite (alpha=1.00e+00)
2025-04-26 18:49:02,401 - INFO -   Regularizing factor (try 1, alpha=1.00e-05)
2025-04-26 18:49:05,838 - INFO -   Regularizing factor (try 2, alpha=1.00e-04)
2025-04-26 18:49:08,486 - INFO -   Regularizing factor (try 3, alpha=1.00e-03)
2025-04-26 18:49:11,796 - INFO -   Regularizing factor (try 4, alpha=1.00e-02)
2025-04-26 18:49:15,540 - INFO -   Regularizing factor (try 5, alpha=1.00e-01)
2025-04-26 18:49:21,190 - INFO -   Factor is positive definite (alpha=1.00e+00)
